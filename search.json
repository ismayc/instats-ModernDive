[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ModernDive v2 Seminar",
    "section": "",
    "text": "Welcome to this page for the ModernDive v2 seminar! Here you will find information about the seminar, including links to the materials (slides and walkthrough/exercise answers)."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "Here are the slides for each day (PDF):\n\nDay 1 (Sessions 1-3)\nDay 2 (Sessions 4-6)\nDay 3 (Sessions 7-9)\nDay 4 (Sessions 10-12)"
  },
  {
    "objectID": "answers.html",
    "href": "answers.html",
    "title": "Answers",
    "section": "",
    "text": "Here are the answer keys for each day:\n\nDay 1 (Sessions 1-3) Walkthrough Answers\nDay 1 (Sessions 1-3) Exercise Answers\nDay 2 (Sessions 4-6) Walkthrough Answers\nDay 2 (Sessions 4-6) Exercise Answers\nDay 3 (Sessions 7-9) Walkthrough Answers\nDay 3 (Sessions 7-9) Exercise Answers\nDay 4 (Sessions 10-12) Walkthrough Answers"
  },
  {
    "objectID": "answers/day3_exercise_answers.html",
    "href": "answers/day3_exercise_answers.html",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# Load the required packages\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(infer)\n\n\nThese packages provide tools for data wrangling, visualization, and modeling.\n\n\n\n\n\n\n# For reproducibility\nset.seed(2024)\n\n# Create vectors of sport ball types and their proportions\ntypes_of_sport_balls &lt;- c(\"Basketball\", \"Pickleball\", \"Tennis Ball\", \n                          \"Football/Soccer\", \"American Football\")\nproportion_of_each_type &lt;- c(0.2, 0.15, 0.3, 0.25, 0.1)\n\n# Create a tibble of 1200 sport balls that will act as our population\nstore_ball_inventory &lt;- tibble(\n  ball_ID = 1:1200,\n  ball_type = sample(x = types_of_sport_balls,\n                     size = 1200, \n                     replace = TRUE, \n                     prob = proportion_of_each_type\n  )\n)\n\n\nWe’ll be exploring a synthesized data set of an inventory at a large sporting goods store. The inventory pertains to different types of sports balls, 1200 in total.\n\n\n\n\n\n\n# Use glimpse to explore the structure of the dataset\nglimpse(store_ball_inventory)\n\nRows: 1,200\nColumns: 2\n$ ball_ID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 2…\n$ ball_type &lt;chr&gt; \"Pickleball\", \"Football/Soccer\", \"Basketball\", \"Basketball\", \"Football/Soccer\", \"Basketball\", \"Footb…\n\n\n\n\n\n\n\n# Create a count of ball_type\nstore_ball_inventory |&gt; \n  count(ball_type)\n\n# A tibble: 5 × 2\n  ball_type             n\n  &lt;chr&gt;             &lt;int&gt;\n1 American Football   117\n2 Basketball          252\n3 Football/Soccer     299\n4 Pickleball          187\n5 Tennis Ball         345\n\n# Determine the proportion of basketballs in the inventory\np_df &lt;- store_ball_inventory |&gt; \n  summarize(prop_bball = mean(ball_type == \"Basketball\"))\n\n# Convert p to a numeric value\np &lt;- p_df$prop_bball\n\n# Or using the tidyverse\np &lt;- p_df |&gt; pull(prop_bball)\np\n\n[1] 0.21\n\n\n\n\n\n\n\n# Retrieve a sample of 10 balls from the inventory\nball_sample &lt;- store_ball_inventory |&gt; \n  slice_sample(n = 10, replace = FALSE)\n\n# Determine the proportion of basketballs in the sample\nball_sample |&gt; \n  summarize(prop_bball = mean(ball_type == \"Basketball\"))\n\n# A tibble: 1 × 1\n  prop_bball\n       &lt;dbl&gt;\n1        0.4\n\n\n\n\n\n\n\n# Retrieve another sample of 50 balls from the inventory\nball_sample2 &lt;- store_ball_inventory |&gt; \n  slice_sample(n = 10, replace = FALSE)\n\n# Determine the proportion of pickleballs in the sample\nball_sample2 |&gt; \n  summarize(prop_bball = mean(ball_type == \"Basketball\"))\n\n# A tibble: 1 × 1\n  prop_bball\n       &lt;dbl&gt;\n1        0.3\n\n\n\n\n\n\n\n# Use `rep_slice_sample()` from the `infer` package\nball_samples &lt;- store_ball_inventory |&gt; \n  rep_slice_sample(n = 10, reps = 1000, replace = FALSE)\nball_samples\n\n# A tibble: 10,000 × 3\n# Groups:   replicate [1,000]\n   replicate ball_ID ball_type        \n       &lt;int&gt;   &lt;int&gt; &lt;chr&gt;            \n 1         1     940 Basketball       \n 2         1    1052 American Football\n 3         1     749 Football/Soccer  \n 4         1     497 Tennis Ball      \n 5         1    1081 Tennis Ball      \n 6         1     737 Basketball       \n 7         1     979 Pickleball       \n 8         1     944 Tennis Ball      \n 9         1    1112 Tennis Ball      \n10         1     855 Tennis Ball      \n# ℹ 9,990 more rows\n\n\n\n\n\n\n\n# Determine sample proportions with `dplyr`\nprops_bball &lt;- ball_samples |&gt; \n  summarize(prop_bball = mean(ball_type == \"Basketball\"))\n\n# Create a histogram of the sample proportions with 8 bins\nggplot(props_bball, aes(x = prop_bball)) +\n  geom_histogram(bins = 8, color = \"white\") +\n  labs(x = \"Sample proportion\", \n       title = \"Histogram of 1000 sample proportions of Basketballs\") \n\n\n\n\n\n\n\n\n\n# Using the simulations, calculate the standard deviation of the \n# sample proportions\nse_sample_props &lt;- props_bball |&gt; \n  summarize(sd(prop_bball)) |&gt; \n  pull()\n\n# Using the formula for the standard error of a sample proportion\nn &lt;- 10\nse_sample_props_formula &lt;- sqrt(p * (1 - p) / n)\nse_sample_props_formula\n\n[1] 0.1288022\n\n\n\n\n\n\n# Here's the function from the walkthrough to calculate the standard error\nse_sample_props &lt;- function(size, repetitions = 1000, type = \"Pickleball\") {\n  props &lt;- store_ball_inventory |&gt; \n    rep_slice_sample(n = size, reps = repetitions, replace = FALSE) |&gt; \n    summarize(prop = mean(ball_type == type))\n  \n  se_sample_props &lt;- props |&gt; \n    summarize(sd(prop)) |&gt; \n    pull()\n  \n  return(se_sample_props)\n}\n\n# Use the function to calculate the standard error for a sample size of 10\n# with 2000 repetitions for Basketball\nse_sample_props(10, 2000, \"Basketball\")\n\n[1] 0.130401\n\n\n\n\n\n\n\n\n\nAssume we have information about the population of homes in Phoenix, AZ. We are interested in their use of electricity factoring in that some homes do not have air conditioning and only use fans.\n\n# For reproducibility\nset.seed(2024)\n\n# Number of homes in each group\nn_ac &lt;- 0.8 * 600000\nn_fans &lt;- 0.2 * 600000\n\n# Simulate electricity usage (in kWh) for homes with AC\nac_usage &lt;- rnorm(n_ac, mean = 1500, sd = 300)  # Higher mean for AC usage\n\n# Simulate electricity usage (in kWh) for homes using fans\nfan_usage &lt;- rnorm(n_fans, mean = 800, sd = 150)  # Lower mean for fan usage\n\n# Combine into a single data frame\nelectricity_usage_phoenix &lt;- tibble(\n  home_ID = 1:(n_ac + n_fans),\n  cooling_system = c(rep(\"AC\", n_ac), rep(\"Fans\", n_fans)),\n  usage_kWh = c(ac_usage, fan_usage)\n)\n\n# View the data\nelectricity_usage_phoenix\n\n# A tibble: 600,000 × 3\n   home_ID cooling_system usage_kWh\n     &lt;int&gt; &lt;chr&gt;              &lt;dbl&gt;\n 1       1 AC                 1795.\n 2       2 AC                 1641.\n 3       3 AC                 1468.\n 4       4 AC                 1436.\n 5       5 AC                 1847.\n 6       6 AC                 1888.\n 7       7 AC                 1660.\n 8       8 AC                 1462.\n 9       9 AC                 1133.\n10      10 AC                 1164.\n# ℹ 599,990 more rows\n\n\n\n\n\n\n# Choose sample size\nsample_size &lt;- 1000\n\n# Generate a sample\nset.seed(2024)\nusage_sample &lt;- electricity_usage_phoenix |&gt; \n  slice_sample(n = sample_size, replace = FALSE)\n\n# Calculate the sample mean\nsample_mean &lt;- usage_sample |&gt; \n  summarize(mean(usage_kWh)) |&gt; \n  pull()\nsample_mean\n\n[1] 1346.212\n\n# Calculate the standard deviation \nsample_sd &lt;- usage_sample |&gt; \n  summarize(sd(usage_kWh)) |&gt; \n  pull()\nsample_sd\n\n[1] 399.186\n\n\n\n\n\n\n# Calculate the population mean\npopulation_mean &lt;- electricity_usage_phoenix |&gt; \n  summarize(mean(usage_kWh)) |&gt; \n  pull()\nmu &lt;- population_mean\nmu\n\n[1] 1360.137\n\n# Calculate the population standard deviation\npopulation_sd &lt;- electricity_usage_phoenix |&gt; \n  summarize(sd(usage_kWh)) |&gt; \n  pull()\nsigma &lt;- population_sd\nsigma\n\n[1] 393.7991\n\n\n\n\n\n\n# Calculate the margin of error for a 90% confidence interval\nz_star &lt;- qnorm(p = 0.95) # Assumes a normal distribution\nmargin_of_error &lt;- z_star * (sigma / sqrt(sample_size))\n\n# Recall the point estimate\npoint_estimate &lt;- sample_mean\n\n# Calculate the confidence interval\nlower_bound &lt;- point_estimate - margin_of_error\nupper_bound &lt;- point_estimate + margin_of_error\n\n# Display the confidence interval\nc(lower_bound, upper_bound)\n\n[1] 1325.729 1366.696\n\n# Remember the population parameter (we usually don't know it)\nbetween(mu, lower_bound, upper_bound) # dplyr::between()\n\n[1] TRUE\n\n\n\n\n\n\n# Calculate the margin of error for a 90% confidence interval\nt_star &lt;- qt(p = 0.95, df = sample_size - 1) # t-distribution\nmargin_of_error_t &lt;- t_star * (sample_sd / sqrt(sample_size))\n\n# Same point estimate\npoint_estimate_t &lt;- sample_mean\n\n# Calculate the confidence interval\nlower_bound_t &lt;- point_estimate_t - margin_of_error_t\nupper_bound_t &lt;- point_estimate_t + margin_of_error_t\n\n# Display the confidence interval\nc(lower_bound_t, upper_bound_t)\n\n[1] 1325.429 1366.995\n\n# Remember the population parameter (we usually don't know it)\nbetween(mu, lower_bound_t, upper_bound_t)\n\n[1] TRUE\n\n\n\n\n\nWe are “95% confident” that the true mean commute time for this population of Phoenix houses is between 1325.4294955 and 1366.9952348 kilowatt-hours. This is the same as saying that if we were to take many samples of size 100 and calculate the confidence interval for each sample, we would expect 95% of those intervals to contain the true population mean of 1360.1370117.\n\n\n\n\n\n\n\n\n# Assume we only have a sample of 1000 homes and their electricity usage\nusage_sample\n\n# A tibble: 1,000 × 3\n   home_ID cooling_system usage_kWh\n     &lt;int&gt; &lt;chr&gt;              &lt;dbl&gt;\n 1   86565 AC                 1826.\n 2  532090 Fans                625.\n 3  127999 AC                 1738.\n 4  597164 Fans                503.\n 5  121242 AC                 1183.\n 6   53451 AC                 1397.\n 7  132457 AC                 1132.\n 8  503380 Fans               1022.\n 9  256276 AC                 1491.\n10  418903 AC                 1201.\n# ℹ 990 more rows\n\n\n\n\n\n\n\n\nStart with the data and then |&gt; into the following functions:\n- specify(): Define the variable of interest\n- generate(): Create the bootstrap samples\n- calculate(): Calculate the statistic of interest\n- visualize(): Visualize the bootstrap distribution\n\n\n\n\n\n\n# Bootstrapping the sample\nset.seed(2024)\none_bootstrap &lt;- usage_sample |&gt; \n  specify(response = usage_kWh) |&gt; \n  generate(reps = 1, type = \"bootstrap\")\none_bootstrap\n\nResponse: usage_kWh (numeric)\n# A tibble: 1,000 × 2\n# Groups:   replicate [1]\n   replicate usage_kWh\n       &lt;int&gt;     &lt;dbl&gt;\n 1         1     1933.\n 2         1     1733.\n 3         1      947.\n 4         1     1497.\n 5         1     1980.\n 6         1     1334.\n 7         1      784.\n 8         1     1175.\n 9         1     1593.\n10         1     1431.\n# ℹ 990 more rows\n\n\n\n\n\n\n\n# Calculate the mean of the bootstrap sample\none_bootstrap |&gt; \n  calculate(stat = \"mean\") |&gt; \n  pull()\n\n[1] 1332.641\n\n\n\n\n\n\n\n# Bootstrapping 1000 samples\nmany_bootstraps &lt;- usage_sample |&gt; \n  specify(response = usage_kWh) |&gt; \n  generate(reps = 1000, type = \"bootstrap\")\nmany_bootstraps\n\nResponse: usage_kWh (numeric)\n# A tibble: 1,000,000 × 2\n# Groups:   replicate [1,000]\n   replicate usage_kWh\n       &lt;int&gt;     &lt;dbl&gt;\n 1         1      727.\n 2         1     1757.\n 3         1     1343.\n 4         1      796.\n 5         1     1542.\n 6         1     1609.\n 7         1     2069.\n 8         1     1044.\n 9         1     2030.\n10         1     1511.\n# ℹ 999,990 more rows\n\n\n\n\n\n\n# Calculate the mean of each bootstrap sample\nbootstrap_means &lt;- many_bootstraps |&gt; \n  calculate(stat = \"mean\")\nbootstrap_means\n\nResponse: usage_kWh (numeric)\n# A tibble: 1,000 × 2\n   replicate  stat\n       &lt;int&gt; &lt;dbl&gt;\n 1         1 1357.\n 2         2 1338.\n 3         3 1335.\n 4         4 1347.\n 5         5 1340.\n 6         6 1336.\n 7         7 1345.\n 8         8 1337.\n 9         9 1372.\n10        10 1377.\n# ℹ 990 more rows\n\n\n\n\n\n\n\n# Create a histogram of the bootstrap means\nggplot(bootstrap_means, aes(x = stat)) +\n  geom_histogram(bins = 30, color = \"white\") +\n  labs(x = \"Bootstrap sample mean\", \n       title = \"Histogram of 1000 bootstrap sample means\")\n\n\n\n\n\n\n\n\n\n# Calculate the 90% bootstrap confidence interval in two ways since bell-shaped\nbootstrap_percentile_ci &lt;- bootstrap_means |&gt; \n  get_confidence_interval(level = 0.90, type = \"percentile\")\n\nbootstrap_se_ci &lt;- bootstrap_means |&gt; \n  get_confidence_interval(level = 0.90, \n                          type = \"se\",\n                          point_estimate = sample_mean)\n\n\n\n\n\nWe are “95% confident” that the true mean electricity usage for this population of adults is between 1326.0090851 and 1366.9481513 kilowatt-hours. This is the same as saying that if we were to take many samples of size 100 and calculate the confidence interval for each sample, we would expect 95% of those intervals to contain the true population mean of 1360.1370117.\n\n\n\n\n\n# Show the histogram of bootstrap means with the confidence interval\n# and the population parameter (not usually known)\nbootstrap_means |&gt; \n  visualize() +\n  shade_confidence_interval(endpoints = bootstrap_percentile_ci) +\n  geom_vline(xintercept = mu, color = \"purple\", linewidth = 2)"
  },
  {
    "objectID": "answers/day3_exercise_answers.html#session-1-sampling",
    "href": "answers/day3_exercise_answers.html#session-1-sampling",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# Load the required packages\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(infer)\n\n\nThese packages provide tools for data wrangling, visualization, and modeling.\n\n\n\n\n\n\n# For reproducibility\nset.seed(2024)\n\n# Create vectors of sport ball types and their proportions\ntypes_of_sport_balls &lt;- c(\"Basketball\", \"Pickleball\", \"Tennis Ball\", \n                          \"Football/Soccer\", \"American Football\")\nproportion_of_each_type &lt;- c(0.2, 0.15, 0.3, 0.25, 0.1)\n\n# Create a tibble of 1200 sport balls that will act as our population\nstore_ball_inventory &lt;- tibble(\n  ball_ID = 1:1200,\n  ball_type = sample(x = types_of_sport_balls,\n                     size = 1200, \n                     replace = TRUE, \n                     prob = proportion_of_each_type\n  )\n)\n\n\nWe’ll be exploring a synthesized data set of an inventory at a large sporting goods store. The inventory pertains to different types of sports balls, 1200 in total.\n\n\n\n\n\n\n# Use glimpse to explore the structure of the dataset\nglimpse(store_ball_inventory)\n\nRows: 1,200\nColumns: 2\n$ ball_ID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 2…\n$ ball_type &lt;chr&gt; \"Pickleball\", \"Football/Soccer\", \"Basketball\", \"Basketball\", \"Football/Soccer\", \"Basketball\", \"Footb…\n\n\n\n\n\n\n\n# Create a count of ball_type\nstore_ball_inventory |&gt; \n  count(ball_type)\n\n# A tibble: 5 × 2\n  ball_type             n\n  &lt;chr&gt;             &lt;int&gt;\n1 American Football   117\n2 Basketball          252\n3 Football/Soccer     299\n4 Pickleball          187\n5 Tennis Ball         345\n\n# Determine the proportion of basketballs in the inventory\np_df &lt;- store_ball_inventory |&gt; \n  summarize(prop_bball = mean(ball_type == \"Basketball\"))\n\n# Convert p to a numeric value\np &lt;- p_df$prop_bball\n\n# Or using the tidyverse\np &lt;- p_df |&gt; pull(prop_bball)\np\n\n[1] 0.21\n\n\n\n\n\n\n\n# Retrieve a sample of 10 balls from the inventory\nball_sample &lt;- store_ball_inventory |&gt; \n  slice_sample(n = 10, replace = FALSE)\n\n# Determine the proportion of basketballs in the sample\nball_sample |&gt; \n  summarize(prop_bball = mean(ball_type == \"Basketball\"))\n\n# A tibble: 1 × 1\n  prop_bball\n       &lt;dbl&gt;\n1        0.4\n\n\n\n\n\n\n\n# Retrieve another sample of 50 balls from the inventory\nball_sample2 &lt;- store_ball_inventory |&gt; \n  slice_sample(n = 10, replace = FALSE)\n\n# Determine the proportion of pickleballs in the sample\nball_sample2 |&gt; \n  summarize(prop_bball = mean(ball_type == \"Basketball\"))\n\n# A tibble: 1 × 1\n  prop_bball\n       &lt;dbl&gt;\n1        0.3\n\n\n\n\n\n\n\n# Use `rep_slice_sample()` from the `infer` package\nball_samples &lt;- store_ball_inventory |&gt; \n  rep_slice_sample(n = 10, reps = 1000, replace = FALSE)\nball_samples\n\n# A tibble: 10,000 × 3\n# Groups:   replicate [1,000]\n   replicate ball_ID ball_type        \n       &lt;int&gt;   &lt;int&gt; &lt;chr&gt;            \n 1         1     940 Basketball       \n 2         1    1052 American Football\n 3         1     749 Football/Soccer  \n 4         1     497 Tennis Ball      \n 5         1    1081 Tennis Ball      \n 6         1     737 Basketball       \n 7         1     979 Pickleball       \n 8         1     944 Tennis Ball      \n 9         1    1112 Tennis Ball      \n10         1     855 Tennis Ball      \n# ℹ 9,990 more rows\n\n\n\n\n\n\n\n# Determine sample proportions with `dplyr`\nprops_bball &lt;- ball_samples |&gt; \n  summarize(prop_bball = mean(ball_type == \"Basketball\"))\n\n# Create a histogram of the sample proportions with 8 bins\nggplot(props_bball, aes(x = prop_bball)) +\n  geom_histogram(bins = 8, color = \"white\") +\n  labs(x = \"Sample proportion\", \n       title = \"Histogram of 1000 sample proportions of Basketballs\") \n\n\n\n\n\n\n\n\n\n# Using the simulations, calculate the standard deviation of the \n# sample proportions\nse_sample_props &lt;- props_bball |&gt; \n  summarize(sd(prop_bball)) |&gt; \n  pull()\n\n# Using the formula for the standard error of a sample proportion\nn &lt;- 10\nse_sample_props_formula &lt;- sqrt(p * (1 - p) / n)\nse_sample_props_formula\n\n[1] 0.1288022\n\n\n\n\n\n\n# Here's the function from the walkthrough to calculate the standard error\nse_sample_props &lt;- function(size, repetitions = 1000, type = \"Pickleball\") {\n  props &lt;- store_ball_inventory |&gt; \n    rep_slice_sample(n = size, reps = repetitions, replace = FALSE) |&gt; \n    summarize(prop = mean(ball_type == type))\n  \n  se_sample_props &lt;- props |&gt; \n    summarize(sd(prop)) |&gt; \n    pull()\n  \n  return(se_sample_props)\n}\n\n# Use the function to calculate the standard error for a sample size of 10\n# with 2000 repetitions for Basketball\nse_sample_props(10, 2000, \"Basketball\")\n\n[1] 0.130401"
  },
  {
    "objectID": "answers/day3_exercise_answers.html#session-2-estimation-using-theory-based-methods",
    "href": "answers/day3_exercise_answers.html#session-2-estimation-using-theory-based-methods",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "Assume we have information about the population of homes in Phoenix, AZ. We are interested in their use of electricity factoring in that some homes do not have air conditioning and only use fans.\n\n# For reproducibility\nset.seed(2024)\n\n# Number of homes in each group\nn_ac &lt;- 0.8 * 600000\nn_fans &lt;- 0.2 * 600000\n\n# Simulate electricity usage (in kWh) for homes with AC\nac_usage &lt;- rnorm(n_ac, mean = 1500, sd = 300)  # Higher mean for AC usage\n\n# Simulate electricity usage (in kWh) for homes using fans\nfan_usage &lt;- rnorm(n_fans, mean = 800, sd = 150)  # Lower mean for fan usage\n\n# Combine into a single data frame\nelectricity_usage_phoenix &lt;- tibble(\n  home_ID = 1:(n_ac + n_fans),\n  cooling_system = c(rep(\"AC\", n_ac), rep(\"Fans\", n_fans)),\n  usage_kWh = c(ac_usage, fan_usage)\n)\n\n# View the data\nelectricity_usage_phoenix\n\n# A tibble: 600,000 × 3\n   home_ID cooling_system usage_kWh\n     &lt;int&gt; &lt;chr&gt;              &lt;dbl&gt;\n 1       1 AC                 1795.\n 2       2 AC                 1641.\n 3       3 AC                 1468.\n 4       4 AC                 1436.\n 5       5 AC                 1847.\n 6       6 AC                 1888.\n 7       7 AC                 1660.\n 8       8 AC                 1462.\n 9       9 AC                 1133.\n10      10 AC                 1164.\n# ℹ 599,990 more rows\n\n\n\n\n\n\n# Choose sample size\nsample_size &lt;- 1000\n\n# Generate a sample\nset.seed(2024)\nusage_sample &lt;- electricity_usage_phoenix |&gt; \n  slice_sample(n = sample_size, replace = FALSE)\n\n# Calculate the sample mean\nsample_mean &lt;- usage_sample |&gt; \n  summarize(mean(usage_kWh)) |&gt; \n  pull()\nsample_mean\n\n[1] 1346.212\n\n# Calculate the standard deviation \nsample_sd &lt;- usage_sample |&gt; \n  summarize(sd(usage_kWh)) |&gt; \n  pull()\nsample_sd\n\n[1] 399.186\n\n\n\n\n\n\n# Calculate the population mean\npopulation_mean &lt;- electricity_usage_phoenix |&gt; \n  summarize(mean(usage_kWh)) |&gt; \n  pull()\nmu &lt;- population_mean\nmu\n\n[1] 1360.137\n\n# Calculate the population standard deviation\npopulation_sd &lt;- electricity_usage_phoenix |&gt; \n  summarize(sd(usage_kWh)) |&gt; \n  pull()\nsigma &lt;- population_sd\nsigma\n\n[1] 393.7991\n\n\n\n\n\n\n# Calculate the margin of error for a 90% confidence interval\nz_star &lt;- qnorm(p = 0.95) # Assumes a normal distribution\nmargin_of_error &lt;- z_star * (sigma / sqrt(sample_size))\n\n# Recall the point estimate\npoint_estimate &lt;- sample_mean\n\n# Calculate the confidence interval\nlower_bound &lt;- point_estimate - margin_of_error\nupper_bound &lt;- point_estimate + margin_of_error\n\n# Display the confidence interval\nc(lower_bound, upper_bound)\n\n[1] 1325.729 1366.696\n\n# Remember the population parameter (we usually don't know it)\nbetween(mu, lower_bound, upper_bound) # dplyr::between()\n\n[1] TRUE\n\n\n\n\n\n\n# Calculate the margin of error for a 90% confidence interval\nt_star &lt;- qt(p = 0.95, df = sample_size - 1) # t-distribution\nmargin_of_error_t &lt;- t_star * (sample_sd / sqrt(sample_size))\n\n# Same point estimate\npoint_estimate_t &lt;- sample_mean\n\n# Calculate the confidence interval\nlower_bound_t &lt;- point_estimate_t - margin_of_error_t\nupper_bound_t &lt;- point_estimate_t + margin_of_error_t\n\n# Display the confidence interval\nc(lower_bound_t, upper_bound_t)\n\n[1] 1325.429 1366.995\n\n# Remember the population parameter (we usually don't know it)\nbetween(mu, lower_bound_t, upper_bound_t)\n\n[1] TRUE\n\n\n\n\n\nWe are “95% confident” that the true mean commute time for this population of Phoenix houses is between 1325.4294955 and 1366.9952348 kilowatt-hours. This is the same as saying that if we were to take many samples of size 100 and calculate the confidence interval for each sample, we would expect 95% of those intervals to contain the true population mean of 1360.1370117."
  },
  {
    "objectID": "answers/day3_exercise_answers.html#session-3-estimation-via-bootstrapping-methods",
    "href": "answers/day3_exercise_answers.html#session-3-estimation-via-bootstrapping-methods",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# Assume we only have a sample of 1000 homes and their electricity usage\nusage_sample\n\n# A tibble: 1,000 × 3\n   home_ID cooling_system usage_kWh\n     &lt;int&gt; &lt;chr&gt;              &lt;dbl&gt;\n 1   86565 AC                 1826.\n 2  532090 Fans                625.\n 3  127999 AC                 1738.\n 4  597164 Fans                503.\n 5  121242 AC                 1183.\n 6   53451 AC                 1397.\n 7  132457 AC                 1132.\n 8  503380 Fans               1022.\n 9  256276 AC                 1491.\n10  418903 AC                 1201.\n# ℹ 990 more rows\n\n\n\n\n\n\n\n\nStart with the data and then |&gt; into the following functions:\n- specify(): Define the variable of interest\n- generate(): Create the bootstrap samples\n- calculate(): Calculate the statistic of interest\n- visualize(): Visualize the bootstrap distribution\n\n\n\n\n\n\n# Bootstrapping the sample\nset.seed(2024)\none_bootstrap &lt;- usage_sample |&gt; \n  specify(response = usage_kWh) |&gt; \n  generate(reps = 1, type = \"bootstrap\")\none_bootstrap\n\nResponse: usage_kWh (numeric)\n# A tibble: 1,000 × 2\n# Groups:   replicate [1]\n   replicate usage_kWh\n       &lt;int&gt;     &lt;dbl&gt;\n 1         1     1933.\n 2         1     1733.\n 3         1      947.\n 4         1     1497.\n 5         1     1980.\n 6         1     1334.\n 7         1      784.\n 8         1     1175.\n 9         1     1593.\n10         1     1431.\n# ℹ 990 more rows\n\n\n\n\n\n\n\n# Calculate the mean of the bootstrap sample\none_bootstrap |&gt; \n  calculate(stat = \"mean\") |&gt; \n  pull()\n\n[1] 1332.641\n\n\n\n\n\n\n\n# Bootstrapping 1000 samples\nmany_bootstraps &lt;- usage_sample |&gt; \n  specify(response = usage_kWh) |&gt; \n  generate(reps = 1000, type = \"bootstrap\")\nmany_bootstraps\n\nResponse: usage_kWh (numeric)\n# A tibble: 1,000,000 × 2\n# Groups:   replicate [1,000]\n   replicate usage_kWh\n       &lt;int&gt;     &lt;dbl&gt;\n 1         1      727.\n 2         1     1757.\n 3         1     1343.\n 4         1      796.\n 5         1     1542.\n 6         1     1609.\n 7         1     2069.\n 8         1     1044.\n 9         1     2030.\n10         1     1511.\n# ℹ 999,990 more rows\n\n\n\n\n\n\n# Calculate the mean of each bootstrap sample\nbootstrap_means &lt;- many_bootstraps |&gt; \n  calculate(stat = \"mean\")\nbootstrap_means\n\nResponse: usage_kWh (numeric)\n# A tibble: 1,000 × 2\n   replicate  stat\n       &lt;int&gt; &lt;dbl&gt;\n 1         1 1357.\n 2         2 1338.\n 3         3 1335.\n 4         4 1347.\n 5         5 1340.\n 6         6 1336.\n 7         7 1345.\n 8         8 1337.\n 9         9 1372.\n10        10 1377.\n# ℹ 990 more rows\n\n\n\n\n\n\n\n# Create a histogram of the bootstrap means\nggplot(bootstrap_means, aes(x = stat)) +\n  geom_histogram(bins = 30, color = \"white\") +\n  labs(x = \"Bootstrap sample mean\", \n       title = \"Histogram of 1000 bootstrap sample means\")\n\n\n\n\n\n\n\n\n\n# Calculate the 90% bootstrap confidence interval in two ways since bell-shaped\nbootstrap_percentile_ci &lt;- bootstrap_means |&gt; \n  get_confidence_interval(level = 0.90, type = \"percentile\")\n\nbootstrap_se_ci &lt;- bootstrap_means |&gt; \n  get_confidence_interval(level = 0.90, \n                          type = \"se\",\n                          point_estimate = sample_mean)\n\n\n\n\n\nWe are “95% confident” that the true mean electricity usage for this population of adults is between 1326.0090851 and 1366.9481513 kilowatt-hours. This is the same as saying that if we were to take many samples of size 100 and calculate the confidence interval for each sample, we would expect 95% of those intervals to contain the true population mean of 1360.1370117.\n\n\n\n\n\n# Show the histogram of bootstrap means with the confidence interval\n# and the population parameter (not usually known)\nbootstrap_means |&gt; \n  visualize() +\n  shade_confidence_interval(endpoints = bootstrap_percentile_ci) +\n  geom_vline(xintercept = mu, color = \"purple\", linewidth = 2)"
  },
  {
    "objectID": "answers/day3_walkthrough_answers.html",
    "href": "answers/day3_walkthrough_answers.html",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# Load the required packages\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(infer)\n\n\nThese packages provide tools for data wrangling, visualization, modeling, and inference.\n\n\n\n\n\n\n# For reproducibility\nset.seed(2024)\n\n# Create vectors of sport ball types and their proportions\ntypes_of_sport_balls &lt;- c(\"Basketball\", \"Pickleball\", \"Tennis Ball\", \n                          \"Football/Soccer\", \"American Football\")\nproportion_of_each_type &lt;- c(0.2, 0.15, 0.3, 0.25, 0.1)\n\n# Create a tibble of 1200 sport balls that will act as our population\nstore_ball_inventory &lt;- tibble(\n  ball_ID = 1:1200,\n  ball_type = sample(x = types_of_sport_balls,\n                     size = 1200, \n                     replace = TRUE, \n                     prob = proportion_of_each_type\n  )\n)\n\n\nWe’ll be exploring a synthesized data set of an inventory at a large sporting goods store. The inventory pertains to different types of sports balls, 1200 in total.\n\n\n\n\n\n\n# Use glimpse to explore the structure of the dataset\nglimpse(store_ball_inventory)\n\nRows: 1,200\nColumns: 2\n$ ball_ID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 2…\n$ ball_type &lt;chr&gt; \"Pickleball\", \"Football/Soccer\", \"Basketball\", \"Basketball\", \"Football/Soccer\", \"Basketball\", \"Footb…\n\n\n\n\n\n\n\n# Create a count of ball_type\nstore_ball_inventory |&gt; \n  count(ball_type)\n\n# A tibble: 5 × 2\n  ball_type             n\n  &lt;chr&gt;             &lt;int&gt;\n1 American Football   117\n2 Basketball          252\n3 Football/Soccer     299\n4 Pickleball          187\n5 Tennis Ball         345\n\n# Determine the proportion of pickleballs in the inventory\np_df &lt;- store_ball_inventory |&gt; \n  summarize(prop_pickle = mean(ball_type == \"Pickleball\"))\n\n# Convert p to a numeric value\np &lt;- p_df$prop_pickle\n\n# Or using the tidyverse\np &lt;- p_df |&gt; pull(prop_pickle)\n\n\n\n\n\n\n# Retrieve a sample of 50 balls from the inventory\nball_sample &lt;- store_ball_inventory |&gt; \n  slice_sample(n = 50, replace = FALSE)\n\n# Determine the proportion of pickleballs in the sample\nball_sample |&gt; \n  summarize(prop_pickle = mean(ball_type == \"Pickleball\"))\n\n# A tibble: 1 × 1\n  prop_pickle\n        &lt;dbl&gt;\n1        0.14\n\n\n\n\n\n\n\n# Retrieve another sample of 50 balls from the inventory\nball_sample2 &lt;- store_ball_inventory |&gt; \n  slice_sample(n = 50, replace = FALSE)\n\n# Determine the proportion of pickleballs in the sample\nball_sample2 |&gt; \n  summarize(prop_pickle = mean(ball_type == \"Pickleball\"))\n\n# A tibble: 1 × 1\n  prop_pickle\n        &lt;dbl&gt;\n1         0.1\n\n\n\n\n\n\n\n# Use `rep_slice_sample()` from the `infer` package\nball_samples &lt;- store_ball_inventory |&gt; \n  rep_slice_sample(n = 50, reps = 1000, replace = FALSE)\n\n\n\n\n\n\n# Determine sample proportions with `dplyr`\nprops_pickle &lt;- ball_samples |&gt; \n  summarize(prop_pickle = mean(ball_type == \"Pickleball\"))\n\n# Create a histogram of the sample proportions\nggplot(props_pickle, aes(x = prop_pickle)) +\n  geom_histogram(bins = 15, color = \"white\") +\n  labs(x = \"Sample proportion\", \n       title = \"Histogram of 1000 sample proportions of Pickleballs\") \n\n\n\n\n\n\n\n\n\n# Using the simulations, calculate the standard deviation of the \n# sample proportions\nse_sample_props &lt;- props_pickle |&gt; \n  summarize(sd(prop_pickle)) |&gt; \n  pull()\n\n# Using the formula for the standard error of a sample proportion\nn &lt;- 50\nse_sample_props_formula &lt;- sqrt(p * (1 - p) / n)\n\n\n\n\n\n\n# Create a function to calculate the standard error of sample proportions\n# using simulation\nse_sample_props &lt;- function(size, repetitions = 1000, type = \"Pickleball\") {\n  props &lt;- store_ball_inventory |&gt; \n    rep_slice_sample(n = size, reps = repetitions, replace = FALSE) |&gt; \n    summarize(prop = mean(ball_type == type))\n  \n  se_sample_props &lt;- props |&gt; \n    summarize(sd(prop)) |&gt; \n    pull()\n  \n  return(se_sample_props)\n}\n\n# Standard errors for different sample sizes\nse_sample_props_20 &lt;- se_sample_props(20)\nse_sample_props_50 &lt;- se_sample_props(50)\nse_sample_props_100 &lt;- se_sample_props(100)\n\n\n\n\n(1.1) What is the purpose of using the sample() function in the code provided?\nA. To randomly select a subset of the population without replacement.\nB. To calculate the mean value of a numeric variable.\nC. To remove missing values from the dataset.\nD. To calculate the population parameters.\n\n(1.2) In the context of the sporting goods store example, what does the sample proportion of pickleballs represent?\nA. The actual number of pickleballs in the entire inventory.\nB. The proportion of pickleballs in a random sample taken from the inventory.\nC. The total number of all types of sport balls in the inventory.\nD. The probability of not selecting a pickleball from the population.\n\n(1.3) What does the function rep_slice_sample() do in the sampling process?\nA. It generates multiple samples from a population, each with the same size.\nB. It creates a histogram of sample proportions.\nC. It removes duplicate samples from the dataset.\nD. It replicates the population to simulate different proportions.\n\n(1.4) Why is the standard error calculated when taking samples from a population?\nA. To ensure that the sample is randomly selected. B. To estimate the total number of items in the population.\nC. To adjust the sample size for better accuracy.\nD. To measure how much sample proportions vary from the population proportion.\n\n(1.5) How does increasing the sample size affect the standard error of the sample proportions?\nA. It increases the standard error because more data points create more variation.\nB. It decreases the standard error, leading to more precise estimates of the population proportion.\nC. It has no effect on the standard error.\nD. It changes the population proportion directly.\n\n\n\n\n(7.1) What is the purpose of using the sample() function in the code provided?\nCorrect Answer:\nA. To randomly select a subset of the population without replacement.\nExplanation:\nThe sample() function is used to randomly select a subset of observations from the population, which helps create samples for analysis.\n\n(7.2) In the context of the sporting goods store example, what does the sample proportion of pickleballs represent?\nCorrect Answer:\nB. The proportion of pickleballs in a random sample taken from the inventory.\nExplanation:\nThe sample proportion reflects how many pickleballs appear in a random sample of the inventory, not the entire population.\n\n(7.3) What does the function rep_slice_sample() do in the sampling process?\nCorrect Answer:\nA. It generates multiple samples from a population, each with the same size.\nExplanation:\nThe rep_slice_sample() function takes repeated samples of the same size from a population, allowing for the simulation of sampling variability.\n\n(7.4) Why is the standard error calculated when taking samples from a population?\nCorrect Answer:\nD. To measure how much sample proportions vary from the population proportion.\nExplanation:\nThe standard error measures the variability in the sample proportions and indicates how close the sample proportions are to the true population proportion.\n\n(7.5) How does increasing the sample size affect the standard error of the sample proportions?\nCorrect Answer:\nB. It decreases the standard error, leading to more precise estimates of the population proportion.\nExplanation:\nLarger sample sizes reduce the variability in sample proportions, which decreases the standard error and improves the precision of the estimate of the population proportion.\n\n\n\n\n\n\n\n\n# Create a tibble of 9500 adults and their corresponding commute times \n# in minutes\n# This acts as a population of adults and their commute times\ncommute_data &lt;- tibble(\n  person_ID = 1:9500,\n  commute_time = rnorm(n = 9500, mean = 30, sd = 10)\n)\n\n\n\n\n\n# Choose sample size\nsample_size &lt;- 100\n\n# Generate a sample\nset.seed(2024)\ncommute_sample &lt;- commute_data |&gt; \n  slice_sample(n = sample_size, replace = FALSE)\n\n# Calculate the sample mean\nsample_mean &lt;- commute_sample |&gt; \n  summarize(mean(commute_time)) |&gt; \n  pull()\nsample_mean\n\n[1] 29.16929\n\n# Calculate the standard deviation \nsample_sd &lt;- commute_sample |&gt; \n  summarize(sd(commute_time)) |&gt; \n  pull()\nsample_sd\n\n[1] 10.06214\n\n\n\n\n\n\n# Calculate the population mean\npopulation_mean &lt;- commute_data |&gt; \n  summarize(mean(commute_time)) |&gt; \n  pull()\nmu &lt;- population_mean\nmu\n\n[1] 29.98266\n\n# Calculate the population standard deviation\npopulation_sd &lt;- commute_data |&gt; \n  summarize(sd(commute_time)) |&gt; \n  pull()\nsigma &lt;- population_sd\nsigma\n\n[1] 10.02985\n\n\n\n\n\n\n# Calculate the margin of error\nz_star &lt;- qnorm(p = 0.975) # Assumes a normal distribution\nmargin_of_error &lt;- z_star * (sigma / sqrt(sample_size))\n\n# Recall the point estimate\npoint_estimate &lt;- sample_mean\n\n# Calculate the confidence interval\nlower_bound &lt;- point_estimate - margin_of_error\nupper_bound &lt;- point_estimate + margin_of_error\n\n# Display the confidence interval\nc(lower_bound, upper_bound)\n\n[1] 27.20347 31.13510\n\n# Remember the population parameter (we usually don't know it)\nbetween(mu, lower_bound, upper_bound) # dplyr::between()\n\n[1] TRUE\n\n\n\n\n\n\n# Calculate the margin of error\nt_star &lt;- qt(p = 0.975, df = sample_size - 1) # t-distribution\nmargin_of_error_t &lt;- t_star * (sample_sd / sqrt(sample_size))\n\n# Same point estimate\npoint_estimate_t &lt;- sample_mean\n\n# Calculate the confidence interval\nlower_bound_t &lt;- point_estimate_t - margin_of_error_t\nupper_bound_t &lt;- point_estimate_t + margin_of_error_t\n\n# Display the confidence interval\nc(lower_bound_t, upper_bound_t)\n\n[1] 27.17274 31.16584\n\n# Remember the population parameter (we usually don't know it)\nbetween(mu, lower_bound_t, upper_bound_t)\n\n[1] TRUE\n\n\n\n\n\nWe are “95% confident” that the true mean commute time for this population of adults is between 27.1727413 and 31.1658354 minutes. This is the same as saying that if we were to take many samples of size 100 and calculate the confidence interval for each sample, we would expect 95% of those intervals to contain the true population mean of 29.9826594.\n\n\n\n\n(8.1) What does the sample mean represent in general?\nA. The mean for the entire population.\nB. The mean for the smaller collection from the larger group of interest.\nC. The mean for those outside the sample.\nD. The population parameter.\n\n(8.2) Which of the following describes the purpose of calculating a margin of error?\nA. To estimate the standard deviation of the sample.\nB. To account for the variability in sample means and create a confidence interval.\nC. To find the population mean directly.\nD. To calculate the proportion of people with a commute time under the sample mean.\n\n(8.3) How is the margin of error calculated when the population standard deviation is known?\nA. Using a t-distribution and the sample standard deviation.\nB. Using a z-distribution and the sample standard deviation.\nC. Using a z-distribution and the population standard deviation.\nD. Using a t-distribution and the population standard deviation.\n\n(8.4) When using the \\(t\\)-distribution for confidence intervals, why is it used instead of the \\(z\\)-distribution?\nA. The t-distribution adjusts for larger sample sizes.\nB. The t-distribution is used when the sample standard deviation is smaller than the population standard deviation. C. The t-distribution accounts for the population mean being known.\nD. The t-distribution is used when the population standard deviation is unknown.\n\n(8.5) What does it mean to be “95% confident” in the confidence interval calculated?\nA. That 95% of similarly constructed confidence intervals from repeated samples would contain the true population mean. B. That the population mean is exactly equal to the sample mean.\nC. That 95% of the data points in the sample fall within the interval.\nD. That 95% of the sample means from different samples will be the same as the sample mean in this confidence interval.\n\n\n\n\n(8.1) What does the sample mean represent in this context of commute times?\nCorrect Answer:\nB. The mean for the smaller collection from the larger group of interest.\nExplanation:\nThe sample mean is the average for those in the sample.\n\n(8.2) Which of the following describes the purpose of calculating a margin of error?\nCorrect Answer:\nB. To account for the variability in sample means and create a confidence interval.\nExplanation:\nThe margin of error helps account for the natural variation in different samples and is used to create a range (confidence interval) around the sample mean where the true population mean likely falls.\n\n(8.3) How is the margin of error calculated when the population standard deviation is known?\nCorrect Answer:\nC. Using a z-distribution and the population standard deviation.\nExplanation:\nWhen the population standard deviation is known, the margin of error is calculated using the z-distribution, reflecting normal distribution assumptions.\n\n(8.4) When using the t-distribution for confidence intervals, why is it used instead of the z-distribution?\nCorrect Answer:\nD. The t-distribution is used when the population standard deviation is unknown.\nExplanation:\nThe t-distribution is used when we do not know the population standard deviation, and it adjusts for the added uncertainty in estimating this value using the sample standard deviation.\n\n(8.5) What does it mean to be “95% confident” in the confidence interval calculated?\nCorrect Answer:\nA. That 95% of similarly constructed confidence intervals from repeated samples would contain the true population mean.\nExplanation:\nBeing 95% confident means that if we repeatedly took samples and calculated confidence intervals, 95% of those intervals would capture the true population mean.\n\n\n\n\n\n\n\n\n# Assume we only have a sample of 100 adults and their commute times\ncommute_sample\n\n# A tibble: 100 × 2\n   person_ID commute_time\n       &lt;int&gt;        &lt;dbl&gt;\n 1      5698         63.2\n 2      4645         20.0\n 3      3488         18.6\n 4      8297         31.1\n 5      7802         41.8\n 6      1035         24.4\n 7      6045         32.7\n 8      7230         19.3\n 9      7420         12.0\n10      8590         27.5\n# ℹ 90 more rows\n\n\n\n\n\n\n\n\n\ninfer_framework\n\n\n\n\n\n\n\n# Bootstrapping the sample\nset.seed(2024)\none_bootstrap &lt;- commute_sample |&gt; \n  specify(response = commute_time) |&gt; \n  generate(reps = 1, type = \"bootstrap\")\none_bootstrap\n\nResponse: commute_time (numeric)\n# A tibble: 100 × 2\n# Groups:   replicate [1]\n   replicate commute_time\n       &lt;int&gt;        &lt;dbl&gt;\n 1         1         34.1\n 2         1         40.7\n 3         1         31.5\n 4         1         27.3\n 5         1         42.2\n 6         1         42.3\n 7         1         25.6\n 8         1         51.4\n 9         1         19.7\n10         1         43.9\n# ℹ 90 more rows\n\n\n\n\n\n\n\n# Calculate the mean of the bootstrap sample\none_bootstrap |&gt; \n  calculate(stat = \"mean\") |&gt; \n  pull()\n\n[1] 29.51807\n\n\n\n\n\n\n\n# Bootstrapping 1000 samples\nmany_bootstraps &lt;- commute_sample |&gt; \n  specify(response = commute_time) |&gt; \n  generate(reps = 1000, type = \"bootstrap\")\nmany_bootstraps\n\nResponse: commute_time (numeric)\n# A tibble: 100,000 × 2\n# Groups:   replicate [1,000]\n   replicate commute_time\n       &lt;int&gt;        &lt;dbl&gt;\n 1         1         27.7\n 2         1         27.9\n 3         1         20.3\n 4         1         22.9\n 5         1         31.2\n 6         1         25.8\n 7         1         33.9\n 8         1         19.3\n 9         1         27.8\n10         1         25.8\n# ℹ 99,990 more rows\n\n\n\n\n\n\n# Calculate the mean of each bootstrap sample\nbootstrap_means &lt;- many_bootstraps |&gt; \n  calculate(stat = \"mean\")\nbootstrap_means\n\nResponse: commute_time (numeric)\n# A tibble: 1,000 × 2\n   replicate  stat\n       &lt;int&gt; &lt;dbl&gt;\n 1         1  30.1\n 2         2  28.9\n 3         3  30.0\n 4         4  30.2\n 5         5  28.4\n 6         6  28.8\n 7         7  29.8\n 8         8  30.2\n 9         9  28.9\n10        10  29.6\n# ℹ 990 more rows\n\n\n\n\n\n\n\n# Create a histogram of the bootstrap means\nggplot(bootstrap_means, aes(x = stat)) +\n  geom_histogram(bins = 30, color = \"white\") +\n  labs(x = \"Bootstrap sample mean\", \n       title = \"Histogram of 1000 bootstrap sample means\")\n\n\n\n\n\n\n\n\n\n# Calculate the bootstrap confidence interval in two ways since bell-shaped\nbootstrap_percentile_ci &lt;- bootstrap_means |&gt; \n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\nbootstrap_se_ci &lt;- bootstrap_means |&gt; \n  get_confidence_interval(level = 0.95, \n                          type = \"se\",\n                          point_estimate = sample_mean)\n\n\n\n\n\nWe are “95% confident” that the true mean commute time for this population of adults is between 27.3707695 and 31.1042937 minutes. This is the same as saying that if we were to take many samples of size 100 and calculate the confidence interval for each sample, we would expect 95% of those intervals to contain the true population mean of 29.9826594.\n\n\n\n\n\n# Show the histogram of bootstrap means with the confidence interval\n# and the population parameter (not usually known)\nbootstrap_means |&gt; \n  visualize() +\n  shade_confidence_interval(endpoints = bootstrap_percentile_ci) +\n  geom_vline(xintercept = mu, color = \"purple\", linewidth = 2)\n\n\n\n\n\n\n\n\n(9.1) What is the purpose of bootstrapping?\nA. To calculate the exact population mean from a sample.\nB. To generate multiple samples from the population without replacement.\nC. To estimate the sampling distribution of the sample mean by resampling the original sample.\nD. To directly calculate the confidence interval without using the sample data.\n\n(9.2) In the bootstrapping process, what does the generate(reps = 1000, type = \"bootstrap\") function do?\nA. It creates 1000 random samples from the original population.\nB. It creates 1000 random samples with replacement from the original sample.\nC. It creates 1000 exact copies of the population.\nD. It creates 1000 different statistics from the original population.\n\n(9.3) What does the histogram of bootstrap sample means represent?\nA. The distribution of sample means from the 1000 bootstrap samples.\nB. The distribution of values in the original population.\nC. The distribution of the population means calculated from the original sample.\nD. The actual population mean with 95% certainty.\n\n(9.4) How is the bootstrap percentile confidence interval calculated?\nA. By calculating the standard deviation of the bootstrap samples.\nB. By using the \\(t\\)-distribution to calculate the margin of error.\nC. By taking the 2.5th and 97.5th percentiles of the bootstrap sample means.\nD. By calculating the \\(z\\)-distribution based on the sample size.\n\n(9.5) What does it mean to be “95% confident” in the bootstrap confidence interval?\nA. That 95% of the bootstrap samples contain the population mean.\nB. That the true population mean lies within the interval for 95% of all bootstrap samples.\nC. That 95% of the population values fall within the confidence interval. D. That if we repeated the bootstrapping process many times, 95% of the confidence intervals would contain the true population mean.\n\n\n\n\n(9.1) What is the purpose of bootstrapping in the context of this commute time example?\nCorrect Answer:\nC. To estimate the sampling distribution of the sample mean by resampling the original sample.\nExplanation:\nBootstrapping allows us to approximate the sampling distribution by repeatedly resampling the original sample with replacement and calculating the statistic of interest (in this case, the mean).\n\n(9.2) In the bootstrapping process, what does the generate(reps = 1000, type = \"bootstrap\") function do?\nCorrect Answer:\nA. It generates 1000 random samples with replacement from the original sample.\nExplanation:\nThe generate() function resamples the original data with replacement to create 1000 new bootstrap samples, which are used to estimate the sampling distribution of the sample mean.\n\n(9.3) What does the histogram of bootstrap sample means represent?\nCorrect Answer:\nB. The distribution of sample means from the 1000 bootstrap samples.\nExplanation:\nThe histogram shows the variability in sample means across the 1000 bootstrap samples, giving insight into the sampling distribution of the sample mean.\n\n(9.4) How is the bootstrap percentile confidence interval calculated?\nCorrect Answer:\nC. By taking the 2.5th and 97.5th percentiles of the bootstrap sample means.\nExplanation:\nThe bootstrap percentile confidence interval is found by identifying the lower and upper bounds at the 2.5th and 97.5th percentiles of the bootstrap sample means.\n\n(9.5) What does it mean to be “95% confident” in the bootstrap confidence interval?\nCorrect Answer:\nD. That if we repeated the bootstrapping process many times, 95% of the confidence intervals would contain the true population mean.\nExplanation:\nBeing “95% confident” means that if we repeatedly generated bootstrap samples and calculated confidence intervals, 95% of those intervals would capture the true population mean."
  },
  {
    "objectID": "answers/day3_walkthrough_answers.html#session-7-sampling",
    "href": "answers/day3_walkthrough_answers.html#session-7-sampling",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# Load the required packages\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(infer)\n\n\nThese packages provide tools for data wrangling, visualization, modeling, and inference.\n\n\n\n\n\n\n# For reproducibility\nset.seed(2024)\n\n# Create vectors of sport ball types and their proportions\ntypes_of_sport_balls &lt;- c(\"Basketball\", \"Pickleball\", \"Tennis Ball\", \n                          \"Football/Soccer\", \"American Football\")\nproportion_of_each_type &lt;- c(0.2, 0.15, 0.3, 0.25, 0.1)\n\n# Create a tibble of 1200 sport balls that will act as our population\nstore_ball_inventory &lt;- tibble(\n  ball_ID = 1:1200,\n  ball_type = sample(x = types_of_sport_balls,\n                     size = 1200, \n                     replace = TRUE, \n                     prob = proportion_of_each_type\n  )\n)\n\n\nWe’ll be exploring a synthesized data set of an inventory at a large sporting goods store. The inventory pertains to different types of sports balls, 1200 in total.\n\n\n\n\n\n\n# Use glimpse to explore the structure of the dataset\nglimpse(store_ball_inventory)\n\nRows: 1,200\nColumns: 2\n$ ball_ID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 2…\n$ ball_type &lt;chr&gt; \"Pickleball\", \"Football/Soccer\", \"Basketball\", \"Basketball\", \"Football/Soccer\", \"Basketball\", \"Footb…\n\n\n\n\n\n\n\n# Create a count of ball_type\nstore_ball_inventory |&gt; \n  count(ball_type)\n\n# A tibble: 5 × 2\n  ball_type             n\n  &lt;chr&gt;             &lt;int&gt;\n1 American Football   117\n2 Basketball          252\n3 Football/Soccer     299\n4 Pickleball          187\n5 Tennis Ball         345\n\n# Determine the proportion of pickleballs in the inventory\np_df &lt;- store_ball_inventory |&gt; \n  summarize(prop_pickle = mean(ball_type == \"Pickleball\"))\n\n# Convert p to a numeric value\np &lt;- p_df$prop_pickle\n\n# Or using the tidyverse\np &lt;- p_df |&gt; pull(prop_pickle)\n\n\n\n\n\n\n# Retrieve a sample of 50 balls from the inventory\nball_sample &lt;- store_ball_inventory |&gt; \n  slice_sample(n = 50, replace = FALSE)\n\n# Determine the proportion of pickleballs in the sample\nball_sample |&gt; \n  summarize(prop_pickle = mean(ball_type == \"Pickleball\"))\n\n# A tibble: 1 × 1\n  prop_pickle\n        &lt;dbl&gt;\n1        0.14\n\n\n\n\n\n\n\n# Retrieve another sample of 50 balls from the inventory\nball_sample2 &lt;- store_ball_inventory |&gt; \n  slice_sample(n = 50, replace = FALSE)\n\n# Determine the proportion of pickleballs in the sample\nball_sample2 |&gt; \n  summarize(prop_pickle = mean(ball_type == \"Pickleball\"))\n\n# A tibble: 1 × 1\n  prop_pickle\n        &lt;dbl&gt;\n1         0.1\n\n\n\n\n\n\n\n# Use `rep_slice_sample()` from the `infer` package\nball_samples &lt;- store_ball_inventory |&gt; \n  rep_slice_sample(n = 50, reps = 1000, replace = FALSE)\n\n\n\n\n\n\n# Determine sample proportions with `dplyr`\nprops_pickle &lt;- ball_samples |&gt; \n  summarize(prop_pickle = mean(ball_type == \"Pickleball\"))\n\n# Create a histogram of the sample proportions\nggplot(props_pickle, aes(x = prop_pickle)) +\n  geom_histogram(bins = 15, color = \"white\") +\n  labs(x = \"Sample proportion\", \n       title = \"Histogram of 1000 sample proportions of Pickleballs\") \n\n\n\n\n\n\n\n\n\n# Using the simulations, calculate the standard deviation of the \n# sample proportions\nse_sample_props &lt;- props_pickle |&gt; \n  summarize(sd(prop_pickle)) |&gt; \n  pull()\n\n# Using the formula for the standard error of a sample proportion\nn &lt;- 50\nse_sample_props_formula &lt;- sqrt(p * (1 - p) / n)\n\n\n\n\n\n\n# Create a function to calculate the standard error of sample proportions\n# using simulation\nse_sample_props &lt;- function(size, repetitions = 1000, type = \"Pickleball\") {\n  props &lt;- store_ball_inventory |&gt; \n    rep_slice_sample(n = size, reps = repetitions, replace = FALSE) |&gt; \n    summarize(prop = mean(ball_type == type))\n  \n  se_sample_props &lt;- props |&gt; \n    summarize(sd(prop)) |&gt; \n    pull()\n  \n  return(se_sample_props)\n}\n\n# Standard errors for different sample sizes\nse_sample_props_20 &lt;- se_sample_props(20)\nse_sample_props_50 &lt;- se_sample_props(50)\nse_sample_props_100 &lt;- se_sample_props(100)\n\n\n\n\n(1.1) What is the purpose of using the sample() function in the code provided?\nA. To randomly select a subset of the population without replacement.\nB. To calculate the mean value of a numeric variable.\nC. To remove missing values from the dataset.\nD. To calculate the population parameters.\n\n(1.2) In the context of the sporting goods store example, what does the sample proportion of pickleballs represent?\nA. The actual number of pickleballs in the entire inventory.\nB. The proportion of pickleballs in a random sample taken from the inventory.\nC. The total number of all types of sport balls in the inventory.\nD. The probability of not selecting a pickleball from the population.\n\n(1.3) What does the function rep_slice_sample() do in the sampling process?\nA. It generates multiple samples from a population, each with the same size.\nB. It creates a histogram of sample proportions.\nC. It removes duplicate samples from the dataset.\nD. It replicates the population to simulate different proportions.\n\n(1.4) Why is the standard error calculated when taking samples from a population?\nA. To ensure that the sample is randomly selected. B. To estimate the total number of items in the population.\nC. To adjust the sample size for better accuracy.\nD. To measure how much sample proportions vary from the population proportion.\n\n(1.5) How does increasing the sample size affect the standard error of the sample proportions?\nA. It increases the standard error because more data points create more variation.\nB. It decreases the standard error, leading to more precise estimates of the population proportion.\nC. It has no effect on the standard error.\nD. It changes the population proportion directly.\n\n\n\n\n(7.1) What is the purpose of using the sample() function in the code provided?\nCorrect Answer:\nA. To randomly select a subset of the population without replacement.\nExplanation:\nThe sample() function is used to randomly select a subset of observations from the population, which helps create samples for analysis.\n\n(7.2) In the context of the sporting goods store example, what does the sample proportion of pickleballs represent?\nCorrect Answer:\nB. The proportion of pickleballs in a random sample taken from the inventory.\nExplanation:\nThe sample proportion reflects how many pickleballs appear in a random sample of the inventory, not the entire population.\n\n(7.3) What does the function rep_slice_sample() do in the sampling process?\nCorrect Answer:\nA. It generates multiple samples from a population, each with the same size.\nExplanation:\nThe rep_slice_sample() function takes repeated samples of the same size from a population, allowing for the simulation of sampling variability.\n\n(7.4) Why is the standard error calculated when taking samples from a population?\nCorrect Answer:\nD. To measure how much sample proportions vary from the population proportion.\nExplanation:\nThe standard error measures the variability in the sample proportions and indicates how close the sample proportions are to the true population proportion.\n\n(7.5) How does increasing the sample size affect the standard error of the sample proportions?\nCorrect Answer:\nB. It decreases the standard error, leading to more precise estimates of the population proportion.\nExplanation:\nLarger sample sizes reduce the variability in sample proportions, which decreases the standard error and improves the precision of the estimate of the population proportion."
  },
  {
    "objectID": "answers/day3_walkthrough_answers.html#session-8-estimation-using-theory-based-methods",
    "href": "answers/day3_walkthrough_answers.html#session-8-estimation-using-theory-based-methods",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# Create a tibble of 9500 adults and their corresponding commute times \n# in minutes\n# This acts as a population of adults and their commute times\ncommute_data &lt;- tibble(\n  person_ID = 1:9500,\n  commute_time = rnorm(n = 9500, mean = 30, sd = 10)\n)\n\n\n\n\n\n# Choose sample size\nsample_size &lt;- 100\n\n# Generate a sample\nset.seed(2024)\ncommute_sample &lt;- commute_data |&gt; \n  slice_sample(n = sample_size, replace = FALSE)\n\n# Calculate the sample mean\nsample_mean &lt;- commute_sample |&gt; \n  summarize(mean(commute_time)) |&gt; \n  pull()\nsample_mean\n\n[1] 29.16929\n\n# Calculate the standard deviation \nsample_sd &lt;- commute_sample |&gt; \n  summarize(sd(commute_time)) |&gt; \n  pull()\nsample_sd\n\n[1] 10.06214\n\n\n\n\n\n\n# Calculate the population mean\npopulation_mean &lt;- commute_data |&gt; \n  summarize(mean(commute_time)) |&gt; \n  pull()\nmu &lt;- population_mean\nmu\n\n[1] 29.98266\n\n# Calculate the population standard deviation\npopulation_sd &lt;- commute_data |&gt; \n  summarize(sd(commute_time)) |&gt; \n  pull()\nsigma &lt;- population_sd\nsigma\n\n[1] 10.02985\n\n\n\n\n\n\n# Calculate the margin of error\nz_star &lt;- qnorm(p = 0.975) # Assumes a normal distribution\nmargin_of_error &lt;- z_star * (sigma / sqrt(sample_size))\n\n# Recall the point estimate\npoint_estimate &lt;- sample_mean\n\n# Calculate the confidence interval\nlower_bound &lt;- point_estimate - margin_of_error\nupper_bound &lt;- point_estimate + margin_of_error\n\n# Display the confidence interval\nc(lower_bound, upper_bound)\n\n[1] 27.20347 31.13510\n\n# Remember the population parameter (we usually don't know it)\nbetween(mu, lower_bound, upper_bound) # dplyr::between()\n\n[1] TRUE\n\n\n\n\n\n\n# Calculate the margin of error\nt_star &lt;- qt(p = 0.975, df = sample_size - 1) # t-distribution\nmargin_of_error_t &lt;- t_star * (sample_sd / sqrt(sample_size))\n\n# Same point estimate\npoint_estimate_t &lt;- sample_mean\n\n# Calculate the confidence interval\nlower_bound_t &lt;- point_estimate_t - margin_of_error_t\nupper_bound_t &lt;- point_estimate_t + margin_of_error_t\n\n# Display the confidence interval\nc(lower_bound_t, upper_bound_t)\n\n[1] 27.17274 31.16584\n\n# Remember the population parameter (we usually don't know it)\nbetween(mu, lower_bound_t, upper_bound_t)\n\n[1] TRUE\n\n\n\n\n\nWe are “95% confident” that the true mean commute time for this population of adults is between 27.1727413 and 31.1658354 minutes. This is the same as saying that if we were to take many samples of size 100 and calculate the confidence interval for each sample, we would expect 95% of those intervals to contain the true population mean of 29.9826594.\n\n\n\n\n(8.1) What does the sample mean represent in general?\nA. The mean for the entire population.\nB. The mean for the smaller collection from the larger group of interest.\nC. The mean for those outside the sample.\nD. The population parameter.\n\n(8.2) Which of the following describes the purpose of calculating a margin of error?\nA. To estimate the standard deviation of the sample.\nB. To account for the variability in sample means and create a confidence interval.\nC. To find the population mean directly.\nD. To calculate the proportion of people with a commute time under the sample mean.\n\n(8.3) How is the margin of error calculated when the population standard deviation is known?\nA. Using a t-distribution and the sample standard deviation.\nB. Using a z-distribution and the sample standard deviation.\nC. Using a z-distribution and the population standard deviation.\nD. Using a t-distribution and the population standard deviation.\n\n(8.4) When using the \\(t\\)-distribution for confidence intervals, why is it used instead of the \\(z\\)-distribution?\nA. The t-distribution adjusts for larger sample sizes.\nB. The t-distribution is used when the sample standard deviation is smaller than the population standard deviation. C. The t-distribution accounts for the population mean being known.\nD. The t-distribution is used when the population standard deviation is unknown.\n\n(8.5) What does it mean to be “95% confident” in the confidence interval calculated?\nA. That 95% of similarly constructed confidence intervals from repeated samples would contain the true population mean. B. That the population mean is exactly equal to the sample mean.\nC. That 95% of the data points in the sample fall within the interval.\nD. That 95% of the sample means from different samples will be the same as the sample mean in this confidence interval.\n\n\n\n\n(8.1) What does the sample mean represent in this context of commute times?\nCorrect Answer:\nB. The mean for the smaller collection from the larger group of interest.\nExplanation:\nThe sample mean is the average for those in the sample.\n\n(8.2) Which of the following describes the purpose of calculating a margin of error?\nCorrect Answer:\nB. To account for the variability in sample means and create a confidence interval.\nExplanation:\nThe margin of error helps account for the natural variation in different samples and is used to create a range (confidence interval) around the sample mean where the true population mean likely falls.\n\n(8.3) How is the margin of error calculated when the population standard deviation is known?\nCorrect Answer:\nC. Using a z-distribution and the population standard deviation.\nExplanation:\nWhen the population standard deviation is known, the margin of error is calculated using the z-distribution, reflecting normal distribution assumptions.\n\n(8.4) When using the t-distribution for confidence intervals, why is it used instead of the z-distribution?\nCorrect Answer:\nD. The t-distribution is used when the population standard deviation is unknown.\nExplanation:\nThe t-distribution is used when we do not know the population standard deviation, and it adjusts for the added uncertainty in estimating this value using the sample standard deviation.\n\n(8.5) What does it mean to be “95% confident” in the confidence interval calculated?\nCorrect Answer:\nA. That 95% of similarly constructed confidence intervals from repeated samples would contain the true population mean.\nExplanation:\nBeing 95% confident means that if we repeatedly took samples and calculated confidence intervals, 95% of those intervals would capture the true population mean."
  },
  {
    "objectID": "answers/day3_walkthrough_answers.html#session-9-estimation-using-bootstrapping-methods",
    "href": "answers/day3_walkthrough_answers.html#session-9-estimation-using-bootstrapping-methods",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# Assume we only have a sample of 100 adults and their commute times\ncommute_sample\n\n# A tibble: 100 × 2\n   person_ID commute_time\n       &lt;int&gt;        &lt;dbl&gt;\n 1      5698         63.2\n 2      4645         20.0\n 3      3488         18.6\n 4      8297         31.1\n 5      7802         41.8\n 6      1035         24.4\n 7      6045         32.7\n 8      7230         19.3\n 9      7420         12.0\n10      8590         27.5\n# ℹ 90 more rows\n\n\n\n\n\n\n\n\n\ninfer_framework\n\n\n\n\n\n\n\n# Bootstrapping the sample\nset.seed(2024)\none_bootstrap &lt;- commute_sample |&gt; \n  specify(response = commute_time) |&gt; \n  generate(reps = 1, type = \"bootstrap\")\none_bootstrap\n\nResponse: commute_time (numeric)\n# A tibble: 100 × 2\n# Groups:   replicate [1]\n   replicate commute_time\n       &lt;int&gt;        &lt;dbl&gt;\n 1         1         34.1\n 2         1         40.7\n 3         1         31.5\n 4         1         27.3\n 5         1         42.2\n 6         1         42.3\n 7         1         25.6\n 8         1         51.4\n 9         1         19.7\n10         1         43.9\n# ℹ 90 more rows\n\n\n\n\n\n\n\n# Calculate the mean of the bootstrap sample\none_bootstrap |&gt; \n  calculate(stat = \"mean\") |&gt; \n  pull()\n\n[1] 29.51807\n\n\n\n\n\n\n\n# Bootstrapping 1000 samples\nmany_bootstraps &lt;- commute_sample |&gt; \n  specify(response = commute_time) |&gt; \n  generate(reps = 1000, type = \"bootstrap\")\nmany_bootstraps\n\nResponse: commute_time (numeric)\n# A tibble: 100,000 × 2\n# Groups:   replicate [1,000]\n   replicate commute_time\n       &lt;int&gt;        &lt;dbl&gt;\n 1         1         27.7\n 2         1         27.9\n 3         1         20.3\n 4         1         22.9\n 5         1         31.2\n 6         1         25.8\n 7         1         33.9\n 8         1         19.3\n 9         1         27.8\n10         1         25.8\n# ℹ 99,990 more rows\n\n\n\n\n\n\n# Calculate the mean of each bootstrap sample\nbootstrap_means &lt;- many_bootstraps |&gt; \n  calculate(stat = \"mean\")\nbootstrap_means\n\nResponse: commute_time (numeric)\n# A tibble: 1,000 × 2\n   replicate  stat\n       &lt;int&gt; &lt;dbl&gt;\n 1         1  30.1\n 2         2  28.9\n 3         3  30.0\n 4         4  30.2\n 5         5  28.4\n 6         6  28.8\n 7         7  29.8\n 8         8  30.2\n 9         9  28.9\n10        10  29.6\n# ℹ 990 more rows\n\n\n\n\n\n\n\n# Create a histogram of the bootstrap means\nggplot(bootstrap_means, aes(x = stat)) +\n  geom_histogram(bins = 30, color = \"white\") +\n  labs(x = \"Bootstrap sample mean\", \n       title = \"Histogram of 1000 bootstrap sample means\")\n\n\n\n\n\n\n\n\n\n# Calculate the bootstrap confidence interval in two ways since bell-shaped\nbootstrap_percentile_ci &lt;- bootstrap_means |&gt; \n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\nbootstrap_se_ci &lt;- bootstrap_means |&gt; \n  get_confidence_interval(level = 0.95, \n                          type = \"se\",\n                          point_estimate = sample_mean)\n\n\n\n\n\nWe are “95% confident” that the true mean commute time for this population of adults is between 27.3707695 and 31.1042937 minutes. This is the same as saying that if we were to take many samples of size 100 and calculate the confidence interval for each sample, we would expect 95% of those intervals to contain the true population mean of 29.9826594.\n\n\n\n\n\n# Show the histogram of bootstrap means with the confidence interval\n# and the population parameter (not usually known)\nbootstrap_means |&gt; \n  visualize() +\n  shade_confidence_interval(endpoints = bootstrap_percentile_ci) +\n  geom_vline(xintercept = mu, color = \"purple\", linewidth = 2)\n\n\n\n\n\n\n\n\n(9.1) What is the purpose of bootstrapping?\nA. To calculate the exact population mean from a sample.\nB. To generate multiple samples from the population without replacement.\nC. To estimate the sampling distribution of the sample mean by resampling the original sample.\nD. To directly calculate the confidence interval without using the sample data.\n\n(9.2) In the bootstrapping process, what does the generate(reps = 1000, type = \"bootstrap\") function do?\nA. It creates 1000 random samples from the original population.\nB. It creates 1000 random samples with replacement from the original sample.\nC. It creates 1000 exact copies of the population.\nD. It creates 1000 different statistics from the original population.\n\n(9.3) What does the histogram of bootstrap sample means represent?\nA. The distribution of sample means from the 1000 bootstrap samples.\nB. The distribution of values in the original population.\nC. The distribution of the population means calculated from the original sample.\nD. The actual population mean with 95% certainty.\n\n(9.4) How is the bootstrap percentile confidence interval calculated?\nA. By calculating the standard deviation of the bootstrap samples.\nB. By using the \\(t\\)-distribution to calculate the margin of error.\nC. By taking the 2.5th and 97.5th percentiles of the bootstrap sample means.\nD. By calculating the \\(z\\)-distribution based on the sample size.\n\n(9.5) What does it mean to be “95% confident” in the bootstrap confidence interval?\nA. That 95% of the bootstrap samples contain the population mean.\nB. That the true population mean lies within the interval for 95% of all bootstrap samples.\nC. That 95% of the population values fall within the confidence interval. D. That if we repeated the bootstrapping process many times, 95% of the confidence intervals would contain the true population mean.\n\n\n\n\n(9.1) What is the purpose of bootstrapping in the context of this commute time example?\nCorrect Answer:\nC. To estimate the sampling distribution of the sample mean by resampling the original sample.\nExplanation:\nBootstrapping allows us to approximate the sampling distribution by repeatedly resampling the original sample with replacement and calculating the statistic of interest (in this case, the mean).\n\n(9.2) In the bootstrapping process, what does the generate(reps = 1000, type = \"bootstrap\") function do?\nCorrect Answer:\nA. It generates 1000 random samples with replacement from the original sample.\nExplanation:\nThe generate() function resamples the original data with replacement to create 1000 new bootstrap samples, which are used to estimate the sampling distribution of the sample mean.\n\n(9.3) What does the histogram of bootstrap sample means represent?\nCorrect Answer:\nB. The distribution of sample means from the 1000 bootstrap samples.\nExplanation:\nThe histogram shows the variability in sample means across the 1000 bootstrap samples, giving insight into the sampling distribution of the sample mean.\n\n(9.4) How is the bootstrap percentile confidence interval calculated?\nCorrect Answer:\nC. By taking the 2.5th and 97.5th percentiles of the bootstrap sample means.\nExplanation:\nThe bootstrap percentile confidence interval is found by identifying the lower and upper bounds at the 2.5th and 97.5th percentiles of the bootstrap sample means.\n\n(9.5) What does it mean to be “95% confident” in the bootstrap confidence interval?\nCorrect Answer:\nD. That if we repeated the bootstrapping process many times, 95% of the confidence intervals would contain the true population mean.\nExplanation:\nBeing “95% confident” means that if we repeatedly generated bootstrap samples and calculated confidence intervals, 95% of those intervals would capture the true population mean."
  },
  {
    "objectID": "answers/day1_walkthrough_answers.html",
    "href": "answers/day1_walkthrough_answers.html",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "You need to install R first from https://cloud.r-project.org/ and then install RStudio from https://posit.co/download/rstudio-desktop/.\nOnce installed, work in RStudio to interact with R efficiently.\n\n\n\n\n\n\nIn RStudio, you will see three panes: Console, Environment, and Files.\n\nConsole: This is where you write and run your code.\nEnvironment: Keeps track of all the objects, like data frames or variables, that you create.\nFiles: Helps you navigate through your project files and folders, similar to how you browse files on your computer.\n\nThe Console is where you type and run your R code.\n\nYou can type a command and press Enter to see the result immediately.\nIf you need to run multiple lines of code, you can write them in the Source Editor (another pane you’ll see later) and send them to the Console.\nThe Console also shows any error messages or outputs when your code runs.\n\nThe Environment pane shows all objects (like datasets) currently in memory.\n\nAs you create variables or load datasets, they’ll appear here, making it easy to keep track of what’s currently active in your session.\nYou can click on any object to inspect it or open it for a detailed view.\nIf your workspace gets cluttered, you can clear it by using the broom icon to remove objects no longer needed.\n\nThe Files pane helps you navigate files in your project.\n\nHere, you can open scripts, view data files, and manage folders related to your project.\nYou can upload files into RStudio, set your working directory, or view the file contents directly.\nRight-click on files to see options like opening, renaming, or deleting files as part of your project management.\n\n\n\n\n\n\n\n# To install a package, use the install.packages() function.\ninstall.packages(\"moderndive\")\ninstall.packages(\"dplyr\")\n\n# or together using the c() function:\n# install.packages(c(\"moderndive\", \"dplyr\"))\n\nThe install.packages() function downloads and installs packages from CRAN (the Comprehensive R Archive Network), allowing you to access additional tools and functions in R. For example, here we’re installing the moderndive and dplyr packages.\nThe c() function is used to create a vector in R, which is essentially a collection or sequence of elements. In this case, it’s combining the package names into a single vector, so you can install multiple packages with one command instead of typing install.packages() multiple times. This helps make your code cleaner and more efficient.\n\n\n\n\n\n# To load a package, use the library() function.\nlibrary(moderndive)\nlibrary(dplyr)\n\n\n# TIP: Check help with `?` operator or `help()` function.\n?library\nhelp(library)\n\nOnce you’ve installed a package using install.packages(), you need to load it into your current R session to access its functions and features. The library() function does just that—it activates the package so you can use its tools. In this example, we’re loading both the moderndive and dplyr packages.\nLoading a package is essential because installing a package only happens once, but you need to load it every time you start a new R session in RStudio and want to use that package.\nIf you ever need more information about how a function or package works, you can use the ? operator or the help() function.\n\n?library opens the help documentation for the library() function, giving you details on what it does, how to use it, and its arguments.\nhelp(library) does the same thing, providing a full description of the function or topic you need assistance with.\n\nThis is a great way to explore R’s built-in help system and get immediate answers or examples while coding.\n\n\n\n\n\n# Can load the data into the Environment pane\ndata(\"un_member_states_2024\")\n\n# To view a dataset in RStudio, use the View() function\nView(un_member_states_2024)\n\nThis opens a spreadsheet-like viewer in RStudio to explore the dataset. This is what is known as a data frame in R. More specifically, this is a special kind of data frame called a tibble.\nThe un_member_states_2024 data contains information on 193 UN member states, with 39 columns capturing various aspects of each country. These columns include details such as the country name, ISO codes, continent, GDP per capita, population, life expectancy, and Olympic participation. It provides a comprehensive dataset for exploring demographic, economic, geographic, and social indicators at the country level.\n\nIdentification variables: country and iso are the clearest\nMeasurement variables: remaining\n\n\n\n\n\nEach column in a dataset is assigned a data type, which describes the kind of information that the column holds. Understanding the data types is important because it determines how R interprets and handles the data. Here are some common data types you’ll encounter in R:\n\nchr (character): Used for text data, such as names or descriptions.\ndbl (double): Represents numeric data with decimal values (floating-point numbers).\nlgl (logical): Holds TRUE or FALSE values, often used for conditions or binary data.\nint (integer): Stores whole numbers (without decimal points).\nfct (factor): Represents categorical data, where values belong to predefined categories or groups (e.g., “Male” and “Female” for gender).\n\nTo quickly check the structure of your dataset and see the data types of each column, you can use the glimpse() function from the dplyr package. This function provides a compact preview of the dataset, showing the name of each column, its data type, and the first few entries.\n\n# To see an overview of the dataset’s structure:\nglimpse(un_member_states_2024)\n\nRows: 193\nColumns: 39\n$ country                            &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andorra\", \"Angola\", \"Antigua and Barb…\n$ iso                                &lt;chr&gt; \"AFG\", \"ALB\", \"DZA\", \"AND\", \"AGO\", \"ATG\", \"ARG\", \"ARM\", \"AUS\", \"AUT\", \"AZE\"…\n$ official_state_name                &lt;chr&gt; \"The Islamic Republic of Afghanistan\", \"The Republic of Albania\", \"The Peop…\n$ continent                          &lt;fct&gt; Asia, Europe, Africa, Europe, Africa, North America, South America, Asia, O…\n$ region                             &lt;chr&gt; \"Southern and Central Asia\", \"Southern Europe\", \"Northern Africa\", \"Souther…\n$ capital_city                       &lt;chr&gt; \"Kabul\", \"Tirana\", \"Algiers\", \"Andorra la Vella\", \"Luanda\", \"St. John's\", \"…\n$ capital_population                 &lt;dbl&gt; 4601789, 557422, 3915811, 22873, 2571861, 22219, 3120612, 1096100, 431380, …\n$ capital_perc_of_country            &lt;dbl&gt; 11.5, 19.5, 8.9, 28.9, 7.5, 23.8, 6.9, 39.3, 1.7, 22.0, 22.3, 67.3, 13.7, 5…\n$ capital_data_year                  &lt;int&gt; 2021, 2011, 2011, 2022, 2020, 2011, 2022, 2021, 2020, 2022, 2022, 2016, 202…\n$ gdp_per_capita                     &lt;dbl&gt; 355.7778, 6810.1140, 4342.6380, 41992.7728, 3000.4442, 19919.7267, 13650.60…\n$ gdp_per_capita_year                &lt;dbl&gt; 2021, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 202…\n$ summers_competed_in                &lt;dbl&gt; 15, NA, 14, NA, NA, NA, 25, 7, 27, 28, 7, 17, 10, NA, 13, 7, 27, NA, NA, NA…\n$ summer_golds                       &lt;int&gt; 0, NA, 5, NA, NA, NA, 21, 2, 164, 20, 7, 8, 2, NA, 0, 13, 44, NA, NA, NA, N…\n$ summer_silvers                     &lt;int&gt; 0, NA, 4, NA, NA, NA, 26, 8, 173, 35, 14, 2, 2, NA, 0, 30, 56, NA, NA, NA, …\n$ summer_bronzes                     &lt;int&gt; 2, NA, 8, NA, NA, NA, 30, 8, 210, 41, 28, 6, 0, NA, 1, 42, 57, NA, NA, NA, …\n$ summer_total                       &lt;int&gt; 2, NA, 17, NA, NA, NA, 77, 18, 547, 96, 49, 16, 4, NA, 1, 85, 157, NA, NA, …\n$ winters_competed_in                &lt;int&gt; 0, NA, 3, NA, NA, NA, 20, 8, 20, 24, 7, 0, 0, NA, 0, 8, 22, NA, NA, NA, NA,…\n$ winter_golds                       &lt;int&gt; 0, NA, 0, NA, NA, NA, 0, 0, 6, 71, 0, 0, 0, NA, 0, 8, 2, NA, NA, NA, NA, NA…\n$ winter_silvers                     &lt;int&gt; 0, NA, 0, NA, NA, NA, 0, 0, 7, 88, 0, 0, 0, NA, 0, 7, 2, NA, NA, NA, NA, NA…\n$ winter_bronzes                     &lt;int&gt; 0, NA, 0, NA, NA, NA, 0, 0, 6, 91, 0, 0, 0, NA, 0, 5, 4, NA, NA, NA, NA, NA…\n$ winter_total                       &lt;int&gt; 0, NA, 0, NA, NA, NA, 0, 0, 19, 250, 0, 0, 0, NA, 0, 20, 8, NA, NA, NA, NA,…\n$ combined_competed_ins              &lt;int&gt; 15, NA, 17, NA, NA, NA, 45, 15, 47, 52, 14, 17, 10, NA, 13, 15, 49, NA, NA,…\n$ combined_golds                     &lt;int&gt; 0, NA, 5, NA, NA, NA, 21, 2, 170, 91, 7, 8, 2, NA, 0, 21, 46, NA, NA, NA, N…\n$ combined_silvers                   &lt;int&gt; 0, NA, 4, NA, NA, NA, 26, 8, 180, 123, 14, 2, 2, NA, 0, 37, 58, NA, NA, NA,…\n$ combined_bronzes                   &lt;int&gt; 2, NA, 8, NA, NA, NA, 30, 8, 216, 132, 28, 6, 0, NA, 1, 47, 61, NA, NA, NA,…\n$ combined_total                     &lt;int&gt; 2, NA, 17, NA, NA, NA, 77, 18, 566, 346, 49, 16, 4, NA, 1, 105, 165, NA, NA…\n$ driving_side                       &lt;chr&gt; \"Right-hand traffic\", \"Right-hand traffic\", \"Right-hand traffic\", \"Right-ha…\n$ obesity_rate_2024                  &lt;dbl&gt; 10.3, 21.3, 21.8, 13.6, 6.8, 43.1, 32.4, 19.5, 31.3, 17.1, 20.6, 43.7, 36.9…\n$ obesity_rate_2016                  &lt;dbl&gt; 5.5, 21.7, 27.4, 25.6, 8.2, 18.9, 28.3, 20.2, 29.0, 20.1, 19.9, 31.6, 29.8,…\n$ has_nuclear_weapons_2024           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ population_2024                    &lt;dbl&gt; 40121552, 3107100, 47022473, 85370, 37202061, 102634, 46994384, 2976765, 26…\n$ area_in_square_km                  &lt;dbl&gt; 652864, 28748, 2381741, 468, 1246700, 442, 2796427, 29743, 7692024, 83878, …\n$ area_in_square_miles               &lt;dbl&gt; 252072.0961, 11099.6603, 919594.9636, 180.6957, 481353.3634, 170.6571, 1079…\n$ population_density_in_square_km    &lt;dbl&gt; 61.454686, 108.080562, 19.742899, 182.414530, 29.840428, 232.203620, 16.805…\n$ population_density_in_square_miles &lt;dbl&gt; 159.166971, 279.927486, 51.133896, 472.451658, 77.286384, 601.404862, 43.52…\n$ income_group_2024                  &lt;fct&gt; Low income, Upper middle income, Lower middle income, High income, Lower mi…\n$ life_expectancy_2022               &lt;dbl&gt; 53.65, 79.47, 78.03, 83.42, 62.11, 77.80, 78.31, 76.13, 83.09, 82.27, 74.15…\n$ fertility_rate_2022                &lt;dbl&gt; 4.3, 1.4, 2.7, NA, 5.0, 1.6, 1.9, 1.6, 1.6, 1.5, 1.6, 1.4, 1.8, 1.9, 1.6, 1…\n$ hdi_2022                           &lt;dbl&gt; 0.462, 0.789, 0.745, 0.884, 0.591, 0.826, 0.849, 0.786, 0.946, 0.926, 0.760…\n\n\nIn this example, glimpse(un_member_states_2024) gives you a summary of the un_member_states_2024 dataset, allowing you to quickly inspect the data types and a snapshot of the content in each column. It’s a handy tool for understanding your data before diving into further analysis!\n\n\n\n\nTo access a specific column in a dataset, R provides a simple and efficient way to do so using the $ operator. This allows you to extract a single column from a data frame by specifying the column name.\n\n# To access a specific column, use the `$` operator:\nun_member_states_2024$country\n\n  [1] \"Afghanistan\"                     \"Albania\"                         \"Algeria\"                        \n  [4] \"Andorra\"                         \"Angola\"                          \"Antigua and Barbuda\"            \n  [7] \"Argentina\"                       \"Armenia\"                         \"Australia\"                      \n [10] \"Austria\"                         \"Azerbaijan\"                      \"Bahamas, The\"                   \n [13] \"Bahrain\"                         \"Bangladesh\"                      \"Barbados\"                       \n [16] \"Belarus\"                         \"Belgium\"                         \"Belize\"                         \n [19] \"Benin\"                           \"Bhutan\"                          \"Bolivia\"                        \n [22] \"Bosnia and Herzegovina\"          \"Botswana\"                        \"Brazil\"                         \n [25] \"Brunei\"                          \"Bulgaria\"                        \"Burkina Faso\"                   \n [28] \"Burundi\"                         \"Cabo Verde\"                      \"Cambodia\"                       \n [31] \"Cameroon\"                        \"Canada\"                          \"Central African Republic\"       \n [34] \"Chad\"                            \"Chile\"                           \"China\"                          \n [37] \"Colombia\"                        \"Comoros\"                         \"Congo, Dem. Rep.\"               \n [40] \"Congo, Rep.\"                     \"Costa Rica\"                      \"Cote d'Ivoire\"                  \n [43] \"Croatia\"                         \"Cuba\"                            \"Cyprus\"                         \n [46] \"Czechia\"                         \"Denmark\"                         \"Djibouti\"                       \n [49] \"Dominica\"                        \"Dominican Republic\"              \"Ecuador\"                        \n [52] \"Egypt\"                           \"El Salvador\"                     \"Equatorial Guinea\"              \n [55] \"Eritrea\"                         \"Estonia\"                         \"Eswatini\"                       \n [58] \"Ethiopia\"                        \"Fiji\"                            \"Finland\"                        \n [61] \"France\"                          \"Gabon\"                           \"Gambia, The\"                    \n [64] \"Georgia\"                         \"Germany\"                         \"Ghana\"                          \n [67] \"Greece\"                          \"Grenada\"                         \"Guatemala\"                      \n [70] \"Guinea\"                          \"Guinea-Bissau\"                   \"Guyana\"                         \n [73] \"Haiti\"                           \"Honduras\"                        \"Hungary\"                        \n [76] \"Iceland\"                         \"India\"                           \"Indonesia\"                      \n [79] \"Iran\"                            \"Iraq\"                            \"Ireland\"                        \n [82] \"Israel\"                          \"Italy\"                           \"Jamaica\"                        \n [85] \"Japan\"                           \"Jordan\"                          \"Kazakhstan\"                     \n [88] \"Kenya\"                           \"Kiribati\"                        \"Korea, North\"                   \n [91] \"Korea, South\"                    \"Kuwait\"                          \"Kyrgyzstan\"                     \n [94] \"Laos\"                            \"Latvia\"                          \"Lebanon\"                        \n [97] \"Lesotho\"                         \"Liberia\"                         \"Libya\"                          \n[100] \"Liechtenstein\"                   \"Lithuania\"                       \"Luxembourg\"                     \n[103] \"Madagascar\"                      \"Malawi\"                          \"Malaysia\"                       \n[106] \"Maldives\"                        \"Mali\"                            \"Malta\"                          \n[109] \"Marshall Islands\"                \"Mauritania\"                      \"Mauritius\"                      \n[112] \"Mexico\"                          \"Micronesia, Federated States of\" \"Moldova\"                        \n[115] \"Monaco\"                          \"Mongolia\"                        \"Montenegro\"                     \n[118] \"Morocco\"                         \"Mozambique\"                      \"Myanmar\"                        \n[121] \"Namibia\"                         \"Nauru\"                           \"Nepal\"                          \n[124] \"Netherlands\"                     \"New Zealand\"                     \"Nicaragua\"                      \n[127] \"Niger\"                           \"Nigeria\"                         \"North Macedonia\"                \n[130] \"Norway\"                          \"Oman\"                            \"Pakistan\"                       \n[133] \"Palau\"                           \"Panama\"                          \"Papua New Guinea\"               \n[136] \"Paraguay\"                        \"Peru\"                            \"Philippines\"                    \n[139] \"Poland\"                          \"Portugal\"                        \"Qatar\"                          \n[142] \"Romania\"                         \"Russia\"                          \"Rwanda\"                         \n[145] \"Samoa\"                           \"San Marino\"                      \"Sao Tome and Principe\"          \n[148] \"Saudi Arabia\"                    \"Senegal\"                         \"Serbia\"                         \n[151] \"Seychelles\"                      \"Sierra Leone\"                    \"Singapore\"                      \n[154] \"Slovakia\"                        \"Slovenia\"                        \"Solomon Islands\"                \n[157] \"Somalia\"                         \"South Africa\"                    \"South Sudan\"                    \n[160] \"Spain\"                           \"Sri Lanka\"                       \"St. Kitts and Nevis\"            \n[163] \"St. Lucia\"                       \"St. Vincent and the Grenadines\"  \"Sudan\"                          \n[166] \"Suriname\"                        \"Sweden\"                          \"Switzerland\"                    \n[169] \"Syria\"                           \"Tajikistan\"                      \"Tanzania\"                       \n[172] \"Thailand\"                        \"Timor-Leste\"                     \"Togo\"                           \n[175] \"Tonga\"                           \"Trinidad and Tobago\"             \"Tunisia\"                        \n[178] \"Turkiye\"                         \"Turkmenistan\"                    \"Tuvalu\"                         \n[181] \"Uganda\"                          \"Ukraine\"                         \"United Arab Emirates\"           \n[184] \"United Kingdom\"                  \"United States\"                   \"Uruguay\"                        \n[187] \"Uzbekistan\"                      \"Vanuatu\"                         \"Venezuela\"                      \n[190] \"Vietnam\"                         \"Yemen\"                           \"Zambia\"                         \n[193] \"Zimbabwe\"                       \n\n\nIn this example, un_member_states_2024$country extracts the country column from the un_member_states_2024 dataset. This column contains the names of all the UN member states.\n\nThe $ operator acts like a shortcut, letting you directly pull out and work with one column without needing to extract the entire dataset.\nThis is useful when you need to perform operations or analyses on just one column of data, such as calculating summary statistics or visualizing that particular variable.\n\nWhen you run this line of code, R will return a list of all the countries in the dataset, making it easier to focus on that specific part of your data.\n\n\n\n\n\n# To quickly see the first 6 rows of the dataset:\nhead(un_member_states_2024)\n\n# A tibble: 6 × 39\n  country             iso   official_state_name  continent region capital_city capital_population capital_perc_of_coun…¹\n  &lt;chr&gt;               &lt;chr&gt; &lt;chr&gt;                &lt;fct&gt;     &lt;chr&gt;  &lt;chr&gt;                     &lt;dbl&gt;                  &lt;dbl&gt;\n1 Afghanistan         AFG   The Islamic Republi… Asia      South… Kabul                   4601789                   11.5\n2 Albania             ALB   The Republic of Alb… Europe    South… Tirana                   557422                   19.5\n3 Algeria             DZA   The People's Democr… Africa    North… Algiers                 3915811                    8.9\n4 Andorra             AND   The Principality of… Europe    South… Andorra la …              22873                   28.9\n5 Angola              AGO   The Republic of Ang… Africa    Centr… Luanda                  2571861                    7.5\n6 Antigua and Barbuda ATG   Antigua and Barbuda  North Am… Carib… St. John's                22219                   23.8\n# ℹ abbreviated name: ¹​capital_perc_of_country\n# ℹ 31 more variables: capital_data_year &lt;int&gt;, gdp_per_capita &lt;dbl&gt;, gdp_per_capita_year &lt;dbl&gt;,\n#   summers_competed_in &lt;dbl&gt;, summer_golds &lt;int&gt;, summer_silvers &lt;int&gt;, summer_bronzes &lt;int&gt;, summer_total &lt;int&gt;,\n#   winters_competed_in &lt;int&gt;, winter_golds &lt;int&gt;, winter_silvers &lt;int&gt;, winter_bronzes &lt;int&gt;, winter_total &lt;int&gt;,\n#   combined_competed_ins &lt;int&gt;, combined_golds &lt;int&gt;, combined_silvers &lt;int&gt;, combined_bronzes &lt;int&gt;,\n#   combined_total &lt;int&gt;, driving_side &lt;chr&gt;, obesity_rate_2024 &lt;dbl&gt;, obesity_rate_2016 &lt;dbl&gt;,\n#   has_nuclear_weapons_2024 &lt;lgl&gt;, population_2024 &lt;dbl&gt;, area_in_square_km &lt;dbl&gt;, area_in_square_miles &lt;dbl&gt;, …\n\n\nThis helps in getting a peek at the data without overwhelming your screen. It can be a little hard to use if lots of columns though. glimpse() can be better for that.\n\n\n\n\nIn R, you can use it just like a calculator to perform basic arithmetic operations or create sequences of numbers. Let’s look at two examples:\n\n# You can do basic math in R, like addition:\n2 + 2\n\n[1] 4\n\n# Or create sequences:\nseq(from = 1, to = 10, by = 2)\n\n[1] 1 3 5 7 9\n\n\n\nIn the first line, 2 + 2 simply adds two numbers together, and R will output the result (which is 4) directly in the Console.\nThe second line, seq(from = 1, to = 10, by = 2), generates a sequence of numbers. This command creates a sequence that starts at 1, ends at 10, and increases by 2 with each step. The output will be: 1, 3, 5, 7, 9.\n\nNote: R will automatically display the results in the Console unless you assign the output to a variable or name. If you want to store the result for later use, you can assign it using the assignment operator (&lt;-), like this:\n\n# Assign the result of a sequence to a variable:\nmy_sequence &lt;- seq(from = 1, to = 10, by = 2)\n\nNow, the sequence is saved as my_sequence, and you can use it later in your analysis!\n\n\n\n\nIn R, functions often have arguments that specify how they should behave. You can pass these arguments in two ways: by naming them explicitly or by using their position in the function. Let’s look at how this works with the seq() function, which creates sequences of numbers.\n\n# Using named arguments in functions:\nseq(from = 5, to = 100, by = 5)\n\n [1]   5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95 100\n\n# Or using positional arguments:\nseq(5, 100, 5)\n\n [1]   5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95 100\n\n\n\nIn the first line, we’re using named arguments (from, to, by) to clearly specify where the sequence starts, where it ends, and the step size between numbers. This makes it easy to understand what each value represents.\n\nfrom = 5 means the sequence starts at 5.\nto = 100 means it ends at 100.\nby = 5 specifies the step size of 5 between each number.\n\nIn the second line, we’re using positional arguments. R assumes the order of the arguments based on their position:\n\nThe first value 5 is the starting point (from).\nThe second value 100 is the ending point (to).\nThe third value 5 is the step size (by).\n\n\nBoth versions of the function produce the same result: 5, 10, 15, ..., 100. Using named arguments makes your code more readable, especially in longer functions, while positional arguments can make your code shorter and quicker to write when the order of the arguments is clear.\n\n\n\n\n(1.1) Which of the following is required before you can use RStudio effectively?\nA. You need to install RStudio first, as R is automatically included.\nB. R should be installed from the R Project website, and then RStudio can be installed separately.\nC. R is pre-installed on most operating systems, so only RStudio needs to be downloaded.\nD. R and RStudio can be used without any installation, directly from a web browser.\n\n(1.2) What is the purpose of the Environment pane in RStudio?\nA. To display the output of the R code you run, including plots and messages.\nB. To show all objects and datasets currently loaded in your R session.\nC. To provide a workspace for writing R scripts and functions.\nD. To navigate and organize the files within your project folder.\n\n(1.3) How do you install multiple R packages in one step?\nA. Use the install.packages() function with each package listed individually: install.packages(\"ggplot2\"), install.packages(\"dplyr\").\nB. You must install each package one by one, as R doesn’t allow installing multiple packages in a single command.\nC. Use the install.packages() function with the c() function to install several packages at once.\nD. You can install multiple packages by using the load() function for each package separately.\n\n(1.4) Which of the following correctly loads the dplyr package into your R session?\nA. load(dplyr)\nB. library(dplyr)\nC. install.packages(\"dplyr\")\nD. View(dplyr)\n\n(1.5) What does the glimpse() function from the dplyr package do?\nA. It provides a detailed report of summary statistics for each variable in the dataset.\nB. It opens the dataset in a spreadsheet-like viewer for easy exploration.\nC. It gives a quick overview of the structure of a dataset, showing the data types and the first few entries of each column.\nD. It creates a visual summary of numeric columns using histograms and scatterplots.\n\n\n\n\n(1.1) Which of the following is required before you can use RStudio effectively?\nCorrect Answer:\nB. R should be installed from the R Project website, and then RStudio can be installed separately.\nExplanation:\nRStudio is an IDE for R, so R must be installed first, as RStudio provides an interface to interact with R.\n\n(1.2) What is the purpose of the Environment pane in RStudio?\nCorrect Answer:\nB. To show all objects and datasets currently loaded in your R session.\nExplanation:\nThe Environment pane allows you to see and manage the datasets and objects that are available in memory during your R session.\n\n(1.3) How do you install multiple R packages in one step?\nCorrect Answer:\nC. Use the install.packages() function with the c() function to install several packages at once.\nExplanation:\nThe install.packages() function can accept multiple package names within the c() function, allowing you to install several packages in one step.\n\n(1.4) Which of the following correctly loads the dplyr package into your R session?\nCorrect Answer:\nB. library(dplyr)\nExplanation:\nTo load a package into your R session, you need to use the library() function. This makes the functions from the package available to use.\n\n(1.5) What does the glimpse() function from the dplyr package do?\nCorrect Answer:\nC. It gives a quick overview of the structure of a dataset, showing the data types and the first few entries of each column.\nExplanation:\nglimpse() provides a compact overview of a dataset, displaying the data types and a preview of the values in each column.\n\n\n\n\n\n\n\n\n# Load required packages for data visualization\nlibrary(ggplot2)\nlibrary(moderndive) # Not necessary if already loaded\n\n# TIP: You can install packages if not already installed\n# install.packages(c(\"ggplot2\", \"moderndive\"))\n\nThis session requires the ggplot2 and moderndive packages for creating visualizations using the un_member_states_2024 data. We’ll introduce a couple other packages as needed throughout.\n\n\n\n\nHistograms are a great way to visualize the distribution of a numeric variable—in this case, the population of UN member states. In R, you can create a histogram using the ggplot2 package, which provides a flexible framework for building visualizations.\n\n# Create a histogram of population distribution\nggplot(un_member_states_2024, aes(x = population_2024)) +\n  geom_histogram(fill = \"steelblue\", color = \"white\") +\n  # Add labels to better understand plot\n  labs(title = \"Population Distribution of UN Member States\", \n       x = \"Population\", \n       y = \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nggplot(un_member_states_2024, aes(x = population_2024)): This initializes the plot using the ggplot() function. It specifies the dataset (un_member_states_2024) and the aesthetic mapping (aes()) to indicate that the population column should be plotted on the x-axis.\ngeom_histogram(): This function adds a histogram layer to the plot. In this case, we use:\n\nfill = \"steelblue\" to color the bars inside the histogram.\ncolor = \"white\" to outline the bars with a white border, making the bars visually distinct.\n\nlabs(): This function adds labels to the plot to make it more informative:\n\ntitle = \"Population Distribution of UN Member States\": The title of the plot.\nx = \"Population\": The label for the x-axis (which represents population).\ny = \"Frequency\": The label for the y-axis (which represents how frequently populations of a certain size appear).\n\n\nThis histogram allows you to visualize how the population is distributed across the UN member states. You can see the frequency of different population ranges, which gives you a sense of where most countries fall in terms of population size.\nNote: You can adjust the binwidth argument in geom_histogram() to control the width of the bars (or “bins”) used in the histogram. Smaller bins give you more detail, while larger bins smooth out the distribution for a broader overview.\nFor example:\ngeom_histogram(binwidth = 5000000)\nThis would change the bin width to 5 million, grouping countries with similar population sizes together.\n\n\n\n\nA boxplot is a great way to visualize the spread of data and detect any outliers in a dataset. It provides a clear summary of how the values are distributed, such as life expectancy in this example, and helps to compare across different groups—like continents.\n\n# Create a boxplot to see GDP per capita spread by continent\nggplot(un_member_states_2024, aes(x = continent, y = life_expectancy_2022)) +\n  geom_boxplot() +\n  labs(title = \"Life Expectancy in 2022 of UN Member States by Continent\", \n       y = \"Life Expectancy\")\n\nWarning: Removed 4 rows containing non-finite outside the scale range (`stat_boxplot()`).\n\n\n\n\n\n\nggplot(un_member_states_2024, aes(x = continent, y = life_expectancy_2022)): This initializes the plot using the ggplot() function, setting the dataset (un_member_states_2024) and defining the aesthetics:\n\nThe x-axis represents continent, which shows the different continents.\nThe y-axis represents life_expectancy_2022, showing the life expectancy values for each UN member state in 2022.\n\ngeom_boxplot(): This function adds the boxplot to the graph. Each continent will have its own boxplot, summarizing the spread of life expectancy within that group. The box shows the interquartile range (IQR) (middle 50% of the data), the line inside represents the median, and the “whiskers” show the range of the data. Outliers are represented as points outside the whiskers.\nlabs(): This function adds labels to make the plot more informative:\n\ntitle = \"Life Expectancy in 2022 of UN Member States by Continent\": The title helps clarify what the plot is showing.\ny = \"Life Expectancy\": The y-axis label indicates that we’re looking at life expectancy values.\n\n\nThe boxplot allows you to compare the distribution of life expectancy across different continents and see any outliers—countries whose life expectancy significantly differs from others on the same continent.\n\nKey insights from a boxplot:\n\nThe spread of each box shows how life expectancy varies within each continent.\nThe median line helps you quickly see the central tendency for each continent.\nOutliers are points that fall outside the whiskers, signaling countries with unusual life expectancy values.\n\n\nThis plot provides a quick visual summary of how life expectancy varies by continent, highlighting any significant differences or unusual cases.\n\n\n\n\nA barplot is a simple yet powerful tool for comparing the number of items across different categories—in this case, the number of countries on each continent. This plot visually shows how continents differ in terms of the number of countries they contain.\n\n# Create a barplot to compare the number of countries by continent\nggplot(un_member_states_2024, aes(x = continent)) +\n  geom_bar() +\n  labs(title = \"Number of Countries by Continent\", \n       x = \"Continent\", y = \"Number of Countries\")\n\n\n\n\n\nggplot(un_member_states_2024, aes(x = continent)): This initializes the plot using ggplot(), setting the dataset as un_member_states_2024. The aesthetic mapping (aes()) specifies that the x-axis will represent the different continents.\ngeom_bar(): This function creates the barplot, automatically counting the number of countries for each continent. Each bar represents a continent, and the height of the bar shows how many countries belong to that continent.\nlabs(): This function adds informative labels to the plot:\n\ntitle = \"Number of Countries by Continent\": The title helps clarify the purpose of the plot.\nx = \"Continent\": The label for the x-axis, indicating the different continents.\ny = \"Number of Countries\": The label for the y-axis, showing the count of countries for each continent.\n\n\nThe barplot provides a clear visual comparison of the number of countries on each continent. You can quickly see which continent has the most or fewest countries and compare them side by side.\n\nKey insights from a barplot:\n\nBar height: The taller the bar, the more countries that continent has.\nComparison: It’s easy to compare the number of countries across continents at a glance.\nProportional differences: You can quickly spot which continents have significantly more or fewer countries than others.\n\n\nThis barplot effectively shows how the number of countries varies across continents, making it easy to visually compare the different regions of the world.\n\n\n\n\nA scatterplot is an excellent way to visualize the relationship between two numerical variables. In this case, we are examining the relationship between a country’s GDP per capita and its life expectancy. The scatterplot helps us see if there’s any pattern or correlation between these two factors across UN member states.\n\n# Create a scatterplot to show the relationship between GDP per capita and life expectancy\nggplot(un_member_states_2024, aes(x = gdp_per_capita, y = life_expectancy_2022)) +\n  geom_point(color = \"darkred\") +\n  labs(title = \"GDP per Capita vs. Life Expectancy\", \n       x = \"GDP per Capita (USD)\", y = \"Life Expectancy (Years)\")\n\nWarning: Removed 5 rows containing missing values or values outside the scale range (`geom_point()`).\n\n\n\n\n\n\nggplot(un_member_states_2024, aes(x = gdp_per_capita, y = life_expectancy_2022)): This initializes the plot with ggplot(), using un_member_states_2024 as the dataset. The aesthetic mapping (aes()) tells R to plot GDP per capita on the x-axis and life expectancy on the y-axis, showing how these two variables relate.\ngeom_point(color = \"darkred\"): This function adds points to the plot, with each point representing a country. We use dark red as the point color to make the data stand out and easy to interpret.\nlabs(): This function adds helpful labels to the plot:\n\ntitle = \"GDP per Capita vs. Life Expectancy\": The title explains what the plot is showing.\nx = \"GDP per Capita (USD)\": The x-axis label specifies that we’re looking at GDP per capita in US dollars.\ny = \"Life Expectancy (Years)\": The y-axis label shows that life expectancy is measured in years.\n\n\nThe scatterplot helps visualize the relationship between a country’s GDP per capita and life expectancy. Each point on the graph represents a country, showing how its wealth (GDP per capita) relates to the average life expectancy of its population.\n\nKey insights from a scatterplot:\n\nPositive or negative trends: You can observe whether there’s a correlation between GDP per capita and life expectancy—whether countries with higher GDP tend to have higher life expectancy, or if there’s no clear pattern.\nClustering: Look for groups of points that cluster together, which might suggest countries with similar characteristics.\nOutliers: Notice any points that stand far away from the rest, indicating outlier countries where the relationship between GDP and life expectancy doesn’t follow the overall trend.\n\n\nThis scatterplot allows us to explore the relationship between economic prosperity and life expectancy across countries, giving us a visual clue about how these two variables might be connected.\nCheck out Subsection 2.3.2 of ModernDive V2 for overplotting discussion.\n\n\n\n\nFaceting is a powerful feature in ggplot2 that allows you to break down a plot by different categories—in this case, by continent. It creates multiple smaller scatterplots, each representing the relationship between GDP per capita and life expectancy for a specific continent. This helps you explore how the relationship might differ across regions of the world.\n\n# Faceted scatterplot to see relationship by continent\nggplot(un_member_states_2024, aes(x = gdp_per_capita, y = life_expectancy_2022)) +\n  geom_point(color = \"blue\") +\n  facet_wrap(~continent) +\n  labs(title = \"GDP per Capita vs. Life Expectancy by Continent\", \n       x = \"GDP per Capita (USD)\", y = \"Life Expectancy (Years)\")\n\nWarning: Removed 5 rows containing missing values or values outside the scale range (`geom_point()`).\n\n\n\n\n\n\nggplot(un_member_states_2024, aes(x = gdp_per_capita, y = life_expectancy_2022)): This initializes the plot using the un_member_states_2024 dataset. The aesthetic mapping (aes()) tells R to plot GDP per capita on the x-axis and life expectancy on the y-axis, showing how these two variables are related for each country.\ngeom_point(color = \"blue\"): This adds the points to the scatterplot, with each point representing a country. The points are colored blue to distinguish them visually.\nfacet_wrap(~continent): The facet_wrap() function divides the plot into smaller subplots, one for each continent. This helps you compare the relationship between GDP per capita and life expectancy across different continents. Each subplot shows the countries from that specific continent, making it easier to see regional patterns.\nlabs(): This function adds labels to the plot:\n\ntitle = \"GDP per Capita vs. Life Expectancy by Continent\": The title explains that we are comparing GDP per capita and life expectancy for each continent.\nx = \"GDP per Capita (USD)\": The x-axis label specifies that GDP per capita is measured in US dollars.\ny = \"Life Expectancy (Years)\": The y-axis label indicates that life expectancy is measured in years.\n\n\nThe faceted scatterplot allows you to explore the relationship between GDP per capita and life expectancy, broken down by continent. This helps you see whether the relationship holds across different regions or if there are unique patterns specific to certain continents.\n\nKey insights from a faceted scatterplot:\n\nComparing regions: You can easily see how the relationship between GDP per capita and life expectancy differs between continents. For example, some continents may show a stronger correlation, while others may not.\nRegional trends: Faceting allows you to spot trends that are specific to each region. You might notice that countries in certain continents cluster differently.\nOutliers: Within each facet, you can spot outlier countries that deviate from the regional trend, helping you identify exceptional cases.\n\n\nThis faceted scatterplot provides a detailed look at how the relationship between GDP per capita and life expectancy plays out across different continents, giving you a more nuanced understanding of global data patterns.\n\n\n\n\nHere’s an expanded explanation for a new R user, comparing the two approaches:\nThe pie chart and flipped bar chart both aim to show the distribution of UN member states across different regions, but a bar chart often provides clearer comparisons between categories due to the ease of interpreting bar lengths over pie slices.\n\n\nA pie chart displays proportions, breaking the data into slices that represent the different regions.\n\n# Create a pie chart for region distribution\nggplot(un_member_states_2024, aes(x = \"\", fill = region)) +\n  geom_bar() +\n  labs(title = \"Region Distribution of UN Member States\") +\n  coord_polar(theta = \"y\")\n\n\n\n\n\nggplot(un_member_states_2024, aes(x = \"\", fill = region)): This initializes the plot with ggplot(), mapping the region variable to the fill aesthetic, which assigns a different color to each region. The x = ““ ensures the chart is a single stacked bar, which we will later turn into a pie.\ngeom_bar(): This creates a bar plot where each bar section represents the count of UN member states in each region.\ncoord_polar(theta = \"y\"): This converts the bar chart into a pie chart by transforming the stacked bar into circular form, making the height of each bar section correspond to the size of a slice in the pie chart.\nlabs(): Adds a title to explain that the chart shows the “Region Distribution of UN Member States”.\n\nThe pie chart visualizes the distribution of UN member states by region, but pie charts can sometimes make it difficult to compare slices precisely, especially if the differences between regions are small.\n\n\n\nA flipped bar chart shows the same information but in a way that is often easier to compare visually.\n\n# As a bar chart instead\nggplot(un_member_states_2024, aes(x = region)) +\n  geom_bar() +\n  labs(title = \"Region Distribution of UN Member States\") +\n  coord_flip()\n\n\n\n\n\nggplot(un_member_states_2024, aes(x = region)): This initializes the plot, mapping the region variable to the x-axis, which automatically groups the data by region and counts the number of UN member states in each.\ngeom_bar(): Creates a bar plot, where the height of each bar represents the number of countries in each region.\ncoord_flip(): Flips the chart so that the regions are displayed along the y-axis, and the length of the bars is on the x-axis. This makes it easier to compare the regions, as we can now read the bar lengths more easily.\nlabs(): Adds a title to explain that the chart shows the “Region Distribution of UN Member States”.\n\nThe flipped bar chart provides a much clearer way to compare the number of countries across regions because the lengths of the bars are easier to distinguish than the slices of a pie chart. This makes it a preferred option for visualizing categorical data with multiple regions.\n\nKey Takeaway: - Pie Chart: Useful for showing proportions but can be harder to interpret when categories are close in size. - Flipped Bar Chart: Easier to read and compare across categories, especially when there are several regions or when the differences between regions are small.\nIn most cases, a flipped bar chart is the better choice for comparing categorical data like the number of countries by region.\n\n\n\n\n\nThere isn’t a good example of time series data in un_member_states_2024 since it is missing data with a time component. But here is an example of how you could do from the fivethirtyeight package:\n\n\n\n#install.packages(\"fivethirtyeight\")\nlibrary(fivethirtyeight)\n\nSome larger datasets need to be installed separately, like senators and house_district_forecast. To install\nthese, we recommend you install the fivethirtyeightdata package by running:\ninstall.packages('fivethirtyeightdata', repos = 'https://fivethirtyeightdata.github.io/drat/', type =\n'source')\n\n# Create a line graph showing the number of births over 2014\nggplot(US_births_2000_2014, aes(x = date, y = births)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Daily U.S. Births\", \n       x = \"Date\", y = \"Number of Births\")\n\n\n\n\n\nggplot(US_births_2000_2014, aes(x = date, y = births)): This initializes the plot using the US_births_2000_2014 dataset, where date is mapped to the x-axis and births to the y-axis, plotting the number of births over time from 2000 to 2014.\ngeom_line(color = \"blue\"): This function adds a line plot, using blue to represent the number of daily births over the years.\nlabs(): This adds a title and axis labels to clarify the plot:\n\nTitle: “Daily U.S. Births” explains what’s being plotted.\nx = “Date” labels the x-axis as the dates from 2000 to 2014.\ny = “Number of Births” labels the y-axis with the number of births each day.\n\n\nHowever, the sheer volume of data makes this graph difficult to read. The line is too dense because it includes 14 years of daily data, creating a cluttered visual. To make the graph more interpretable, we can filter the data to focus on just one year—2014.\n\n\n\nTo improve readability and gain clearer insights, we’ll filter the dataset to focus only on the year 2014, reducing the number of data points and creating a cleaner visualization.\n\n# Include package for more easily working with dates\n# install.packages(\"lubridate\")\nlibrary(lubridate)\nlibrary(dplyr)\n\n# Filter the data for 2014\nUS_births_2014 &lt;- US_births_2000_2014 |&gt;\n  filter(year(date) == 2014)\n\n# Create a line graph showing the number of births in 2014\nggplot(US_births_2014, aes(x = date, y = births)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Daily U.S. Births in 2014\", \n       x = \"Date\", y = \"Number of Births\")\n\n\n\n\n\nlibrary(lubridate): The lubridate package simplifies working with dates in R. It provides easy-to-use functions like year() to extract the year from a date, making it perfect for filtering the data by year.\nUS_births_2014 &lt;- US_births_2000_2014 |&gt; filter(year(date) == 2014): This line uses the pipe operator (|&gt;) and the filter() function to extract only the data from 2014. The year(date) function (from lubridate) helps to filter the dataset by selecting only rows where the year is 2014.\nCreating the refined plot:\n\nThe process for building the plot remains the same as before, but now we’re working with only 2014 data, significantly reducing the number of points on the line.\n\ngeom_line(color = \"blue\"): Again, we use blue to plot the number of births on each day of 2014, which now results in a much more readable line graph.\nlabs(): This adds labels to clarify the refined plot:\n\nTitle: “Daily U.S. Births in 2014” reflects the new focus on just one year.\nx = “Date” labels the x-axis with the dates of 2014.\ny = “Number of Births” labels the y-axis with the daily number of births.\n\n\nKey Insights: - Initial Graph: The original line graph included data from 2000 to 2014, but it was too dense to be useful, with too many points crammed into the plot. - Refined Graph: By focusing on just 2014, the refined line graph provides a clear and readable view of how births varied day-by-day throughout the year. This makes it easier to spot trends or unusual patterns in the data, like seasonal changes or spikes on certain dates.\n\n\n\n\n(2.1) In the following code for creating a scatterplot of Fertility Rate (fertility_rate_2022) vs. Human Development Index (hdi_2022), which entries should be in quotes?\nggplot(un_member_states_2024, aes(x = fertility_rate_2022, y = hdi_2022)) + \n  geom_point()\nA. un_member_states_2024, fertility_rate_2022, and hdi_2022 should all be in quotes.\nB. Only un_member_states_2024 should be in quotes, as it refers to the dataset.\nC. Only fertility_rate_2022 and hdi_2022 should be in quotes, as they are variable names.\nD. None of the elements in the code should be in quotes.\n\n(2.2) How can you create a histogram of the population distribution in the un_member_states_2024 data frame?\nA. geom_bar(aes(x = population_2024))\nB. ggplot(aes(y = population_2024)) + geom_boxplot()\nC. ggplot(un_member_states_2024, aes(x = population_2024)) + geom_histogram()\nD. ggplot(un_member_states_2024, aes(x = continent)) + geom_bar()\n\n(2.3) What does a boxplot created using the ggplot() function reveal about life expectancy?\nA. The boxplot reveals the mean and standard deviation of life expectancy values.\nB. It shows how life expectancy is spread and identifies outliers.\nC. A boxplot represents the frequency of different life expectancy ranges.\nD. It is used to display the total sum of life expectancy values for each continent.\n\n(2.4) How can you compare the number of countries by continent using a barplot?\nA. ggplot(un_member_states_2024, aes(x = continent)) + geom_bar()\nB. ggplot(un_member_states_2024, aes(x = gdp_per_capita)) + geom_bar()\nC. ggplot(un_member_states_2024, aes(x = continent, y = life_expectancy_2022)) + geom_bar()\nD. ggplot(un_member_states_2024, aes(x = continent, fill = population_2024)) + geom_bar()\n\n(2.5) What is an appropriate purpose of facet wrapping in a scatterplot showing Capital Population vs. Obesity Rate 2024?\nA. To split the scatterplot into multiple graphs by region.\nB. To add color to each region based on obesity rate.\nC. To combine the population of the capital city and the country’s obesity rate in a single graph.\nD. To display the relationship between capital city population and obesity rate over time.\n\n\n\n\n(2.1) In the following code for creating a scatterplot of GDP per capita vs. life expectancy, which entries should be in quotes?\nCorrect Answer:\nD. None of the elements in the code should be in quotes because the data frame and variable names do not require them in this context.\nExplanation:\nIn ggplot(), the dataset and the variable names should not be in quotes. Quotes are only used for strings, such as labels or titles, not for referencing data frames or columns.\n\n(2.2) How can you create a histogram of the population distribution in the un_member_states_2024 data frame?\nCorrect Answer:\nC. ggplot(un_member_states_2024, aes(x = population_2024)) + geom_histogram()\nExplanation:\nThe correct syntax for creating a histogram includes specifying the data frame, defining the x-axis variable inside aes(), and using geom_histogram() to create the histogram.\n\n(2.3) What does a boxplot created using the ggplot() function reveal about life expectancy?\nCorrect Answer:\nB. It shows how life expectancy is spread and identifies outliers.\nExplanation:\nA boxplot helps to visualize the spread of data by showing the quartiles and highlighting any outliers.\n\n(2.4) How can you compare the number of countries by continent using a barplot?\nCorrect Answer:\nA. ggplot(un_member_states_2024, aes(x = continent)) + geom_bar()\nExplanation:\nUsing geom_bar() with the x aesthetic set to continent creates a barplot, where the height of each bar represents the number of countries in each continent.\n\n(2.5) What is an appropriate purpose of facet wrapping in a scatterplot showing Capital Population vs. Obesity Rate 2024?\nCorrect Answer:\nA. To split the scatterplot into multiple graphs by region.\nExplanation:\nFacet wrapping divides the scatterplot into separate panels for each region, allowing for a clearer comparison of relationships across groups.\nOne way to produce this facetting:\n\nggplot(un_member_states_2024, aes(x = capital_population, y = obesity_rate_2024)) +\n  geom_point(aes(color = region)) +\n  facet_wrap(~continent) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nWarning: Removed 42 rows containing missing values or values outside the scale range (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\nThe filter() function allows us to pick out specific rows that meet certain conditions. For example, let’s filter out countries in Africa with a GDP per capita above a certain threshold.\nThe pipe operator |&gt; allows us to chain multiple functions together for a clean and readable data transformation process. It makes it easier to pass the result of one function to the next.\n\n# Filter countries in Africa with GDP per capita greater than $5,000\nafrican_high_gdp &lt;- un_member_states_2024 |&gt; \n  filter(continent == \"Africa\", gdp_per_capita &gt; 5000)\n\n# TIP: Can also use the `&` instead of `,` for AND conditions\nun_member_states_2024 |&gt; \n  filter(continent == \"Africa\" & gdp_per_capita &gt; 5000)\n\n# A tibble: 8 × 39\n  country           iso   official_state_name    continent region capital_city capital_population capital_perc_of_coun…¹\n  &lt;chr&gt;             &lt;chr&gt; &lt;chr&gt;                  &lt;fct&gt;     &lt;chr&gt;  &lt;chr&gt;                     &lt;dbl&gt;                  &lt;dbl&gt;\n1 Botswana          BWA   The Republic of Botsw… Africa    South… Gaborone                 273602                   10.6\n2 Equatorial Guinea GNQ   The Republic of Equat… Africa    Centr… Malabo                   297000                   18.2\n3 Gabon             GAB   The Gabonese Republic  Africa    Centr… Libreville               703904                   30.1\n4 Libya             LBY   The State of Libya     Africa    North… Tripoli                 1170000                   17.4\n5 Mauritius         MUS   The Republic of Mauri… Africa    Easte… Port Louis               147066                   11.3\n6 Namibia           NAM   The Republic of Namib… Africa    South… Windhoek                 431000                   17  \n7 Seychelles        SYC   The Republic of Seych… Africa    Easte… Victoria                  26450                   24.8\n8 South Africa      ZAF   The Republic of South… Africa    South… Pretoria                2921488                    4.9\n# ℹ abbreviated name: ¹​capital_perc_of_country\n# ℹ 31 more variables: capital_data_year &lt;int&gt;, gdp_per_capita &lt;dbl&gt;, gdp_per_capita_year &lt;dbl&gt;,\n#   summers_competed_in &lt;dbl&gt;, summer_golds &lt;int&gt;, summer_silvers &lt;int&gt;, summer_bronzes &lt;int&gt;, summer_total &lt;int&gt;,\n#   winters_competed_in &lt;int&gt;, winter_golds &lt;int&gt;, winter_silvers &lt;int&gt;, winter_bronzes &lt;int&gt;, winter_total &lt;int&gt;,\n#   combined_competed_ins &lt;int&gt;, combined_golds &lt;int&gt;, combined_silvers &lt;int&gt;, combined_bronzes &lt;int&gt;,\n#   combined_total &lt;int&gt;, driving_side &lt;chr&gt;, obesity_rate_2024 &lt;dbl&gt;, obesity_rate_2016 &lt;dbl&gt;,\n#   has_nuclear_weapons_2024 &lt;lgl&gt;, population_2024 &lt;dbl&gt;, area_in_square_km &lt;dbl&gt;, area_in_square_miles &lt;dbl&gt;, …\n\n# This is not the same as using `|` which is an OR condition\nun_member_states_2024 |&gt; \n  filter(continent == \"Africa\" | gdp_per_capita &gt; 5000)\n\n# A tibble: 158 × 39\n   country             iso   official_state_name continent region capital_city capital_population capital_perc_of_coun…¹\n   &lt;chr&gt;               &lt;chr&gt; &lt;chr&gt;               &lt;fct&gt;     &lt;chr&gt;  &lt;chr&gt;                     &lt;dbl&gt;                  &lt;dbl&gt;\n 1 Albania             ALB   The Republic of Al… Europe    South… Tirana                   557422                   19.5\n 2 Algeria             DZA   The People's Democ… Africa    North… Algiers                 3915811                    8.9\n 3 Andorra             AND   The Principality o… Europe    South… Andorra la …              22873                   28.9\n 4 Angola              AGO   The Republic of An… Africa    Centr… Luanda                  2571861                    7.5\n 5 Antigua and Barbuda ATG   Antigua and Barbuda North Am… Carib… St. John's                22219                   23.8\n 6 Argentina           ARG   The Argentine Repu… South Am… South… Buenos Aires            3120612                    6.9\n 7 Armenia             ARM   The Republic of Ar… Asia      Middl… Yerevan                 1096100                   39.3\n 8 Australia           AUS   The Commonwealth o… Oceania   Austr… Canberra                 431380                    1.7\n 9 Austria             AUT   The Republic of Au… Europe    Weste… Vienna                  1962779                   22  \n10 Azerbaijan          AZE   The Republic of Az… Asia      Middl… Baku                    2303100                   22.3\n# ℹ 148 more rows\n# ℹ abbreviated name: ¹​capital_perc_of_country\n# ℹ 31 more variables: capital_data_year &lt;int&gt;, gdp_per_capita &lt;dbl&gt;, gdp_per_capita_year &lt;dbl&gt;,\n#   summers_competed_in &lt;dbl&gt;, summer_golds &lt;int&gt;, summer_silvers &lt;int&gt;, summer_bronzes &lt;int&gt;, summer_total &lt;int&gt;,\n#   winters_competed_in &lt;int&gt;, winter_golds &lt;int&gt;, winter_silvers &lt;int&gt;, winter_bronzes &lt;int&gt;, winter_total &lt;int&gt;,\n#   combined_competed_ins &lt;int&gt;, combined_golds &lt;int&gt;, combined_silvers &lt;int&gt;, combined_bronzes &lt;int&gt;,\n#   combined_total &lt;int&gt;, driving_side &lt;chr&gt;, obesity_rate_2024 &lt;dbl&gt;, obesity_rate_2016 &lt;dbl&gt;, …\n\n\nExplanation:\n- continent == \"Africa\": Selects only rows where the continent is Africa. - gdp_per_capita &gt; 5000: Filters for countries with a GDP per capita above $5,000.\n\n\n\n\nThe summarize() function computes summary statistics for one or more variables. This reduces the data frame to a summary based on the functions applied.\n\n# Summarize average life expectancy and population\nsummary_stats &lt;- un_member_states_2024 |&gt; \n  summarize(\n    avg_life_expectancy = mean(life_expectancy_2022, na.rm = TRUE),\n    med_population = median(population_2024, na.rm = TRUE)\n  )\n\n# Can also display as a markdown table\nlibrary(knitr)\nkable(summary_stats)\n\n\n\n\navg_life_expectancy\nmed_population\n\n\n\n\n73.82566\n9402617\n\n\n\n\n\nExplanation:\n- mean(life_expectancy_2022, na.rm = TRUE): Calculates the average life expectancy while ignoring missing values. - median(population_2024, na.rm = TRUE): Computes the median population across all member states.\n\n\n\n\nThe group_by() function is used to split the data frame into groups. You can then apply functions like summarize() to calculate statistics for each group separately.\n\n# Group by continent and summarize the average life expectancy for each group\nlife_expectancy_by_continent &lt;- un_member_states_2024 |&gt; \n  group_by(continent) |&gt; \n  summarize(\n    avg_life_expectancy = mean(life_expectancy_2022, na.rm = TRUE),\n    sd_life_expectancy = sd(life_expectancy_2022, na.rm = TRUE)\n  )\nlife_expectancy_by_continent\n\n# A tibble: 6 × 3\n  continent     avg_life_expectancy sd_life_expectancy\n  &lt;fct&gt;                       &lt;dbl&gt;              &lt;dbl&gt;\n1 Africa                       66.3               5.59\n2 Asia                         74.9               5.58\n3 Europe                       79.9               3.71\n4 North America                76.3               3.42\n5 Oceania                      74.4               4.83\n6 South America                75.2               3.42\n\n\nExplanation:\n- group_by(continent): Groups the data by continent. - summarize(): Calculates the average life expectancy for each continent.\n\n\n\n\nThe mutate() function is used to add new variables or transform existing ones. This is useful for creating new columns based on existing data.\n\n# Create a new variable that categorizes countries by GDP per capita\nun_member_states_2024 &lt;- un_member_states_2024 |&gt; \n  mutate(\n    gdp_category = case_when(\n      gdp_per_capita &gt; 30000 ~ \"High\",\n      gdp_per_capita &gt; 10000 ~ \"Medium\",\n      TRUE ~ \"Low\"\n    )\n  )\n\nExplanation:\n- mutate(): Adds a new column called gdp_category. - case_when(): Categorizes countries based on their GDP per capita into “High”, “Medium”, or “Low” categories.\n\n\n\n\nThe arrange() function sorts the rows of a data frame. By default, it sorts in ascending order, but you can sort in descending order using the desc() function.\n\n# Arrange countries by population in descending order\ntop_populated_countries &lt;- un_member_states_2024 |&gt; \n  arrange(desc(population_2024))\ntop_populated_countries\n\n# A tibble: 193 × 40\n   country       iso   official_state_name       continent region capital_city capital_population capital_perc_of_coun…¹\n   &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;                     &lt;fct&gt;     &lt;chr&gt;  &lt;chr&gt;                     &lt;dbl&gt;                  &lt;dbl&gt;\n 1 China         CHN   The People's Republic of… Asia      Easte… Beijing                21542000                    1.5\n 2 India         IND   The Republic of India     Asia      South… New Delhi                249998                    0  \n 3 United States USA   The United States of Ame… North Am… North… Washington,…             670050                    0.2\n 4 Indonesia     IDN   The Republic of Indonesia Asia      South… Jakarta                10562088                    3.9\n 5 Pakistan      PAK   The Islamic Republic of … Asia      South… Islamabad               1014825                    0.4\n 6 Nigeria       NGA   The Federal Republic of … Africa    Weste… Abuja                   1235880                    0.6\n 7 Brazil        BRA   The Federative Republic … South Am… South… Brasília                2648532                    1.2\n 8 Bangladesh    BGD   The People's Republic of… Asia      South… Dhaka                   8906039                    5.3\n 9 Russia        RUS   The Russian Federation    Europe    Easte… Moscow                 13104177                    9  \n10 Mexico        MEX   The United Mexican States North Am… Centr… Mexico City             9209944                    7.3\n# ℹ 183 more rows\n# ℹ abbreviated name: ¹​capital_perc_of_country\n# ℹ 32 more variables: capital_data_year &lt;int&gt;, gdp_per_capita &lt;dbl&gt;, gdp_per_capita_year &lt;dbl&gt;,\n#   summers_competed_in &lt;dbl&gt;, summer_golds &lt;int&gt;, summer_silvers &lt;int&gt;, summer_bronzes &lt;int&gt;, summer_total &lt;int&gt;,\n#   winters_competed_in &lt;int&gt;, winter_golds &lt;int&gt;, winter_silvers &lt;int&gt;, winter_bronzes &lt;int&gt;, winter_total &lt;int&gt;,\n#   combined_competed_ins &lt;int&gt;, combined_golds &lt;int&gt;, combined_silvers &lt;int&gt;, combined_bronzes &lt;int&gt;,\n#   combined_total &lt;int&gt;, driving_side &lt;chr&gt;, obesity_rate_2024 &lt;dbl&gt;, obesity_rate_2016 &lt;dbl&gt;, …\n\n\nExplanation:\n- arrange(desc(population_2024)): Sorts the countries from highest to lowest population.\n\n\n\n\nThe select() function allows you to choose specific columns from a data frame. This is useful when you only need to work with a subset of variables.\n\n# Select country name, continent, and population\nselected_data &lt;- un_member_states_2024 |&gt; \n  select(country, continent, population_2024)\nselected_data\n\n# A tibble: 193 × 3\n   country             continent     population_2024\n   &lt;chr&gt;               &lt;fct&gt;                   &lt;dbl&gt;\n 1 Afghanistan         Asia                 40121552\n 2 Albania             Europe                3107100\n 3 Algeria             Africa               47022473\n 4 Andorra             Europe                  85370\n 5 Angola              Africa               37202061\n 6 Antigua and Barbuda North America          102634\n 7 Argentina           South America        46994384\n 8 Armenia             Asia                  2976765\n 9 Australia           Oceania              26768598\n10 Austria             Europe                8967982\n# ℹ 183 more rows\n\n\nExplanation:\n- select(): Chooses the country, continent, and population_2024 columns from the data frame.\n\n\n\n\nHere’s an expanded explanation for a new R user, guiding them through the entire pipeline process step-by-step, which filters, groups, summarizes, mutates, arranges, and selects columns from the un_member_states_2024 dataset:\n\n# Create a pipeline that filters, groups, summarizes, mutates, arranges, \n# and selects columns for countries in Asia and Europe\nun_member_states_2024 |&gt;\n  filter(continent %in% c(\"Asia\", \"Europe\")) |&gt;\n  group_by(continent) |&gt;\n  summarize(\n    avg_gdp_per_capita = mean(gdp_per_capita, na.rm = TRUE),\n    avg_life_expectancy = mean(life_expectancy_2022, na.rm = TRUE)\n  ) |&gt;\n  ungroup() |&gt; # Ungroup before creating new variables\n  mutate(\n    gdp_category = case_when(\n      avg_gdp_per_capita &gt; 30000 ~ \"High\",\n      avg_gdp_per_capita &gt; 10000 ~ \"Medium\",\n      TRUE ~ \"Low\"\n    )\n  ) |&gt;\n  arrange(desc(avg_life_expectancy)) |&gt;\n  select(continent, \n         `Average GDP per capita` = avg_gdp_per_capita, \n         `Average Life Expectancy` = avg_life_expectancy, \n         gdp_category)\n\n# A tibble: 2 × 4\n  continent `Average GDP per capita` `Average Life Expectancy` gdp_category\n  &lt;fct&gt;                        &lt;dbl&gt;                     &lt;dbl&gt; &lt;chr&gt;       \n1 Europe                      45698.                      79.9 High        \n2 Asia                        15272.                      74.9 Medium      \n\n\nExplanation:\n1. filter(continent %in% c(\"Asia\", \"Europe\")): - Filter the dataset to include only countries located in Asia and Europe. The %in% operator is used to match the continent variable with either “Asia” or “Europe,” keeping only those rows in the dataset. - This step reduces the dataset to only the countries from these two continents.\n\ngroup_by(continent):\n\nAfter filtering, we group the data by continent. This ensures that all subsequent calculations are performed separately for each continent (Asia and Europe).\nGrouping is a crucial step for computing statistics like averages by continent.\n\nsummarize():\n\nWe then summarize the grouped data to calculate the following statistics:\n\navg_gdp_per_capita = mean(gdp_per_capita, na.rm = TRUE): Calculates the average GDP per capita for each continent, excluding missing values (na.rm = TRUE).\navg_life_expectancy = mean(life_expectancy_2022, na.rm = TRUE): Computes the average life expectancy in 2022 for each continent, again excluding missing values.\n\nThis results in a summary dataset containing the average GDP per capita and life expectancy for Asia and Europe.\n\nungroup():\n\nAfter summarizing, we ungroup the data. Ungrouping ensures that any further operations (like creating new variables) are applied to the entire dataset, rather than within groups.\n\nmutate():\n\nWe create a new variable, gdp_category, using the case_when() function. This function helps categorize the average GDP per capita into three categories:\n\n“High” if the average GDP per capita is greater than $30,000.\n“Medium” if it’s between $10,000 and $30,000.\n“Low” for all other values (below $10,000).\n\nThis adds a new column to the dataset, categorizing each continent based on its economic standing.\n\narrange(desc(avg_life_expectancy)):\n\nWe arrange the dataset in descending order of average life expectancy. This means that the continent with the highest average life expectancy will appear at the top.\nSorting the data in this way helps prioritize the continents based on health outcomes.\n\nselect():\n\nFinally, we select only the columns we want to display:\n\ncontinent: The name of the continent.\nAverage GDP per capita: The average GDP per capita (renamed for readability).\nAverage Life Expectancy: The average life expectancy (renamed for readability).\ngdp_category: The economic category (High, Medium, or Low) based on the average GDP per capita.\n\nRenaming the columns with backticks (``) makes the column names more descriptive.\n\n\nKey Insights:\n- Pipeline Structure: This pipeline combines several steps—filtering, grouping, summarizing, mutating, arranging, and selecting—into one fluid operation. The pipe operator (|&gt;) allows for a clear, readable flow of operations.\n\nGrouped Summarization: The group_by() and summarize() steps are used to calculate summary statistics like the average GDP per capita and life expectancy for each continent.\nCategorization: The mutate() step uses case_when() to classify continents into economic categories based on GDP, showing how to transform and add new variables based on existing data.\nArranging and Selecting: The data is sorted by life expectancy and only the relevant columns are retained, making the final output clean and easy to interpret.\n\n\n\n\n\n\nCreate a Summary Data Frame (region_data):\n\nWe first need to group the dataset by region, count the number of countries in each region, and then arrange them in descending order to make the plot more readable.\n\n\n\nregion_data &lt;- un_member_states_2024 |&gt; \n  group_by(region) |&gt; \n  summarize(num_countries = n()) |&gt; \n  arrange(desc(num_countries))\n\n\ngroup_by(region): This groups the dataset by the region variable, so each region is treated as a separate group.\nsummarize(num_countries = n()): This calculates the number of countries (n()) in each region. The result is a data frame with two columns: the region name and the number of countries in that region.\narrange(desc(num_countries)): This arranges the regions in descending order of the number of countries. The region with the most countries will appear first in the dataset, making the bar chart easier to interpret.\n\nAt this stage, you have a data frame (region_data) with two columns: region and num_countries, sorted by the number of countries per region.\n\nCreate the Bar Chart Using geom_col()\n\nNow that the data is prepared, let’s create a bar chart using ggplot2 to visualize the number of countries per region. We’ll use geom_col() to create the bars and coord_flip() to flip the axes for better readability.\n\nggplot(region_data, aes(x = reorder(region, num_countries), y = num_countries)) +\n  geom_col(fill = \"skyblue\") +\n  coord_flip() +\n  labs(title = \"Number of Countries by Region\", \n       x = \"Region\", y = \"Number of Countries\")\n\n\n\n\n\nggplot(region_data, aes(x = reorder(region, num_countries), y = num_countries)):\n\nThis initializes the plot, using region_data as the data frame.\naes() (aesthetic mapping) specifies that region will be mapped to the x-axis and num_countries to the y-axis.\nreorder(region, num_countries) ensures that the regions are ordered by the number of countries, so the chart appears in descending order.\n\ngeom_col(fill = \"skyblue\"):\n\ngeom_col() creates the bars for the bar chart, with the height of each bar representing the number of countries in that region.\nWe use fill = \"skyblue\" to color the bars in a light blue shade, making them visually appealing.\n\ncoord_flip():\n\nThis flips the axes so that the bars run horizontally instead of vertically. This makes the chart easier to read, especially when you have long category names (like region names).\n\nlabs():\n\nTitle: “Number of Countries by Region” clearly describes what the chart is showing.\nx = “Region” labels the x-axis (which now appears on the y-axis after the flip) as the regions.\ny = “Number of Countries” labels the y-axis (now the horizontal axis) with the count of countries for each region.\n\n\n\n\n\n\n\n# Convert wide data into tidy data\nlibrary(tibble)\nlibrary(tidyr)\n\n# Collected from https://data.worldbank.org/?locations=BR-NG-ID\nwide_unemp &lt;- tibble(\n  country = c(\"Brazil\", \"Nigeria\", \"Indonesia\"),\n  `2021` = c(13.2, 5.4, 3.8),\n  `2022` = c(9.2, 3.8, 3.5),\n  `2023` = c(8, 3.1, 3.4)\n)\n\n# Use pivot_longer to convert data\ntidy_unemp &lt;- wide_unemp |&gt; \n  pivot_longer(cols = -country, \n               names_to = \"year\", \n               values_to = \"unemployment_perc\") |&gt; \n  mutate(year = as.integer(year))\n\n# Plot data as a linegraph\nggplot(tidy_unemp, aes(x = year, y = unemployment_perc, color = country)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"Unemployment Rate Over Time\", \n       x = \"Year\", y = \"Unemployment Rate (%)\") +\n  scale_x_continuous(breaks = c(2021, 2022, 2023))\n\n\n\n\nHere’s an expanded explanation for a new R user, guiding them through the process of converting wide data into tidy data using pivot_longer(), followed by creating a line graph to visualize unemployment rates over time:\nExplanation:\n1. Convert Wide Data to Tidy Data (pivot_longer): - The initial dataset, wide_unemp, is in wide format, where each column (except country) represents a year and the unemployment rate for that year. We use pivot_longer() to convert this wide format into a tidy format so that each row represents a single observation (country-year combination).\n\npivot_longer(cols = -country, names_to = \"year\", values_to = \"unemployment_perc\"):\n\nThis function “pivots” the data from wide format to long format.\ncols = -country tells R to pivot all columns except for the country column (since country should remain as a fixed identifier).\nnames_to = \"year\": The column names (representing years) will be stored in a new variable called year.\nvalues_to = \"unemployment_perc\": The values from each year’s column will be stored in a new column called unemployment_perc (representing the unemployment percentage).\n\nmutate(year = as.integer(year)): This converts the newly created year column from a character type (since it came from column names) into an integer type, which is necessary for plotting it as a numerical axis in the line graph.\n\nAt this stage, the data has been transformed from wide format to tidy format, with each row representing a country-year-unemployment rate combination, making it suitable for analysis and visualization.\n\nCreate the Line Graph:\n\nNow that the data is tidy, we can create a line graph to visualize how the unemployment rate has changed over time for each country.\n\nggplot(tidy_unemp, aes(x = year, y = unemployment_perc, color = country)):\n\nThis initializes the plot using the tidy_unemp dataset. The aesthetic mapping (aes()) specifies that:\n\nyear goes on the x-axis.\nunemployment_perc goes on the y-axis (representing the unemployment rate as a percentage).\ncolor = country: Each country will be plotted in a different color, allowing us to compare trends across multiple countries.\n\n\ngeom_point(): Adds points to the plot for each country-year combination, marking the unemployment rate at each year.\ngeom_line(): Draws lines connecting the points for each country, showing the trend over time. The combination of points and lines provides both specific data points and a visual connection between them, making trends easier to see.\nlabs(): This function adds labels to the plot:\n\nTitle: “Unemployment Rate Over Time” clearly explains what the plot is showing.\nx = “Year”: Labels the x-axis as the year.\ny = “Unemployment Rate (%)”: Labels the y-axis with the unemployment rate as a percentage.\n\nscale_x_continuous(breaks = c(2021, 2022, 2023)):\n\nThis ensures that only 2021, 2022, and 2023 are shown as tick marks on the x-axis, making the graph more focused on recent years.\n\n\n\n\n\n\nIn R, we often need to import data from external files to start working with it. One common way to do this is by using the read_csv() function from the readr package to load CSV (Comma-Separated Values) files into R.\nLet’s break down how this works with the following code to load a dataset called data_dev_survey.csv.\n\nlibrary(readr)\n\n# Load data from data_dev_survey.csv\n# Remove the ../ if placing the CSV file in the same folder\n# as your .qmd file\ndata_dev_survey &lt;- read_csv(\"../data_dev_survey.csv\")\n\nRows: 1183 Columns: 24\n── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (18): work_as_dev, age, employment, remote_work, coding_activities, ed_level, dev_type, org_size, country, lang...\ndbl   (5): response_id, years_code, years_code_pro, work_exp, converted_comp_yearly\ndate  (1): survey_completion_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nlibrary(readr): This line loads the readr package, giving us access to the read_csv() function, which is designed to efficiently load CSV files into R.\nread_csv(\"data_dev_survey.csv\"): Here, we are using read_csv() to load the data_dev_survey.csv file.\n\nThe argument \"data_dev_survey.csv\" tells R the file name. It assumes the file is in the same directory as the current Quarto document (the .qmd file). If the file is in a different directory, you would need to specify the full path, like \"path/to/data_dev_survey.csv\".\nThis function reads the CSV file and imports it as a data frame (a table-like structure in R) that you can now work with.\n\ndata_dev_survey &lt;-: We assign the output of read_csv() to a new variable called data_dev_survey. This stores the dataset in memory, so you can easily access it for analysis.\n\n\n\n\n\n(3.1) What is the role of the pipe operator |&gt; in streamlining data wrangling tasks?\nA. It combines two datasets into one, merging data from different sources into a unified data frame.\nB. It allows the output of one function to be passed to the next, improving readability and making code writing more efficient.\nC. It saves a dataset to a file, preserving changes by writing the transformed data to a CSV or another format.\nD. It generates plots automatically from a dataset, creating graphs without needing to specify the plotting functions.\n(3.2) How would you filter countries in the un_member_states_2024 data frame that are in Asia and have a population greater than 100 million?\nA. filter(un_member_states_2024, continent = \"Asia\" & population_2024 &gt; 1e8)\nB. un_member_states_2024 |&gt; select(continent = \"Asia\" & population_2024 &gt; 100000000)\nC. un_member_states_2024 |&gt; filter(continent == \"Asia\", population_2024 &gt; 100000000)\nD. un_member_states_2024 |&gt; filter(population_2024 &gt; 100000000 | continent == \"Asia\")\n(3.3) Using group_by() and summarize(), how can you calculate the total population for each continent where the line of code directly above is us_member_states_2024 |&gt;?\nA. group_by(continent) |&gt; summarize(total_population = mean(population_2024))\nB. group_by(continent) |&gt; summarize(total_population = sum(population_2024))\nC. summarize(continent, total_population = sum(population_2024))\nD. group_by(population_2024) |&gt; summarize(total_population = sum(continent))\n(3.4) How does the mutate() function help in creating new categorical variables?\nA. mutate() always replaces an existing variable with a new one based on a conditional statement.\nB. mutate() drops variables from a dataset based on specific conditions.\nC. mutate() filters rows by removing missing values from a variable.\nD. mutate() adds a new variable based on the transformation of existing variables.\n(3.5) How can you arrange countries by their GDP per capita in ascending order?\nA. arrange(un_member_states_2024, gdp_per_capita)\nB. arrange(un_member_states_2024 |&gt; gdp_per_capita)\nC. group_by(gdp_per_capita) |&gt; arrange(continent)\nD. arrange(gdp_per_capita, un_member_states_2024)\n\n\n\n\n(3.1) What is the role of the pipe operator |&gt; in streamlining data wrangling tasks?\nCorrect Answer:\nB. It allows the output of one function to be passed to the next, improving readability and making code writing more efficient.\nExplanation:\nThe pipe operator |&gt; enables a streamlined process for chaining functions together. It makes code more readable by allowing the output of one function to flow into the next function, without having to nest function calls.\n\n(3.2) How would you filter countries in the un_member_states_2024 data frame that are in Asia and have a population greater than 100 million?\nCorrect Answer:\nC. un_member_states_2024 |&gt; filter(continent == \"Asia\", population_2024 &gt; 100000000)\nExplanation:\nThe correct syntax to filter rows requires == for equality checks and the proper use of the |&gt; pipe. We also use & (and) to combine conditions, ensuring both the continent is Asia and the population exceeds 100 million.\n\n(3.3) Using group_by() and summarize(), how can you calculate the total population for each continent where the line of code directly above is us_member_states_2024 |&gt;?\nCorrect Answer:\nB. group_by(continent) |&gt; summarize(total_population = sum(population_2024))\nExplanation:\nThe correct way to calculate the total population by continent involves first grouping the data by continent with group_by(), and then using summarize() with the sum() function to add up the populations for each group.\n\n(3.4) How does the mutate() function help in creating new categorical variables?\nCorrect Answer:\nD. mutate() adds a new variable based on the transformation of existing variables.\nExplanation:\nmutate() allows you to add new variables or transform existing ones. For example, you can use it to create a new categorical variable based on existing data by applying conditional logic or other operations.\n\n(3.5) How can you arrange countries by their GDP per capita in ascending order?\nCorrect Answer:\nA. arrange(un_member_states_2024, gdp_per_capita)\nExplanation:\nThe arrange() function sorts the rows of a data frame. By default, it arranges the data in ascending order, which is the correct behavior for this case when sorting by gdp_per_capita."
  },
  {
    "objectID": "answers/day1_walkthrough_answers.html#session-1-introduction-to-r-and-rstudio",
    "href": "answers/day1_walkthrough_answers.html#session-1-introduction-to-r-and-rstudio",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "You need to install R first from https://cloud.r-project.org/ and then install RStudio from https://posit.co/download/rstudio-desktop/.\nOnce installed, work in RStudio to interact with R efficiently.\n\n\n\n\n\n\nIn RStudio, you will see three panes: Console, Environment, and Files.\n\nConsole: This is where you write and run your code.\nEnvironment: Keeps track of all the objects, like data frames or variables, that you create.\nFiles: Helps you navigate through your project files and folders, similar to how you browse files on your computer.\n\nThe Console is where you type and run your R code.\n\nYou can type a command and press Enter to see the result immediately.\nIf you need to run multiple lines of code, you can write them in the Source Editor (another pane you’ll see later) and send them to the Console.\nThe Console also shows any error messages or outputs when your code runs.\n\nThe Environment pane shows all objects (like datasets) currently in memory.\n\nAs you create variables or load datasets, they’ll appear here, making it easy to keep track of what’s currently active in your session.\nYou can click on any object to inspect it or open it for a detailed view.\nIf your workspace gets cluttered, you can clear it by using the broom icon to remove objects no longer needed.\n\nThe Files pane helps you navigate files in your project.\n\nHere, you can open scripts, view data files, and manage folders related to your project.\nYou can upload files into RStudio, set your working directory, or view the file contents directly.\nRight-click on files to see options like opening, renaming, or deleting files as part of your project management.\n\n\n\n\n\n\n\n# To install a package, use the install.packages() function.\ninstall.packages(\"moderndive\")\ninstall.packages(\"dplyr\")\n\n# or together using the c() function:\n# install.packages(c(\"moderndive\", \"dplyr\"))\n\nThe install.packages() function downloads and installs packages from CRAN (the Comprehensive R Archive Network), allowing you to access additional tools and functions in R. For example, here we’re installing the moderndive and dplyr packages.\nThe c() function is used to create a vector in R, which is essentially a collection or sequence of elements. In this case, it’s combining the package names into a single vector, so you can install multiple packages with one command instead of typing install.packages() multiple times. This helps make your code cleaner and more efficient.\n\n\n\n\n\n# To load a package, use the library() function.\nlibrary(moderndive)\nlibrary(dplyr)\n\n\n# TIP: Check help with `?` operator or `help()` function.\n?library\nhelp(library)\n\nOnce you’ve installed a package using install.packages(), you need to load it into your current R session to access its functions and features. The library() function does just that—it activates the package so you can use its tools. In this example, we’re loading both the moderndive and dplyr packages.\nLoading a package is essential because installing a package only happens once, but you need to load it every time you start a new R session in RStudio and want to use that package.\nIf you ever need more information about how a function or package works, you can use the ? operator or the help() function.\n\n?library opens the help documentation for the library() function, giving you details on what it does, how to use it, and its arguments.\nhelp(library) does the same thing, providing a full description of the function or topic you need assistance with.\n\nThis is a great way to explore R’s built-in help system and get immediate answers or examples while coding.\n\n\n\n\n\n# Can load the data into the Environment pane\ndata(\"un_member_states_2024\")\n\n# To view a dataset in RStudio, use the View() function\nView(un_member_states_2024)\n\nThis opens a spreadsheet-like viewer in RStudio to explore the dataset. This is what is known as a data frame in R. More specifically, this is a special kind of data frame called a tibble.\nThe un_member_states_2024 data contains information on 193 UN member states, with 39 columns capturing various aspects of each country. These columns include details such as the country name, ISO codes, continent, GDP per capita, population, life expectancy, and Olympic participation. It provides a comprehensive dataset for exploring demographic, economic, geographic, and social indicators at the country level.\n\nIdentification variables: country and iso are the clearest\nMeasurement variables: remaining\n\n\n\n\n\nEach column in a dataset is assigned a data type, which describes the kind of information that the column holds. Understanding the data types is important because it determines how R interprets and handles the data. Here are some common data types you’ll encounter in R:\n\nchr (character): Used for text data, such as names or descriptions.\ndbl (double): Represents numeric data with decimal values (floating-point numbers).\nlgl (logical): Holds TRUE or FALSE values, often used for conditions or binary data.\nint (integer): Stores whole numbers (without decimal points).\nfct (factor): Represents categorical data, where values belong to predefined categories or groups (e.g., “Male” and “Female” for gender).\n\nTo quickly check the structure of your dataset and see the data types of each column, you can use the glimpse() function from the dplyr package. This function provides a compact preview of the dataset, showing the name of each column, its data type, and the first few entries.\n\n# To see an overview of the dataset’s structure:\nglimpse(un_member_states_2024)\n\nRows: 193\nColumns: 39\n$ country                            &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andorra\", \"Angola\", \"Antigua and Barb…\n$ iso                                &lt;chr&gt; \"AFG\", \"ALB\", \"DZA\", \"AND\", \"AGO\", \"ATG\", \"ARG\", \"ARM\", \"AUS\", \"AUT\", \"AZE\"…\n$ official_state_name                &lt;chr&gt; \"The Islamic Republic of Afghanistan\", \"The Republic of Albania\", \"The Peop…\n$ continent                          &lt;fct&gt; Asia, Europe, Africa, Europe, Africa, North America, South America, Asia, O…\n$ region                             &lt;chr&gt; \"Southern and Central Asia\", \"Southern Europe\", \"Northern Africa\", \"Souther…\n$ capital_city                       &lt;chr&gt; \"Kabul\", \"Tirana\", \"Algiers\", \"Andorra la Vella\", \"Luanda\", \"St. John's\", \"…\n$ capital_population                 &lt;dbl&gt; 4601789, 557422, 3915811, 22873, 2571861, 22219, 3120612, 1096100, 431380, …\n$ capital_perc_of_country            &lt;dbl&gt; 11.5, 19.5, 8.9, 28.9, 7.5, 23.8, 6.9, 39.3, 1.7, 22.0, 22.3, 67.3, 13.7, 5…\n$ capital_data_year                  &lt;int&gt; 2021, 2011, 2011, 2022, 2020, 2011, 2022, 2021, 2020, 2022, 2022, 2016, 202…\n$ gdp_per_capita                     &lt;dbl&gt; 355.7778, 6810.1140, 4342.6380, 41992.7728, 3000.4442, 19919.7267, 13650.60…\n$ gdp_per_capita_year                &lt;dbl&gt; 2021, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 202…\n$ summers_competed_in                &lt;dbl&gt; 15, NA, 14, NA, NA, NA, 25, 7, 27, 28, 7, 17, 10, NA, 13, 7, 27, NA, NA, NA…\n$ summer_golds                       &lt;int&gt; 0, NA, 5, NA, NA, NA, 21, 2, 164, 20, 7, 8, 2, NA, 0, 13, 44, NA, NA, NA, N…\n$ summer_silvers                     &lt;int&gt; 0, NA, 4, NA, NA, NA, 26, 8, 173, 35, 14, 2, 2, NA, 0, 30, 56, NA, NA, NA, …\n$ summer_bronzes                     &lt;int&gt; 2, NA, 8, NA, NA, NA, 30, 8, 210, 41, 28, 6, 0, NA, 1, 42, 57, NA, NA, NA, …\n$ summer_total                       &lt;int&gt; 2, NA, 17, NA, NA, NA, 77, 18, 547, 96, 49, 16, 4, NA, 1, 85, 157, NA, NA, …\n$ winters_competed_in                &lt;int&gt; 0, NA, 3, NA, NA, NA, 20, 8, 20, 24, 7, 0, 0, NA, 0, 8, 22, NA, NA, NA, NA,…\n$ winter_golds                       &lt;int&gt; 0, NA, 0, NA, NA, NA, 0, 0, 6, 71, 0, 0, 0, NA, 0, 8, 2, NA, NA, NA, NA, NA…\n$ winter_silvers                     &lt;int&gt; 0, NA, 0, NA, NA, NA, 0, 0, 7, 88, 0, 0, 0, NA, 0, 7, 2, NA, NA, NA, NA, NA…\n$ winter_bronzes                     &lt;int&gt; 0, NA, 0, NA, NA, NA, 0, 0, 6, 91, 0, 0, 0, NA, 0, 5, 4, NA, NA, NA, NA, NA…\n$ winter_total                       &lt;int&gt; 0, NA, 0, NA, NA, NA, 0, 0, 19, 250, 0, 0, 0, NA, 0, 20, 8, NA, NA, NA, NA,…\n$ combined_competed_ins              &lt;int&gt; 15, NA, 17, NA, NA, NA, 45, 15, 47, 52, 14, 17, 10, NA, 13, 15, 49, NA, NA,…\n$ combined_golds                     &lt;int&gt; 0, NA, 5, NA, NA, NA, 21, 2, 170, 91, 7, 8, 2, NA, 0, 21, 46, NA, NA, NA, N…\n$ combined_silvers                   &lt;int&gt; 0, NA, 4, NA, NA, NA, 26, 8, 180, 123, 14, 2, 2, NA, 0, 37, 58, NA, NA, NA,…\n$ combined_bronzes                   &lt;int&gt; 2, NA, 8, NA, NA, NA, 30, 8, 216, 132, 28, 6, 0, NA, 1, 47, 61, NA, NA, NA,…\n$ combined_total                     &lt;int&gt; 2, NA, 17, NA, NA, NA, 77, 18, 566, 346, 49, 16, 4, NA, 1, 105, 165, NA, NA…\n$ driving_side                       &lt;chr&gt; \"Right-hand traffic\", \"Right-hand traffic\", \"Right-hand traffic\", \"Right-ha…\n$ obesity_rate_2024                  &lt;dbl&gt; 10.3, 21.3, 21.8, 13.6, 6.8, 43.1, 32.4, 19.5, 31.3, 17.1, 20.6, 43.7, 36.9…\n$ obesity_rate_2016                  &lt;dbl&gt; 5.5, 21.7, 27.4, 25.6, 8.2, 18.9, 28.3, 20.2, 29.0, 20.1, 19.9, 31.6, 29.8,…\n$ has_nuclear_weapons_2024           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ population_2024                    &lt;dbl&gt; 40121552, 3107100, 47022473, 85370, 37202061, 102634, 46994384, 2976765, 26…\n$ area_in_square_km                  &lt;dbl&gt; 652864, 28748, 2381741, 468, 1246700, 442, 2796427, 29743, 7692024, 83878, …\n$ area_in_square_miles               &lt;dbl&gt; 252072.0961, 11099.6603, 919594.9636, 180.6957, 481353.3634, 170.6571, 1079…\n$ population_density_in_square_km    &lt;dbl&gt; 61.454686, 108.080562, 19.742899, 182.414530, 29.840428, 232.203620, 16.805…\n$ population_density_in_square_miles &lt;dbl&gt; 159.166971, 279.927486, 51.133896, 472.451658, 77.286384, 601.404862, 43.52…\n$ income_group_2024                  &lt;fct&gt; Low income, Upper middle income, Lower middle income, High income, Lower mi…\n$ life_expectancy_2022               &lt;dbl&gt; 53.65, 79.47, 78.03, 83.42, 62.11, 77.80, 78.31, 76.13, 83.09, 82.27, 74.15…\n$ fertility_rate_2022                &lt;dbl&gt; 4.3, 1.4, 2.7, NA, 5.0, 1.6, 1.9, 1.6, 1.6, 1.5, 1.6, 1.4, 1.8, 1.9, 1.6, 1…\n$ hdi_2022                           &lt;dbl&gt; 0.462, 0.789, 0.745, 0.884, 0.591, 0.826, 0.849, 0.786, 0.946, 0.926, 0.760…\n\n\nIn this example, glimpse(un_member_states_2024) gives you a summary of the un_member_states_2024 dataset, allowing you to quickly inspect the data types and a snapshot of the content in each column. It’s a handy tool for understanding your data before diving into further analysis!\n\n\n\n\nTo access a specific column in a dataset, R provides a simple and efficient way to do so using the $ operator. This allows you to extract a single column from a data frame by specifying the column name.\n\n# To access a specific column, use the `$` operator:\nun_member_states_2024$country\n\n  [1] \"Afghanistan\"                     \"Albania\"                         \"Algeria\"                        \n  [4] \"Andorra\"                         \"Angola\"                          \"Antigua and Barbuda\"            \n  [7] \"Argentina\"                       \"Armenia\"                         \"Australia\"                      \n [10] \"Austria\"                         \"Azerbaijan\"                      \"Bahamas, The\"                   \n [13] \"Bahrain\"                         \"Bangladesh\"                      \"Barbados\"                       \n [16] \"Belarus\"                         \"Belgium\"                         \"Belize\"                         \n [19] \"Benin\"                           \"Bhutan\"                          \"Bolivia\"                        \n [22] \"Bosnia and Herzegovina\"          \"Botswana\"                        \"Brazil\"                         \n [25] \"Brunei\"                          \"Bulgaria\"                        \"Burkina Faso\"                   \n [28] \"Burundi\"                         \"Cabo Verde\"                      \"Cambodia\"                       \n [31] \"Cameroon\"                        \"Canada\"                          \"Central African Republic\"       \n [34] \"Chad\"                            \"Chile\"                           \"China\"                          \n [37] \"Colombia\"                        \"Comoros\"                         \"Congo, Dem. Rep.\"               \n [40] \"Congo, Rep.\"                     \"Costa Rica\"                      \"Cote d'Ivoire\"                  \n [43] \"Croatia\"                         \"Cuba\"                            \"Cyprus\"                         \n [46] \"Czechia\"                         \"Denmark\"                         \"Djibouti\"                       \n [49] \"Dominica\"                        \"Dominican Republic\"              \"Ecuador\"                        \n [52] \"Egypt\"                           \"El Salvador\"                     \"Equatorial Guinea\"              \n [55] \"Eritrea\"                         \"Estonia\"                         \"Eswatini\"                       \n [58] \"Ethiopia\"                        \"Fiji\"                            \"Finland\"                        \n [61] \"France\"                          \"Gabon\"                           \"Gambia, The\"                    \n [64] \"Georgia\"                         \"Germany\"                         \"Ghana\"                          \n [67] \"Greece\"                          \"Grenada\"                         \"Guatemala\"                      \n [70] \"Guinea\"                          \"Guinea-Bissau\"                   \"Guyana\"                         \n [73] \"Haiti\"                           \"Honduras\"                        \"Hungary\"                        \n [76] \"Iceland\"                         \"India\"                           \"Indonesia\"                      \n [79] \"Iran\"                            \"Iraq\"                            \"Ireland\"                        \n [82] \"Israel\"                          \"Italy\"                           \"Jamaica\"                        \n [85] \"Japan\"                           \"Jordan\"                          \"Kazakhstan\"                     \n [88] \"Kenya\"                           \"Kiribati\"                        \"Korea, North\"                   \n [91] \"Korea, South\"                    \"Kuwait\"                          \"Kyrgyzstan\"                     \n [94] \"Laos\"                            \"Latvia\"                          \"Lebanon\"                        \n [97] \"Lesotho\"                         \"Liberia\"                         \"Libya\"                          \n[100] \"Liechtenstein\"                   \"Lithuania\"                       \"Luxembourg\"                     \n[103] \"Madagascar\"                      \"Malawi\"                          \"Malaysia\"                       \n[106] \"Maldives\"                        \"Mali\"                            \"Malta\"                          \n[109] \"Marshall Islands\"                \"Mauritania\"                      \"Mauritius\"                      \n[112] \"Mexico\"                          \"Micronesia, Federated States of\" \"Moldova\"                        \n[115] \"Monaco\"                          \"Mongolia\"                        \"Montenegro\"                     \n[118] \"Morocco\"                         \"Mozambique\"                      \"Myanmar\"                        \n[121] \"Namibia\"                         \"Nauru\"                           \"Nepal\"                          \n[124] \"Netherlands\"                     \"New Zealand\"                     \"Nicaragua\"                      \n[127] \"Niger\"                           \"Nigeria\"                         \"North Macedonia\"                \n[130] \"Norway\"                          \"Oman\"                            \"Pakistan\"                       \n[133] \"Palau\"                           \"Panama\"                          \"Papua New Guinea\"               \n[136] \"Paraguay\"                        \"Peru\"                            \"Philippines\"                    \n[139] \"Poland\"                          \"Portugal\"                        \"Qatar\"                          \n[142] \"Romania\"                         \"Russia\"                          \"Rwanda\"                         \n[145] \"Samoa\"                           \"San Marino\"                      \"Sao Tome and Principe\"          \n[148] \"Saudi Arabia\"                    \"Senegal\"                         \"Serbia\"                         \n[151] \"Seychelles\"                      \"Sierra Leone\"                    \"Singapore\"                      \n[154] \"Slovakia\"                        \"Slovenia\"                        \"Solomon Islands\"                \n[157] \"Somalia\"                         \"South Africa\"                    \"South Sudan\"                    \n[160] \"Spain\"                           \"Sri Lanka\"                       \"St. Kitts and Nevis\"            \n[163] \"St. Lucia\"                       \"St. Vincent and the Grenadines\"  \"Sudan\"                          \n[166] \"Suriname\"                        \"Sweden\"                          \"Switzerland\"                    \n[169] \"Syria\"                           \"Tajikistan\"                      \"Tanzania\"                       \n[172] \"Thailand\"                        \"Timor-Leste\"                     \"Togo\"                           \n[175] \"Tonga\"                           \"Trinidad and Tobago\"             \"Tunisia\"                        \n[178] \"Turkiye\"                         \"Turkmenistan\"                    \"Tuvalu\"                         \n[181] \"Uganda\"                          \"Ukraine\"                         \"United Arab Emirates\"           \n[184] \"United Kingdom\"                  \"United States\"                   \"Uruguay\"                        \n[187] \"Uzbekistan\"                      \"Vanuatu\"                         \"Venezuela\"                      \n[190] \"Vietnam\"                         \"Yemen\"                           \"Zambia\"                         \n[193] \"Zimbabwe\"                       \n\n\nIn this example, un_member_states_2024$country extracts the country column from the un_member_states_2024 dataset. This column contains the names of all the UN member states.\n\nThe $ operator acts like a shortcut, letting you directly pull out and work with one column without needing to extract the entire dataset.\nThis is useful when you need to perform operations or analyses on just one column of data, such as calculating summary statistics or visualizing that particular variable.\n\nWhen you run this line of code, R will return a list of all the countries in the dataset, making it easier to focus on that specific part of your data.\n\n\n\n\n\n# To quickly see the first 6 rows of the dataset:\nhead(un_member_states_2024)\n\n# A tibble: 6 × 39\n  country             iso   official_state_name  continent region capital_city capital_population capital_perc_of_coun…¹\n  &lt;chr&gt;               &lt;chr&gt; &lt;chr&gt;                &lt;fct&gt;     &lt;chr&gt;  &lt;chr&gt;                     &lt;dbl&gt;                  &lt;dbl&gt;\n1 Afghanistan         AFG   The Islamic Republi… Asia      South… Kabul                   4601789                   11.5\n2 Albania             ALB   The Republic of Alb… Europe    South… Tirana                   557422                   19.5\n3 Algeria             DZA   The People's Democr… Africa    North… Algiers                 3915811                    8.9\n4 Andorra             AND   The Principality of… Europe    South… Andorra la …              22873                   28.9\n5 Angola              AGO   The Republic of Ang… Africa    Centr… Luanda                  2571861                    7.5\n6 Antigua and Barbuda ATG   Antigua and Barbuda  North Am… Carib… St. John's                22219                   23.8\n# ℹ abbreviated name: ¹​capital_perc_of_country\n# ℹ 31 more variables: capital_data_year &lt;int&gt;, gdp_per_capita &lt;dbl&gt;, gdp_per_capita_year &lt;dbl&gt;,\n#   summers_competed_in &lt;dbl&gt;, summer_golds &lt;int&gt;, summer_silvers &lt;int&gt;, summer_bronzes &lt;int&gt;, summer_total &lt;int&gt;,\n#   winters_competed_in &lt;int&gt;, winter_golds &lt;int&gt;, winter_silvers &lt;int&gt;, winter_bronzes &lt;int&gt;, winter_total &lt;int&gt;,\n#   combined_competed_ins &lt;int&gt;, combined_golds &lt;int&gt;, combined_silvers &lt;int&gt;, combined_bronzes &lt;int&gt;,\n#   combined_total &lt;int&gt;, driving_side &lt;chr&gt;, obesity_rate_2024 &lt;dbl&gt;, obesity_rate_2016 &lt;dbl&gt;,\n#   has_nuclear_weapons_2024 &lt;lgl&gt;, population_2024 &lt;dbl&gt;, area_in_square_km &lt;dbl&gt;, area_in_square_miles &lt;dbl&gt;, …\n\n\nThis helps in getting a peek at the data without overwhelming your screen. It can be a little hard to use if lots of columns though. glimpse() can be better for that.\n\n\n\n\nIn R, you can use it just like a calculator to perform basic arithmetic operations or create sequences of numbers. Let’s look at two examples:\n\n# You can do basic math in R, like addition:\n2 + 2\n\n[1] 4\n\n# Or create sequences:\nseq(from = 1, to = 10, by = 2)\n\n[1] 1 3 5 7 9\n\n\n\nIn the first line, 2 + 2 simply adds two numbers together, and R will output the result (which is 4) directly in the Console.\nThe second line, seq(from = 1, to = 10, by = 2), generates a sequence of numbers. This command creates a sequence that starts at 1, ends at 10, and increases by 2 with each step. The output will be: 1, 3, 5, 7, 9.\n\nNote: R will automatically display the results in the Console unless you assign the output to a variable or name. If you want to store the result for later use, you can assign it using the assignment operator (&lt;-), like this:\n\n# Assign the result of a sequence to a variable:\nmy_sequence &lt;- seq(from = 1, to = 10, by = 2)\n\nNow, the sequence is saved as my_sequence, and you can use it later in your analysis!\n\n\n\n\nIn R, functions often have arguments that specify how they should behave. You can pass these arguments in two ways: by naming them explicitly or by using their position in the function. Let’s look at how this works with the seq() function, which creates sequences of numbers.\n\n# Using named arguments in functions:\nseq(from = 5, to = 100, by = 5)\n\n [1]   5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95 100\n\n# Or using positional arguments:\nseq(5, 100, 5)\n\n [1]   5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95 100\n\n\n\nIn the first line, we’re using named arguments (from, to, by) to clearly specify where the sequence starts, where it ends, and the step size between numbers. This makes it easy to understand what each value represents.\n\nfrom = 5 means the sequence starts at 5.\nto = 100 means it ends at 100.\nby = 5 specifies the step size of 5 between each number.\n\nIn the second line, we’re using positional arguments. R assumes the order of the arguments based on their position:\n\nThe first value 5 is the starting point (from).\nThe second value 100 is the ending point (to).\nThe third value 5 is the step size (by).\n\n\nBoth versions of the function produce the same result: 5, 10, 15, ..., 100. Using named arguments makes your code more readable, especially in longer functions, while positional arguments can make your code shorter and quicker to write when the order of the arguments is clear.\n\n\n\n\n(1.1) Which of the following is required before you can use RStudio effectively?\nA. You need to install RStudio first, as R is automatically included.\nB. R should be installed from the R Project website, and then RStudio can be installed separately.\nC. R is pre-installed on most operating systems, so only RStudio needs to be downloaded.\nD. R and RStudio can be used without any installation, directly from a web browser.\n\n(1.2) What is the purpose of the Environment pane in RStudio?\nA. To display the output of the R code you run, including plots and messages.\nB. To show all objects and datasets currently loaded in your R session.\nC. To provide a workspace for writing R scripts and functions.\nD. To navigate and organize the files within your project folder.\n\n(1.3) How do you install multiple R packages in one step?\nA. Use the install.packages() function with each package listed individually: install.packages(\"ggplot2\"), install.packages(\"dplyr\").\nB. You must install each package one by one, as R doesn’t allow installing multiple packages in a single command.\nC. Use the install.packages() function with the c() function to install several packages at once.\nD. You can install multiple packages by using the load() function for each package separately.\n\n(1.4) Which of the following correctly loads the dplyr package into your R session?\nA. load(dplyr)\nB. library(dplyr)\nC. install.packages(\"dplyr\")\nD. View(dplyr)\n\n(1.5) What does the glimpse() function from the dplyr package do?\nA. It provides a detailed report of summary statistics for each variable in the dataset.\nB. It opens the dataset in a spreadsheet-like viewer for easy exploration.\nC. It gives a quick overview of the structure of a dataset, showing the data types and the first few entries of each column.\nD. It creates a visual summary of numeric columns using histograms and scatterplots.\n\n\n\n\n(1.1) Which of the following is required before you can use RStudio effectively?\nCorrect Answer:\nB. R should be installed from the R Project website, and then RStudio can be installed separately.\nExplanation:\nRStudio is an IDE for R, so R must be installed first, as RStudio provides an interface to interact with R.\n\n(1.2) What is the purpose of the Environment pane in RStudio?\nCorrect Answer:\nB. To show all objects and datasets currently loaded in your R session.\nExplanation:\nThe Environment pane allows you to see and manage the datasets and objects that are available in memory during your R session.\n\n(1.3) How do you install multiple R packages in one step?\nCorrect Answer:\nC. Use the install.packages() function with the c() function to install several packages at once.\nExplanation:\nThe install.packages() function can accept multiple package names within the c() function, allowing you to install several packages in one step.\n\n(1.4) Which of the following correctly loads the dplyr package into your R session?\nCorrect Answer:\nB. library(dplyr)\nExplanation:\nTo load a package into your R session, you need to use the library() function. This makes the functions from the package available to use.\n\n(1.5) What does the glimpse() function from the dplyr package do?\nCorrect Answer:\nC. It gives a quick overview of the structure of a dataset, showing the data types and the first few entries of each column.\nExplanation:\nglimpse() provides a compact overview of a dataset, displaying the data types and a preview of the values in each column."
  },
  {
    "objectID": "answers/day1_walkthrough_answers.html#session-2-data-visualization-with-ggplot2",
    "href": "answers/day1_walkthrough_answers.html#session-2-data-visualization-with-ggplot2",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# Load required packages for data visualization\nlibrary(ggplot2)\nlibrary(moderndive) # Not necessary if already loaded\n\n# TIP: You can install packages if not already installed\n# install.packages(c(\"ggplot2\", \"moderndive\"))\n\nThis session requires the ggplot2 and moderndive packages for creating visualizations using the un_member_states_2024 data. We’ll introduce a couple other packages as needed throughout.\n\n\n\n\nHistograms are a great way to visualize the distribution of a numeric variable—in this case, the population of UN member states. In R, you can create a histogram using the ggplot2 package, which provides a flexible framework for building visualizations.\n\n# Create a histogram of population distribution\nggplot(un_member_states_2024, aes(x = population_2024)) +\n  geom_histogram(fill = \"steelblue\", color = \"white\") +\n  # Add labels to better understand plot\n  labs(title = \"Population Distribution of UN Member States\", \n       x = \"Population\", \n       y = \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nggplot(un_member_states_2024, aes(x = population_2024)): This initializes the plot using the ggplot() function. It specifies the dataset (un_member_states_2024) and the aesthetic mapping (aes()) to indicate that the population column should be plotted on the x-axis.\ngeom_histogram(): This function adds a histogram layer to the plot. In this case, we use:\n\nfill = \"steelblue\" to color the bars inside the histogram.\ncolor = \"white\" to outline the bars with a white border, making the bars visually distinct.\n\nlabs(): This function adds labels to the plot to make it more informative:\n\ntitle = \"Population Distribution of UN Member States\": The title of the plot.\nx = \"Population\": The label for the x-axis (which represents population).\ny = \"Frequency\": The label for the y-axis (which represents how frequently populations of a certain size appear).\n\n\nThis histogram allows you to visualize how the population is distributed across the UN member states. You can see the frequency of different population ranges, which gives you a sense of where most countries fall in terms of population size.\nNote: You can adjust the binwidth argument in geom_histogram() to control the width of the bars (or “bins”) used in the histogram. Smaller bins give you more detail, while larger bins smooth out the distribution for a broader overview.\nFor example:\ngeom_histogram(binwidth = 5000000)\nThis would change the bin width to 5 million, grouping countries with similar population sizes together.\n\n\n\n\nA boxplot is a great way to visualize the spread of data and detect any outliers in a dataset. It provides a clear summary of how the values are distributed, such as life expectancy in this example, and helps to compare across different groups—like continents.\n\n# Create a boxplot to see GDP per capita spread by continent\nggplot(un_member_states_2024, aes(x = continent, y = life_expectancy_2022)) +\n  geom_boxplot() +\n  labs(title = \"Life Expectancy in 2022 of UN Member States by Continent\", \n       y = \"Life Expectancy\")\n\nWarning: Removed 4 rows containing non-finite outside the scale range (`stat_boxplot()`).\n\n\n\n\n\n\nggplot(un_member_states_2024, aes(x = continent, y = life_expectancy_2022)): This initializes the plot using the ggplot() function, setting the dataset (un_member_states_2024) and defining the aesthetics:\n\nThe x-axis represents continent, which shows the different continents.\nThe y-axis represents life_expectancy_2022, showing the life expectancy values for each UN member state in 2022.\n\ngeom_boxplot(): This function adds the boxplot to the graph. Each continent will have its own boxplot, summarizing the spread of life expectancy within that group. The box shows the interquartile range (IQR) (middle 50% of the data), the line inside represents the median, and the “whiskers” show the range of the data. Outliers are represented as points outside the whiskers.\nlabs(): This function adds labels to make the plot more informative:\n\ntitle = \"Life Expectancy in 2022 of UN Member States by Continent\": The title helps clarify what the plot is showing.\ny = \"Life Expectancy\": The y-axis label indicates that we’re looking at life expectancy values.\n\n\nThe boxplot allows you to compare the distribution of life expectancy across different continents and see any outliers—countries whose life expectancy significantly differs from others on the same continent.\n\nKey insights from a boxplot:\n\nThe spread of each box shows how life expectancy varies within each continent.\nThe median line helps you quickly see the central tendency for each continent.\nOutliers are points that fall outside the whiskers, signaling countries with unusual life expectancy values.\n\n\nThis plot provides a quick visual summary of how life expectancy varies by continent, highlighting any significant differences or unusual cases.\n\n\n\n\nA barplot is a simple yet powerful tool for comparing the number of items across different categories—in this case, the number of countries on each continent. This plot visually shows how continents differ in terms of the number of countries they contain.\n\n# Create a barplot to compare the number of countries by continent\nggplot(un_member_states_2024, aes(x = continent)) +\n  geom_bar() +\n  labs(title = \"Number of Countries by Continent\", \n       x = \"Continent\", y = \"Number of Countries\")\n\n\n\n\n\nggplot(un_member_states_2024, aes(x = continent)): This initializes the plot using ggplot(), setting the dataset as un_member_states_2024. The aesthetic mapping (aes()) specifies that the x-axis will represent the different continents.\ngeom_bar(): This function creates the barplot, automatically counting the number of countries for each continent. Each bar represents a continent, and the height of the bar shows how many countries belong to that continent.\nlabs(): This function adds informative labels to the plot:\n\ntitle = \"Number of Countries by Continent\": The title helps clarify the purpose of the plot.\nx = \"Continent\": The label for the x-axis, indicating the different continents.\ny = \"Number of Countries\": The label for the y-axis, showing the count of countries for each continent.\n\n\nThe barplot provides a clear visual comparison of the number of countries on each continent. You can quickly see which continent has the most or fewest countries and compare them side by side.\n\nKey insights from a barplot:\n\nBar height: The taller the bar, the more countries that continent has.\nComparison: It’s easy to compare the number of countries across continents at a glance.\nProportional differences: You can quickly spot which continents have significantly more or fewer countries than others.\n\n\nThis barplot effectively shows how the number of countries varies across continents, making it easy to visually compare the different regions of the world.\n\n\n\n\nA scatterplot is an excellent way to visualize the relationship between two numerical variables. In this case, we are examining the relationship between a country’s GDP per capita and its life expectancy. The scatterplot helps us see if there’s any pattern or correlation between these two factors across UN member states.\n\n# Create a scatterplot to show the relationship between GDP per capita and life expectancy\nggplot(un_member_states_2024, aes(x = gdp_per_capita, y = life_expectancy_2022)) +\n  geom_point(color = \"darkred\") +\n  labs(title = \"GDP per Capita vs. Life Expectancy\", \n       x = \"GDP per Capita (USD)\", y = \"Life Expectancy (Years)\")\n\nWarning: Removed 5 rows containing missing values or values outside the scale range (`geom_point()`).\n\n\n\n\n\n\nggplot(un_member_states_2024, aes(x = gdp_per_capita, y = life_expectancy_2022)): This initializes the plot with ggplot(), using un_member_states_2024 as the dataset. The aesthetic mapping (aes()) tells R to plot GDP per capita on the x-axis and life expectancy on the y-axis, showing how these two variables relate.\ngeom_point(color = \"darkred\"): This function adds points to the plot, with each point representing a country. We use dark red as the point color to make the data stand out and easy to interpret.\nlabs(): This function adds helpful labels to the plot:\n\ntitle = \"GDP per Capita vs. Life Expectancy\": The title explains what the plot is showing.\nx = \"GDP per Capita (USD)\": The x-axis label specifies that we’re looking at GDP per capita in US dollars.\ny = \"Life Expectancy (Years)\": The y-axis label shows that life expectancy is measured in years.\n\n\nThe scatterplot helps visualize the relationship between a country’s GDP per capita and life expectancy. Each point on the graph represents a country, showing how its wealth (GDP per capita) relates to the average life expectancy of its population.\n\nKey insights from a scatterplot:\n\nPositive or negative trends: You can observe whether there’s a correlation between GDP per capita and life expectancy—whether countries with higher GDP tend to have higher life expectancy, or if there’s no clear pattern.\nClustering: Look for groups of points that cluster together, which might suggest countries with similar characteristics.\nOutliers: Notice any points that stand far away from the rest, indicating outlier countries where the relationship between GDP and life expectancy doesn’t follow the overall trend.\n\n\nThis scatterplot allows us to explore the relationship between economic prosperity and life expectancy across countries, giving us a visual clue about how these two variables might be connected.\nCheck out Subsection 2.3.2 of ModernDive V2 for overplotting discussion.\n\n\n\n\nFaceting is a powerful feature in ggplot2 that allows you to break down a plot by different categories—in this case, by continent. It creates multiple smaller scatterplots, each representing the relationship between GDP per capita and life expectancy for a specific continent. This helps you explore how the relationship might differ across regions of the world.\n\n# Faceted scatterplot to see relationship by continent\nggplot(un_member_states_2024, aes(x = gdp_per_capita, y = life_expectancy_2022)) +\n  geom_point(color = \"blue\") +\n  facet_wrap(~continent) +\n  labs(title = \"GDP per Capita vs. Life Expectancy by Continent\", \n       x = \"GDP per Capita (USD)\", y = \"Life Expectancy (Years)\")\n\nWarning: Removed 5 rows containing missing values or values outside the scale range (`geom_point()`).\n\n\n\n\n\n\nggplot(un_member_states_2024, aes(x = gdp_per_capita, y = life_expectancy_2022)): This initializes the plot using the un_member_states_2024 dataset. The aesthetic mapping (aes()) tells R to plot GDP per capita on the x-axis and life expectancy on the y-axis, showing how these two variables are related for each country.\ngeom_point(color = \"blue\"): This adds the points to the scatterplot, with each point representing a country. The points are colored blue to distinguish them visually.\nfacet_wrap(~continent): The facet_wrap() function divides the plot into smaller subplots, one for each continent. This helps you compare the relationship between GDP per capita and life expectancy across different continents. Each subplot shows the countries from that specific continent, making it easier to see regional patterns.\nlabs(): This function adds labels to the plot:\n\ntitle = \"GDP per Capita vs. Life Expectancy by Continent\": The title explains that we are comparing GDP per capita and life expectancy for each continent.\nx = \"GDP per Capita (USD)\": The x-axis label specifies that GDP per capita is measured in US dollars.\ny = \"Life Expectancy (Years)\": The y-axis label indicates that life expectancy is measured in years.\n\n\nThe faceted scatterplot allows you to explore the relationship between GDP per capita and life expectancy, broken down by continent. This helps you see whether the relationship holds across different regions or if there are unique patterns specific to certain continents.\n\nKey insights from a faceted scatterplot:\n\nComparing regions: You can easily see how the relationship between GDP per capita and life expectancy differs between continents. For example, some continents may show a stronger correlation, while others may not.\nRegional trends: Faceting allows you to spot trends that are specific to each region. You might notice that countries in certain continents cluster differently.\nOutliers: Within each facet, you can spot outlier countries that deviate from the regional trend, helping you identify exceptional cases.\n\n\nThis faceted scatterplot provides a detailed look at how the relationship between GDP per capita and life expectancy plays out across different continents, giving you a more nuanced understanding of global data patterns.\n\n\n\n\nHere’s an expanded explanation for a new R user, comparing the two approaches:\nThe pie chart and flipped bar chart both aim to show the distribution of UN member states across different regions, but a bar chart often provides clearer comparisons between categories due to the ease of interpreting bar lengths over pie slices.\n\n\nA pie chart displays proportions, breaking the data into slices that represent the different regions.\n\n# Create a pie chart for region distribution\nggplot(un_member_states_2024, aes(x = \"\", fill = region)) +\n  geom_bar() +\n  labs(title = \"Region Distribution of UN Member States\") +\n  coord_polar(theta = \"y\")\n\n\n\n\n\nggplot(un_member_states_2024, aes(x = \"\", fill = region)): This initializes the plot with ggplot(), mapping the region variable to the fill aesthetic, which assigns a different color to each region. The x = ““ ensures the chart is a single stacked bar, which we will later turn into a pie.\ngeom_bar(): This creates a bar plot where each bar section represents the count of UN member states in each region.\ncoord_polar(theta = \"y\"): This converts the bar chart into a pie chart by transforming the stacked bar into circular form, making the height of each bar section correspond to the size of a slice in the pie chart.\nlabs(): Adds a title to explain that the chart shows the “Region Distribution of UN Member States”.\n\nThe pie chart visualizes the distribution of UN member states by region, but pie charts can sometimes make it difficult to compare slices precisely, especially if the differences between regions are small.\n\n\n\nA flipped bar chart shows the same information but in a way that is often easier to compare visually.\n\n# As a bar chart instead\nggplot(un_member_states_2024, aes(x = region)) +\n  geom_bar() +\n  labs(title = \"Region Distribution of UN Member States\") +\n  coord_flip()\n\n\n\n\n\nggplot(un_member_states_2024, aes(x = region)): This initializes the plot, mapping the region variable to the x-axis, which automatically groups the data by region and counts the number of UN member states in each.\ngeom_bar(): Creates a bar plot, where the height of each bar represents the number of countries in each region.\ncoord_flip(): Flips the chart so that the regions are displayed along the y-axis, and the length of the bars is on the x-axis. This makes it easier to compare the regions, as we can now read the bar lengths more easily.\nlabs(): Adds a title to explain that the chart shows the “Region Distribution of UN Member States”.\n\nThe flipped bar chart provides a much clearer way to compare the number of countries across regions because the lengths of the bars are easier to distinguish than the slices of a pie chart. This makes it a preferred option for visualizing categorical data with multiple regions.\n\nKey Takeaway: - Pie Chart: Useful for showing proportions but can be harder to interpret when categories are close in size. - Flipped Bar Chart: Easier to read and compare across categories, especially when there are several regions or when the differences between regions are small.\nIn most cases, a flipped bar chart is the better choice for comparing categorical data like the number of countries by region.\n\n\n\n\n\nThere isn’t a good example of time series data in un_member_states_2024 since it is missing data with a time component. But here is an example of how you could do from the fivethirtyeight package:\n\n\n\n#install.packages(\"fivethirtyeight\")\nlibrary(fivethirtyeight)\n\nSome larger datasets need to be installed separately, like senators and house_district_forecast. To install\nthese, we recommend you install the fivethirtyeightdata package by running:\ninstall.packages('fivethirtyeightdata', repos = 'https://fivethirtyeightdata.github.io/drat/', type =\n'source')\n\n# Create a line graph showing the number of births over 2014\nggplot(US_births_2000_2014, aes(x = date, y = births)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Daily U.S. Births\", \n       x = \"Date\", y = \"Number of Births\")\n\n\n\n\n\nggplot(US_births_2000_2014, aes(x = date, y = births)): This initializes the plot using the US_births_2000_2014 dataset, where date is mapped to the x-axis and births to the y-axis, plotting the number of births over time from 2000 to 2014.\ngeom_line(color = \"blue\"): This function adds a line plot, using blue to represent the number of daily births over the years.\nlabs(): This adds a title and axis labels to clarify the plot:\n\nTitle: “Daily U.S. Births” explains what’s being plotted.\nx = “Date” labels the x-axis as the dates from 2000 to 2014.\ny = “Number of Births” labels the y-axis with the number of births each day.\n\n\nHowever, the sheer volume of data makes this graph difficult to read. The line is too dense because it includes 14 years of daily data, creating a cluttered visual. To make the graph more interpretable, we can filter the data to focus on just one year—2014.\n\n\n\nTo improve readability and gain clearer insights, we’ll filter the dataset to focus only on the year 2014, reducing the number of data points and creating a cleaner visualization.\n\n# Include package for more easily working with dates\n# install.packages(\"lubridate\")\nlibrary(lubridate)\nlibrary(dplyr)\n\n# Filter the data for 2014\nUS_births_2014 &lt;- US_births_2000_2014 |&gt;\n  filter(year(date) == 2014)\n\n# Create a line graph showing the number of births in 2014\nggplot(US_births_2014, aes(x = date, y = births)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Daily U.S. Births in 2014\", \n       x = \"Date\", y = \"Number of Births\")\n\n\n\n\n\nlibrary(lubridate): The lubridate package simplifies working with dates in R. It provides easy-to-use functions like year() to extract the year from a date, making it perfect for filtering the data by year.\nUS_births_2014 &lt;- US_births_2000_2014 |&gt; filter(year(date) == 2014): This line uses the pipe operator (|&gt;) and the filter() function to extract only the data from 2014. The year(date) function (from lubridate) helps to filter the dataset by selecting only rows where the year is 2014.\nCreating the refined plot:\n\nThe process for building the plot remains the same as before, but now we’re working with only 2014 data, significantly reducing the number of points on the line.\n\ngeom_line(color = \"blue\"): Again, we use blue to plot the number of births on each day of 2014, which now results in a much more readable line graph.\nlabs(): This adds labels to clarify the refined plot:\n\nTitle: “Daily U.S. Births in 2014” reflects the new focus on just one year.\nx = “Date” labels the x-axis with the dates of 2014.\ny = “Number of Births” labels the y-axis with the daily number of births.\n\n\nKey Insights: - Initial Graph: The original line graph included data from 2000 to 2014, but it was too dense to be useful, with too many points crammed into the plot. - Refined Graph: By focusing on just 2014, the refined line graph provides a clear and readable view of how births varied day-by-day throughout the year. This makes it easier to spot trends or unusual patterns in the data, like seasonal changes or spikes on certain dates.\n\n\n\n\n(2.1) In the following code for creating a scatterplot of Fertility Rate (fertility_rate_2022) vs. Human Development Index (hdi_2022), which entries should be in quotes?\nggplot(un_member_states_2024, aes(x = fertility_rate_2022, y = hdi_2022)) + \n  geom_point()\nA. un_member_states_2024, fertility_rate_2022, and hdi_2022 should all be in quotes.\nB. Only un_member_states_2024 should be in quotes, as it refers to the dataset.\nC. Only fertility_rate_2022 and hdi_2022 should be in quotes, as they are variable names.\nD. None of the elements in the code should be in quotes.\n\n(2.2) How can you create a histogram of the population distribution in the un_member_states_2024 data frame?\nA. geom_bar(aes(x = population_2024))\nB. ggplot(aes(y = population_2024)) + geom_boxplot()\nC. ggplot(un_member_states_2024, aes(x = population_2024)) + geom_histogram()\nD. ggplot(un_member_states_2024, aes(x = continent)) + geom_bar()\n\n(2.3) What does a boxplot created using the ggplot() function reveal about life expectancy?\nA. The boxplot reveals the mean and standard deviation of life expectancy values.\nB. It shows how life expectancy is spread and identifies outliers.\nC. A boxplot represents the frequency of different life expectancy ranges.\nD. It is used to display the total sum of life expectancy values for each continent.\n\n(2.4) How can you compare the number of countries by continent using a barplot?\nA. ggplot(un_member_states_2024, aes(x = continent)) + geom_bar()\nB. ggplot(un_member_states_2024, aes(x = gdp_per_capita)) + geom_bar()\nC. ggplot(un_member_states_2024, aes(x = continent, y = life_expectancy_2022)) + geom_bar()\nD. ggplot(un_member_states_2024, aes(x = continent, fill = population_2024)) + geom_bar()\n\n(2.5) What is an appropriate purpose of facet wrapping in a scatterplot showing Capital Population vs. Obesity Rate 2024?\nA. To split the scatterplot into multiple graphs by region.\nB. To add color to each region based on obesity rate.\nC. To combine the population of the capital city and the country’s obesity rate in a single graph.\nD. To display the relationship between capital city population and obesity rate over time.\n\n\n\n\n(2.1) In the following code for creating a scatterplot of GDP per capita vs. life expectancy, which entries should be in quotes?\nCorrect Answer:\nD. None of the elements in the code should be in quotes because the data frame and variable names do not require them in this context.\nExplanation:\nIn ggplot(), the dataset and the variable names should not be in quotes. Quotes are only used for strings, such as labels or titles, not for referencing data frames or columns.\n\n(2.2) How can you create a histogram of the population distribution in the un_member_states_2024 data frame?\nCorrect Answer:\nC. ggplot(un_member_states_2024, aes(x = population_2024)) + geom_histogram()\nExplanation:\nThe correct syntax for creating a histogram includes specifying the data frame, defining the x-axis variable inside aes(), and using geom_histogram() to create the histogram.\n\n(2.3) What does a boxplot created using the ggplot() function reveal about life expectancy?\nCorrect Answer:\nB. It shows how life expectancy is spread and identifies outliers.\nExplanation:\nA boxplot helps to visualize the spread of data by showing the quartiles and highlighting any outliers.\n\n(2.4) How can you compare the number of countries by continent using a barplot?\nCorrect Answer:\nA. ggplot(un_member_states_2024, aes(x = continent)) + geom_bar()\nExplanation:\nUsing geom_bar() with the x aesthetic set to continent creates a barplot, where the height of each bar represents the number of countries in each continent.\n\n(2.5) What is an appropriate purpose of facet wrapping in a scatterplot showing Capital Population vs. Obesity Rate 2024?\nCorrect Answer:\nA. To split the scatterplot into multiple graphs by region.\nExplanation:\nFacet wrapping divides the scatterplot into separate panels for each region, allowing for a clearer comparison of relationships across groups.\nOne way to produce this facetting:\n\nggplot(un_member_states_2024, aes(x = capital_population, y = obesity_rate_2024)) +\n  geom_point(aes(color = region)) +\n  facet_wrap(~continent) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nWarning: Removed 42 rows containing missing values or values outside the scale range (`geom_point()`)."
  },
  {
    "objectID": "answers/day1_walkthrough_answers.html#session-3-data-wrangling-and-tidy-data",
    "href": "answers/day1_walkthrough_answers.html#session-3-data-wrangling-and-tidy-data",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "The filter() function allows us to pick out specific rows that meet certain conditions. For example, let’s filter out countries in Africa with a GDP per capita above a certain threshold.\nThe pipe operator |&gt; allows us to chain multiple functions together for a clean and readable data transformation process. It makes it easier to pass the result of one function to the next.\n\n# Filter countries in Africa with GDP per capita greater than $5,000\nafrican_high_gdp &lt;- un_member_states_2024 |&gt; \n  filter(continent == \"Africa\", gdp_per_capita &gt; 5000)\n\n# TIP: Can also use the `&` instead of `,` for AND conditions\nun_member_states_2024 |&gt; \n  filter(continent == \"Africa\" & gdp_per_capita &gt; 5000)\n\n# A tibble: 8 × 39\n  country           iso   official_state_name    continent region capital_city capital_population capital_perc_of_coun…¹\n  &lt;chr&gt;             &lt;chr&gt; &lt;chr&gt;                  &lt;fct&gt;     &lt;chr&gt;  &lt;chr&gt;                     &lt;dbl&gt;                  &lt;dbl&gt;\n1 Botswana          BWA   The Republic of Botsw… Africa    South… Gaborone                 273602                   10.6\n2 Equatorial Guinea GNQ   The Republic of Equat… Africa    Centr… Malabo                   297000                   18.2\n3 Gabon             GAB   The Gabonese Republic  Africa    Centr… Libreville               703904                   30.1\n4 Libya             LBY   The State of Libya     Africa    North… Tripoli                 1170000                   17.4\n5 Mauritius         MUS   The Republic of Mauri… Africa    Easte… Port Louis               147066                   11.3\n6 Namibia           NAM   The Republic of Namib… Africa    South… Windhoek                 431000                   17  \n7 Seychelles        SYC   The Republic of Seych… Africa    Easte… Victoria                  26450                   24.8\n8 South Africa      ZAF   The Republic of South… Africa    South… Pretoria                2921488                    4.9\n# ℹ abbreviated name: ¹​capital_perc_of_country\n# ℹ 31 more variables: capital_data_year &lt;int&gt;, gdp_per_capita &lt;dbl&gt;, gdp_per_capita_year &lt;dbl&gt;,\n#   summers_competed_in &lt;dbl&gt;, summer_golds &lt;int&gt;, summer_silvers &lt;int&gt;, summer_bronzes &lt;int&gt;, summer_total &lt;int&gt;,\n#   winters_competed_in &lt;int&gt;, winter_golds &lt;int&gt;, winter_silvers &lt;int&gt;, winter_bronzes &lt;int&gt;, winter_total &lt;int&gt;,\n#   combined_competed_ins &lt;int&gt;, combined_golds &lt;int&gt;, combined_silvers &lt;int&gt;, combined_bronzes &lt;int&gt;,\n#   combined_total &lt;int&gt;, driving_side &lt;chr&gt;, obesity_rate_2024 &lt;dbl&gt;, obesity_rate_2016 &lt;dbl&gt;,\n#   has_nuclear_weapons_2024 &lt;lgl&gt;, population_2024 &lt;dbl&gt;, area_in_square_km &lt;dbl&gt;, area_in_square_miles &lt;dbl&gt;, …\n\n# This is not the same as using `|` which is an OR condition\nun_member_states_2024 |&gt; \n  filter(continent == \"Africa\" | gdp_per_capita &gt; 5000)\n\n# A tibble: 158 × 39\n   country             iso   official_state_name continent region capital_city capital_population capital_perc_of_coun…¹\n   &lt;chr&gt;               &lt;chr&gt; &lt;chr&gt;               &lt;fct&gt;     &lt;chr&gt;  &lt;chr&gt;                     &lt;dbl&gt;                  &lt;dbl&gt;\n 1 Albania             ALB   The Republic of Al… Europe    South… Tirana                   557422                   19.5\n 2 Algeria             DZA   The People's Democ… Africa    North… Algiers                 3915811                    8.9\n 3 Andorra             AND   The Principality o… Europe    South… Andorra la …              22873                   28.9\n 4 Angola              AGO   The Republic of An… Africa    Centr… Luanda                  2571861                    7.5\n 5 Antigua and Barbuda ATG   Antigua and Barbuda North Am… Carib… St. John's                22219                   23.8\n 6 Argentina           ARG   The Argentine Repu… South Am… South… Buenos Aires            3120612                    6.9\n 7 Armenia             ARM   The Republic of Ar… Asia      Middl… Yerevan                 1096100                   39.3\n 8 Australia           AUS   The Commonwealth o… Oceania   Austr… Canberra                 431380                    1.7\n 9 Austria             AUT   The Republic of Au… Europe    Weste… Vienna                  1962779                   22  \n10 Azerbaijan          AZE   The Republic of Az… Asia      Middl… Baku                    2303100                   22.3\n# ℹ 148 more rows\n# ℹ abbreviated name: ¹​capital_perc_of_country\n# ℹ 31 more variables: capital_data_year &lt;int&gt;, gdp_per_capita &lt;dbl&gt;, gdp_per_capita_year &lt;dbl&gt;,\n#   summers_competed_in &lt;dbl&gt;, summer_golds &lt;int&gt;, summer_silvers &lt;int&gt;, summer_bronzes &lt;int&gt;, summer_total &lt;int&gt;,\n#   winters_competed_in &lt;int&gt;, winter_golds &lt;int&gt;, winter_silvers &lt;int&gt;, winter_bronzes &lt;int&gt;, winter_total &lt;int&gt;,\n#   combined_competed_ins &lt;int&gt;, combined_golds &lt;int&gt;, combined_silvers &lt;int&gt;, combined_bronzes &lt;int&gt;,\n#   combined_total &lt;int&gt;, driving_side &lt;chr&gt;, obesity_rate_2024 &lt;dbl&gt;, obesity_rate_2016 &lt;dbl&gt;, …\n\n\nExplanation:\n- continent == \"Africa\": Selects only rows where the continent is Africa. - gdp_per_capita &gt; 5000: Filters for countries with a GDP per capita above $5,000.\n\n\n\n\nThe summarize() function computes summary statistics for one or more variables. This reduces the data frame to a summary based on the functions applied.\n\n# Summarize average life expectancy and population\nsummary_stats &lt;- un_member_states_2024 |&gt; \n  summarize(\n    avg_life_expectancy = mean(life_expectancy_2022, na.rm = TRUE),\n    med_population = median(population_2024, na.rm = TRUE)\n  )\n\n# Can also display as a markdown table\nlibrary(knitr)\nkable(summary_stats)\n\n\n\n\navg_life_expectancy\nmed_population\n\n\n\n\n73.82566\n9402617\n\n\n\n\n\nExplanation:\n- mean(life_expectancy_2022, na.rm = TRUE): Calculates the average life expectancy while ignoring missing values. - median(population_2024, na.rm = TRUE): Computes the median population across all member states.\n\n\n\n\nThe group_by() function is used to split the data frame into groups. You can then apply functions like summarize() to calculate statistics for each group separately.\n\n# Group by continent and summarize the average life expectancy for each group\nlife_expectancy_by_continent &lt;- un_member_states_2024 |&gt; \n  group_by(continent) |&gt; \n  summarize(\n    avg_life_expectancy = mean(life_expectancy_2022, na.rm = TRUE),\n    sd_life_expectancy = sd(life_expectancy_2022, na.rm = TRUE)\n  )\nlife_expectancy_by_continent\n\n# A tibble: 6 × 3\n  continent     avg_life_expectancy sd_life_expectancy\n  &lt;fct&gt;                       &lt;dbl&gt;              &lt;dbl&gt;\n1 Africa                       66.3               5.59\n2 Asia                         74.9               5.58\n3 Europe                       79.9               3.71\n4 North America                76.3               3.42\n5 Oceania                      74.4               4.83\n6 South America                75.2               3.42\n\n\nExplanation:\n- group_by(continent): Groups the data by continent. - summarize(): Calculates the average life expectancy for each continent.\n\n\n\n\nThe mutate() function is used to add new variables or transform existing ones. This is useful for creating new columns based on existing data.\n\n# Create a new variable that categorizes countries by GDP per capita\nun_member_states_2024 &lt;- un_member_states_2024 |&gt; \n  mutate(\n    gdp_category = case_when(\n      gdp_per_capita &gt; 30000 ~ \"High\",\n      gdp_per_capita &gt; 10000 ~ \"Medium\",\n      TRUE ~ \"Low\"\n    )\n  )\n\nExplanation:\n- mutate(): Adds a new column called gdp_category. - case_when(): Categorizes countries based on their GDP per capita into “High”, “Medium”, or “Low” categories.\n\n\n\n\nThe arrange() function sorts the rows of a data frame. By default, it sorts in ascending order, but you can sort in descending order using the desc() function.\n\n# Arrange countries by population in descending order\ntop_populated_countries &lt;- un_member_states_2024 |&gt; \n  arrange(desc(population_2024))\ntop_populated_countries\n\n# A tibble: 193 × 40\n   country       iso   official_state_name       continent region capital_city capital_population capital_perc_of_coun…¹\n   &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;                     &lt;fct&gt;     &lt;chr&gt;  &lt;chr&gt;                     &lt;dbl&gt;                  &lt;dbl&gt;\n 1 China         CHN   The People's Republic of… Asia      Easte… Beijing                21542000                    1.5\n 2 India         IND   The Republic of India     Asia      South… New Delhi                249998                    0  \n 3 United States USA   The United States of Ame… North Am… North… Washington,…             670050                    0.2\n 4 Indonesia     IDN   The Republic of Indonesia Asia      South… Jakarta                10562088                    3.9\n 5 Pakistan      PAK   The Islamic Republic of … Asia      South… Islamabad               1014825                    0.4\n 6 Nigeria       NGA   The Federal Republic of … Africa    Weste… Abuja                   1235880                    0.6\n 7 Brazil        BRA   The Federative Republic … South Am… South… Brasília                2648532                    1.2\n 8 Bangladesh    BGD   The People's Republic of… Asia      South… Dhaka                   8906039                    5.3\n 9 Russia        RUS   The Russian Federation    Europe    Easte… Moscow                 13104177                    9  \n10 Mexico        MEX   The United Mexican States North Am… Centr… Mexico City             9209944                    7.3\n# ℹ 183 more rows\n# ℹ abbreviated name: ¹​capital_perc_of_country\n# ℹ 32 more variables: capital_data_year &lt;int&gt;, gdp_per_capita &lt;dbl&gt;, gdp_per_capita_year &lt;dbl&gt;,\n#   summers_competed_in &lt;dbl&gt;, summer_golds &lt;int&gt;, summer_silvers &lt;int&gt;, summer_bronzes &lt;int&gt;, summer_total &lt;int&gt;,\n#   winters_competed_in &lt;int&gt;, winter_golds &lt;int&gt;, winter_silvers &lt;int&gt;, winter_bronzes &lt;int&gt;, winter_total &lt;int&gt;,\n#   combined_competed_ins &lt;int&gt;, combined_golds &lt;int&gt;, combined_silvers &lt;int&gt;, combined_bronzes &lt;int&gt;,\n#   combined_total &lt;int&gt;, driving_side &lt;chr&gt;, obesity_rate_2024 &lt;dbl&gt;, obesity_rate_2016 &lt;dbl&gt;, …\n\n\nExplanation:\n- arrange(desc(population_2024)): Sorts the countries from highest to lowest population.\n\n\n\n\nThe select() function allows you to choose specific columns from a data frame. This is useful when you only need to work with a subset of variables.\n\n# Select country name, continent, and population\nselected_data &lt;- un_member_states_2024 |&gt; \n  select(country, continent, population_2024)\nselected_data\n\n# A tibble: 193 × 3\n   country             continent     population_2024\n   &lt;chr&gt;               &lt;fct&gt;                   &lt;dbl&gt;\n 1 Afghanistan         Asia                 40121552\n 2 Albania             Europe                3107100\n 3 Algeria             Africa               47022473\n 4 Andorra             Europe                  85370\n 5 Angola              Africa               37202061\n 6 Antigua and Barbuda North America          102634\n 7 Argentina           South America        46994384\n 8 Armenia             Asia                  2976765\n 9 Australia           Oceania              26768598\n10 Austria             Europe                8967982\n# ℹ 183 more rows\n\n\nExplanation:\n- select(): Chooses the country, continent, and population_2024 columns from the data frame.\n\n\n\n\nHere’s an expanded explanation for a new R user, guiding them through the entire pipeline process step-by-step, which filters, groups, summarizes, mutates, arranges, and selects columns from the un_member_states_2024 dataset:\n\n# Create a pipeline that filters, groups, summarizes, mutates, arranges, \n# and selects columns for countries in Asia and Europe\nun_member_states_2024 |&gt;\n  filter(continent %in% c(\"Asia\", \"Europe\")) |&gt;\n  group_by(continent) |&gt;\n  summarize(\n    avg_gdp_per_capita = mean(gdp_per_capita, na.rm = TRUE),\n    avg_life_expectancy = mean(life_expectancy_2022, na.rm = TRUE)\n  ) |&gt;\n  ungroup() |&gt; # Ungroup before creating new variables\n  mutate(\n    gdp_category = case_when(\n      avg_gdp_per_capita &gt; 30000 ~ \"High\",\n      avg_gdp_per_capita &gt; 10000 ~ \"Medium\",\n      TRUE ~ \"Low\"\n    )\n  ) |&gt;\n  arrange(desc(avg_life_expectancy)) |&gt;\n  select(continent, \n         `Average GDP per capita` = avg_gdp_per_capita, \n         `Average Life Expectancy` = avg_life_expectancy, \n         gdp_category)\n\n# A tibble: 2 × 4\n  continent `Average GDP per capita` `Average Life Expectancy` gdp_category\n  &lt;fct&gt;                        &lt;dbl&gt;                     &lt;dbl&gt; &lt;chr&gt;       \n1 Europe                      45698.                      79.9 High        \n2 Asia                        15272.                      74.9 Medium      \n\n\nExplanation:\n1. filter(continent %in% c(\"Asia\", \"Europe\")): - Filter the dataset to include only countries located in Asia and Europe. The %in% operator is used to match the continent variable with either “Asia” or “Europe,” keeping only those rows in the dataset. - This step reduces the dataset to only the countries from these two continents.\n\ngroup_by(continent):\n\nAfter filtering, we group the data by continent. This ensures that all subsequent calculations are performed separately for each continent (Asia and Europe).\nGrouping is a crucial step for computing statistics like averages by continent.\n\nsummarize():\n\nWe then summarize the grouped data to calculate the following statistics:\n\navg_gdp_per_capita = mean(gdp_per_capita, na.rm = TRUE): Calculates the average GDP per capita for each continent, excluding missing values (na.rm = TRUE).\navg_life_expectancy = mean(life_expectancy_2022, na.rm = TRUE): Computes the average life expectancy in 2022 for each continent, again excluding missing values.\n\nThis results in a summary dataset containing the average GDP per capita and life expectancy for Asia and Europe.\n\nungroup():\n\nAfter summarizing, we ungroup the data. Ungrouping ensures that any further operations (like creating new variables) are applied to the entire dataset, rather than within groups.\n\nmutate():\n\nWe create a new variable, gdp_category, using the case_when() function. This function helps categorize the average GDP per capita into three categories:\n\n“High” if the average GDP per capita is greater than $30,000.\n“Medium” if it’s between $10,000 and $30,000.\n“Low” for all other values (below $10,000).\n\nThis adds a new column to the dataset, categorizing each continent based on its economic standing.\n\narrange(desc(avg_life_expectancy)):\n\nWe arrange the dataset in descending order of average life expectancy. This means that the continent with the highest average life expectancy will appear at the top.\nSorting the data in this way helps prioritize the continents based on health outcomes.\n\nselect():\n\nFinally, we select only the columns we want to display:\n\ncontinent: The name of the continent.\nAverage GDP per capita: The average GDP per capita (renamed for readability).\nAverage Life Expectancy: The average life expectancy (renamed for readability).\ngdp_category: The economic category (High, Medium, or Low) based on the average GDP per capita.\n\nRenaming the columns with backticks (``) makes the column names more descriptive.\n\n\nKey Insights:\n- Pipeline Structure: This pipeline combines several steps—filtering, grouping, summarizing, mutating, arranging, and selecting—into one fluid operation. The pipe operator (|&gt;) allows for a clear, readable flow of operations.\n\nGrouped Summarization: The group_by() and summarize() steps are used to calculate summary statistics like the average GDP per capita and life expectancy for each continent.\nCategorization: The mutate() step uses case_when() to classify continents into economic categories based on GDP, showing how to transform and add new variables based on existing data.\nArranging and Selecting: The data is sorted by life expectancy and only the relevant columns are retained, making the final output clean and easy to interpret.\n\n\n\n\n\n\nCreate a Summary Data Frame (region_data):\n\nWe first need to group the dataset by region, count the number of countries in each region, and then arrange them in descending order to make the plot more readable.\n\n\n\nregion_data &lt;- un_member_states_2024 |&gt; \n  group_by(region) |&gt; \n  summarize(num_countries = n()) |&gt; \n  arrange(desc(num_countries))\n\n\ngroup_by(region): This groups the dataset by the region variable, so each region is treated as a separate group.\nsummarize(num_countries = n()): This calculates the number of countries (n()) in each region. The result is a data frame with two columns: the region name and the number of countries in that region.\narrange(desc(num_countries)): This arranges the regions in descending order of the number of countries. The region with the most countries will appear first in the dataset, making the bar chart easier to interpret.\n\nAt this stage, you have a data frame (region_data) with two columns: region and num_countries, sorted by the number of countries per region.\n\nCreate the Bar Chart Using geom_col()\n\nNow that the data is prepared, let’s create a bar chart using ggplot2 to visualize the number of countries per region. We’ll use geom_col() to create the bars and coord_flip() to flip the axes for better readability.\n\nggplot(region_data, aes(x = reorder(region, num_countries), y = num_countries)) +\n  geom_col(fill = \"skyblue\") +\n  coord_flip() +\n  labs(title = \"Number of Countries by Region\", \n       x = \"Region\", y = \"Number of Countries\")\n\n\n\n\n\nggplot(region_data, aes(x = reorder(region, num_countries), y = num_countries)):\n\nThis initializes the plot, using region_data as the data frame.\naes() (aesthetic mapping) specifies that region will be mapped to the x-axis and num_countries to the y-axis.\nreorder(region, num_countries) ensures that the regions are ordered by the number of countries, so the chart appears in descending order.\n\ngeom_col(fill = \"skyblue\"):\n\ngeom_col() creates the bars for the bar chart, with the height of each bar representing the number of countries in that region.\nWe use fill = \"skyblue\" to color the bars in a light blue shade, making them visually appealing.\n\ncoord_flip():\n\nThis flips the axes so that the bars run horizontally instead of vertically. This makes the chart easier to read, especially when you have long category names (like region names).\n\nlabs():\n\nTitle: “Number of Countries by Region” clearly describes what the chart is showing.\nx = “Region” labels the x-axis (which now appears on the y-axis after the flip) as the regions.\ny = “Number of Countries” labels the y-axis (now the horizontal axis) with the count of countries for each region.\n\n\n\n\n\n\n\n# Convert wide data into tidy data\nlibrary(tibble)\nlibrary(tidyr)\n\n# Collected from https://data.worldbank.org/?locations=BR-NG-ID\nwide_unemp &lt;- tibble(\n  country = c(\"Brazil\", \"Nigeria\", \"Indonesia\"),\n  `2021` = c(13.2, 5.4, 3.8),\n  `2022` = c(9.2, 3.8, 3.5),\n  `2023` = c(8, 3.1, 3.4)\n)\n\n# Use pivot_longer to convert data\ntidy_unemp &lt;- wide_unemp |&gt; \n  pivot_longer(cols = -country, \n               names_to = \"year\", \n               values_to = \"unemployment_perc\") |&gt; \n  mutate(year = as.integer(year))\n\n# Plot data as a linegraph\nggplot(tidy_unemp, aes(x = year, y = unemployment_perc, color = country)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"Unemployment Rate Over Time\", \n       x = \"Year\", y = \"Unemployment Rate (%)\") +\n  scale_x_continuous(breaks = c(2021, 2022, 2023))\n\n\n\n\nHere’s an expanded explanation for a new R user, guiding them through the process of converting wide data into tidy data using pivot_longer(), followed by creating a line graph to visualize unemployment rates over time:\nExplanation:\n1. Convert Wide Data to Tidy Data (pivot_longer): - The initial dataset, wide_unemp, is in wide format, where each column (except country) represents a year and the unemployment rate for that year. We use pivot_longer() to convert this wide format into a tidy format so that each row represents a single observation (country-year combination).\n\npivot_longer(cols = -country, names_to = \"year\", values_to = \"unemployment_perc\"):\n\nThis function “pivots” the data from wide format to long format.\ncols = -country tells R to pivot all columns except for the country column (since country should remain as a fixed identifier).\nnames_to = \"year\": The column names (representing years) will be stored in a new variable called year.\nvalues_to = \"unemployment_perc\": The values from each year’s column will be stored in a new column called unemployment_perc (representing the unemployment percentage).\n\nmutate(year = as.integer(year)): This converts the newly created year column from a character type (since it came from column names) into an integer type, which is necessary for plotting it as a numerical axis in the line graph.\n\nAt this stage, the data has been transformed from wide format to tidy format, with each row representing a country-year-unemployment rate combination, making it suitable for analysis and visualization.\n\nCreate the Line Graph:\n\nNow that the data is tidy, we can create a line graph to visualize how the unemployment rate has changed over time for each country.\n\nggplot(tidy_unemp, aes(x = year, y = unemployment_perc, color = country)):\n\nThis initializes the plot using the tidy_unemp dataset. The aesthetic mapping (aes()) specifies that:\n\nyear goes on the x-axis.\nunemployment_perc goes on the y-axis (representing the unemployment rate as a percentage).\ncolor = country: Each country will be plotted in a different color, allowing us to compare trends across multiple countries.\n\n\ngeom_point(): Adds points to the plot for each country-year combination, marking the unemployment rate at each year.\ngeom_line(): Draws lines connecting the points for each country, showing the trend over time. The combination of points and lines provides both specific data points and a visual connection between them, making trends easier to see.\nlabs(): This function adds labels to the plot:\n\nTitle: “Unemployment Rate Over Time” clearly explains what the plot is showing.\nx = “Year”: Labels the x-axis as the year.\ny = “Unemployment Rate (%)”: Labels the y-axis with the unemployment rate as a percentage.\n\nscale_x_continuous(breaks = c(2021, 2022, 2023)):\n\nThis ensures that only 2021, 2022, and 2023 are shown as tick marks on the x-axis, making the graph more focused on recent years.\n\n\n\n\n\n\nIn R, we often need to import data from external files to start working with it. One common way to do this is by using the read_csv() function from the readr package to load CSV (Comma-Separated Values) files into R.\nLet’s break down how this works with the following code to load a dataset called data_dev_survey.csv.\n\nlibrary(readr)\n\n# Load data from data_dev_survey.csv\n# Remove the ../ if placing the CSV file in the same folder\n# as your .qmd file\ndata_dev_survey &lt;- read_csv(\"../data_dev_survey.csv\")\n\nRows: 1183 Columns: 24\n── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (18): work_as_dev, age, employment, remote_work, coding_activities, ed_level, dev_type, org_size, country, lang...\ndbl   (5): response_id, years_code, years_code_pro, work_exp, converted_comp_yearly\ndate  (1): survey_completion_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nlibrary(readr): This line loads the readr package, giving us access to the read_csv() function, which is designed to efficiently load CSV files into R.\nread_csv(\"data_dev_survey.csv\"): Here, we are using read_csv() to load the data_dev_survey.csv file.\n\nThe argument \"data_dev_survey.csv\" tells R the file name. It assumes the file is in the same directory as the current Quarto document (the .qmd file). If the file is in a different directory, you would need to specify the full path, like \"path/to/data_dev_survey.csv\".\nThis function reads the CSV file and imports it as a data frame (a table-like structure in R) that you can now work with.\n\ndata_dev_survey &lt;-: We assign the output of read_csv() to a new variable called data_dev_survey. This stores the dataset in memory, so you can easily access it for analysis.\n\n\n\n\n\n(3.1) What is the role of the pipe operator |&gt; in streamlining data wrangling tasks?\nA. It combines two datasets into one, merging data from different sources into a unified data frame.\nB. It allows the output of one function to be passed to the next, improving readability and making code writing more efficient.\nC. It saves a dataset to a file, preserving changes by writing the transformed data to a CSV or another format.\nD. It generates plots automatically from a dataset, creating graphs without needing to specify the plotting functions.\n(3.2) How would you filter countries in the un_member_states_2024 data frame that are in Asia and have a population greater than 100 million?\nA. filter(un_member_states_2024, continent = \"Asia\" & population_2024 &gt; 1e8)\nB. un_member_states_2024 |&gt; select(continent = \"Asia\" & population_2024 &gt; 100000000)\nC. un_member_states_2024 |&gt; filter(continent == \"Asia\", population_2024 &gt; 100000000)\nD. un_member_states_2024 |&gt; filter(population_2024 &gt; 100000000 | continent == \"Asia\")\n(3.3) Using group_by() and summarize(), how can you calculate the total population for each continent where the line of code directly above is us_member_states_2024 |&gt;?\nA. group_by(continent) |&gt; summarize(total_population = mean(population_2024))\nB. group_by(continent) |&gt; summarize(total_population = sum(population_2024))\nC. summarize(continent, total_population = sum(population_2024))\nD. group_by(population_2024) |&gt; summarize(total_population = sum(continent))\n(3.4) How does the mutate() function help in creating new categorical variables?\nA. mutate() always replaces an existing variable with a new one based on a conditional statement.\nB. mutate() drops variables from a dataset based on specific conditions.\nC. mutate() filters rows by removing missing values from a variable.\nD. mutate() adds a new variable based on the transformation of existing variables.\n(3.5) How can you arrange countries by their GDP per capita in ascending order?\nA. arrange(un_member_states_2024, gdp_per_capita)\nB. arrange(un_member_states_2024 |&gt; gdp_per_capita)\nC. group_by(gdp_per_capita) |&gt; arrange(continent)\nD. arrange(gdp_per_capita, un_member_states_2024)\n\n\n\n\n(3.1) What is the role of the pipe operator |&gt; in streamlining data wrangling tasks?\nCorrect Answer:\nB. It allows the output of one function to be passed to the next, improving readability and making code writing more efficient.\nExplanation:\nThe pipe operator |&gt; enables a streamlined process for chaining functions together. It makes code more readable by allowing the output of one function to flow into the next function, without having to nest function calls.\n\n(3.2) How would you filter countries in the un_member_states_2024 data frame that are in Asia and have a population greater than 100 million?\nCorrect Answer:\nC. un_member_states_2024 |&gt; filter(continent == \"Asia\", population_2024 &gt; 100000000)\nExplanation:\nThe correct syntax to filter rows requires == for equality checks and the proper use of the |&gt; pipe. We also use & (and) to combine conditions, ensuring both the continent is Asia and the population exceeds 100 million.\n\n(3.3) Using group_by() and summarize(), how can you calculate the total population for each continent where the line of code directly above is us_member_states_2024 |&gt;?\nCorrect Answer:\nB. group_by(continent) |&gt; summarize(total_population = sum(population_2024))\nExplanation:\nThe correct way to calculate the total population by continent involves first grouping the data by continent with group_by(), and then using summarize() with the sum() function to add up the populations for each group.\n\n(3.4) How does the mutate() function help in creating new categorical variables?\nCorrect Answer:\nD. mutate() adds a new variable based on the transformation of existing variables.\nExplanation:\nmutate() allows you to add new variables or transform existing ones. For example, you can use it to create a new categorical variable based on existing data by applying conditional logic or other operations.\n\n(3.5) How can you arrange countries by their GDP per capita in ascending order?\nCorrect Answer:\nA. arrange(un_member_states_2024, gdp_per_capita)\nExplanation:\nThe arrange() function sorts the rows of a data frame. By default, it arranges the data in ascending order, which is the correct behavior for this case when sorting by gdp_per_capita."
  },
  {
    "objectID": "answers/day1_exercise_answers.html",
    "href": "answers/day1_exercise_answers.html",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "We did this during the course in walkthroughs, but if you haven’t done it yet:\n\nYou need to install R first from https://cloud.r-project.org/ and then install RStudio from https://posit.co/download/rstudio-desktop/.\nOnce installed, work in RStudio to interact with R efficiently.\n\n\n\n\n\n\nIn RStudio, you will see three panes: Console, Environment, and Files.\nThe Console is where you type and run your R code.\nThe Environment pane shows all objects (like datasets) currently in memory.\nThe Files pane helps you navigate files in your project.\n\n\n\n\n\nWe also did this during the walkthroughs, but if you haven’t done it yet:\n\n# To install a package, use the install.packages() function.\ninstall.packages(c(\"moderndive\", \"dplyr\", \"ggplot2\", \"readr\", \"tidyr\", \n                   \"lubridate\", \"fivethirtyeight\", \"knitr\"))\n\nThe c() function creates a vector in R, which is a sequence of elements. Here, it combines the package names into a single vector for installation.\n\n\n\n\n\n# To load a package, use the library() function.\nlibrary(moderndive)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(lubridate)\nlibrary(fivethirtyeight)\nlibrary(knitr)\n\n\n\n\n\n\n# Load the data_dev_survey dataset \n# (if you don't have it still loaded from the end of class)\ndata_dev_survey &lt;- read_csv(\"../data_dev_survey.csv\")\n\nRows: 1183 Columns: 24\n── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (18): work_as_dev, age, employment, remote_work, coding_activities, ed_level, dev_type, org_size, country, lang...\ndbl   (5): response_id, years_code, years_code_pro, work_exp, converted_comp_yearly\ndate  (1): survey_completion_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# View the data_dev_survey data\nView(data_dev_survey)\n\nThe Stack Overflow survey data stored in data_dev_survey includes 1,183 responses from professionals in the data and tech fields, with information on their employment, coding activities, education level, and technical skills. The dataset provides a comprehensive view of respondents’ experience levels, preferred programming languages, work environments, and AI perspectives. It captures various demographic and professional details, including their country of work, salary, and views on AI, making it a valuable resource for analyzing trends in the data science and tech professions.\n\nIdentification variable(s): response_id\nMeasurement variable(s): remaining columns\n\n\n\n\n\nEach column in a dataset has a data type, such as:\n\nchr for character (text)\n\ndbl for numeric (decimal values)\n\nlgl for logical (TRUE/FALSE)\n\nint for integer (non-decimal values)\n\nfct for factor (categorical)\n\nglimpse() from the dplyr package gives a preview of the data types and the first few entries in each column.\n\n# To see an overview of the dataset’s structure:\nglimpse(data_dev_survey)\n\nRows: 1,183\nColumns: 24\n$ response_id                &lt;dbl&gt; 164, 165, 190, 218, 220, 462, 542, 558, 577, 802, 815, 885, 1099, 1187, 1210, 1212,…\n$ work_as_dev                &lt;chr&gt; \"I am a developer by profession\", \"I am a developer by profession\", \"I am a develop…\n$ age                        &lt;chr&gt; \"25-34 years old\", \"18-24 years old\", \"35-44 years old\", \"35-44 years old\", \"25-34 …\n$ employment                 &lt;chr&gt; \"Employed, full-time;Independent contractor, freelancer, or self-employed\", \"Employ…\n$ remote_work                &lt;chr&gt; \"Remote\", \"Hybrid (some remote, some in-person)\", \"Hybrid (some remote, some in-per…\n$ coding_activities          &lt;chr&gt; \"Hobby;Professional development or self-paced learning from online courses;Freelanc…\n$ ed_level                   &lt;chr&gt; \"Bachelor’s degree (B.A., B.S., B.Eng., etc.)\", \"Bachelor’s degree (B.A., B.S., B.E…\n$ dev_type                   &lt;chr&gt; \"Engineer, data\", \"Data scientist or machine learning specialist\", \"Data scientist …\n$ years_code                 &lt;dbl&gt; 14, 7, 8, 29, 7, 25, 13, 8, 22, 10, 3, 13, 8, 19, 3, 3, 7, 10, 15, 7, 27, 40, 15, 4…\n$ years_code_pro             &lt;dbl&gt; 10, 5, 8, 17, 6, 20, 3, 7, 13, 10, 2, 12, 3, 19, 2, 1, 4, 5, 5, 2, 25, 30, 10, 4, 3…\n$ org_size                   &lt;chr&gt; \"1,000 to 4,999 employees\", \"10 to 19 employees\", \"500 to 999 employees\", \"1,000 to…\n$ country                    &lt;chr&gt; \"Romania\", \"Canada\", \"Cyprus\", \"Italy\", \"Netherlands\", \"Switzerland\", \"Germany\", \"U…\n$ language_have_worked_with  &lt;chr&gt; \"HTML/CSS;Java;JavaScript;R;SQL;VBA\", \"C;C++;HTML/CSS;JavaScript;Python;TypeScript\"…\n$ language_want_to_work_with &lt;chr&gt; \"PowerShell;Python;SQL\", \"C;C++;Python\", \"Bash/Shell (all shells);HTML/CSS;Python;R…\n$ op_sys_professional_use    &lt;chr&gt; \"Ubuntu;Windows\", \"Ubuntu\", \"Ubuntu;Windows\", \"Red Hat;Windows\", \"MacOS;Windows\", \"…\n$ ai_view                    &lt;chr&gt; \"Very favorable\", \"Very favorable\", \"Favorable\", \"Very favorable\", \"Favorable\", \"Fa…\n$ ai_trust                   &lt;chr&gt; \"Somewhat trust\", \"Somewhat trust\", \"Somewhat trust\", \"Somewhat trust\", \"Somewhat t…\n$ ic_or_manager              &lt;chr&gt; \"Individual contributor\", \"Individual contributor\", \"Individual contributor\", \"Indi…\n$ work_exp                   &lt;dbl&gt; 10, 7, 16, 17, 7, 25, 3, 7, 13, 10, 10, 13, 10, 19, 3, 14, 3, 5, 5, 2, 25, 30, 11, …\n$ industry                   &lt;chr&gt; \"Information Services, IT, Software Development, or other Technology\", \"Information…\n$ us_or_not                  &lt;chr&gt; \"Not US\", \"Not US\", \"Not US\", \"Not US\", \"Not US\", \"Not US\", \"Not US\", \"US\", \"Not US…\n$ plans_to_use_ai            &lt;chr&gt; \"Using\", \"Using\", \"Using\", \"Plan to use\", \"Using\", \"Plan to use\", \"Using\", \"Plan to…\n$ converted_comp_yearly      &lt;dbl&gt; 3237, 52046, 74963, 56757, 74963, 132260, 117798, 165974, 64254, 71779, 36351, 8134…\n$ survey_completion_date     &lt;date&gt; 2023-05-02, 2023-05-05, 2023-05-13, 2023-05-28, 2023-05-31, 2023-05-17, 2023-05-13…\n\n\n\n\n\n\n\n# Access the work_exp column from the data_dev_survey dataset\ndata_dev_survey$work_exp\n\n   [1] 10  7 16 17  7 25  3  7 13 10 10 13 10 19  3 14  3  5  5  2 25 30 11  2  3 26  2  2  7  3  6 13 20 13  3 13  6 10\n  [39]  3 15  3  8  8  8 10 16  5  6 17 10 10  5 15  4  4  5  8  2  2 12  9  5 10  8 11 19 10 15 35  4  6  6 20  6 24  5\n  [77] 21  5  8  5  4  2  2 15 25 15  3 20 13 20 12  1  7 15  1  6  6  9  6  6  6  2  6 10  1 15  7  2  6  5 10  2  6  1\n [115]  3 10  5  4  5  5  2 22  4 10  1  8  3 18  2  4 13  1 11  2  5 22  5 11  1  3 11  6 33  6  7  4  2  2  2 12  6  2\n [153] 25  7 11  3 10 13  5 16 24 35  5  9 10 10  3 20  6  2 13  2 16  5 25  5  2 20 10 20 20 12  9  8  2  7  2 19 15 19\n [191] 15  5  7 10 13 30  6  4  4  2 34  5  9  2  9  6  3  3 14 10  8  5 26 10  4 28  9  6 13  8  2 18 14  7 10 21 13  7\n [229]  6 10 27  7  7  5 13  6  5  3 10  2 28  3 25  3 15  4 10  9 10  6  6  2  4 10  3  2  5  5  8  5  8  1 11  8  2 12\n [267]  1  6  7 29 10  3  7  2 13  4  9  3  6  1  5  1  5 25 15  3 27  5 22  2 25  4  2  5 15  8  2 20  1  6 19  4  1  6\n [305] 21  0 10 10 12 15 13  8  3  6 10 10  8 36  2 12 35 12  3  3  7 18 10  1  8 11  8 13  2 10  7  8 10 12 16 10  6  7\n [343]  2 14 24 40 26 15 16  4 20  4 27  8  3  3 12 15 10 15  1  3  5 10 10  9 17 24 25  8  5  9  2  2  3 14 40  4 11  7\n [381]  3 13  5 25  6  5 15  2  6 25  0  6  9 12 28 23  3  5  1  1  9 10  2  5 31 23  2  0 16 18  6  6 12  9  4  2 15 25\n [419] 18  4  5  3  8  7 28  1 10  4 47 15 27  5 30  4  4  7 25  7  6  7  1  6  4 10 15 48 13 12 10  5 15 13  2 25 25 10\n [457]  4 16  5 10  2 11  7 14 23 10  6 15 25 19 25  1  8  2  9 27 15 28  3 10  5  5 10 10  1 19  2 12  7 35 23  4  1 10\n [495]  7 40  3  7 10 11  5  5 31 21 25  7 25 12 14 12  5  2 17 17  9 20  5  7  9 36  8  4 22 10 10 22  9  7  3 13  3 12\n [533]  7 15 23 10 13  3  1  5 15  6  1  7 23 10  8 42  5 20 14  7  2 11  8 25 35 16 24  7 10 10 12 19 21  4 16 12 16  3\n [571]  8  6  7 12 10 30 11 20 10 23  6  4 10  7  8  7 17 15 20  8 14  0  6  1 16  1 18 30 10  9 14  6 10 18 13  4 22  1\n [609]  2  8  4  5  8  3  4  4 30 30 26  5  5 11  4  7 15  5  5  7 20  4 10  8  3  5  3  2  5  1  3  7 31 20  1  5  6  3\n [647] 12  8 16  7  3  1  1  7  9 32 11  8  9  2  1  6  7 22  8 21  2 15 10 35  5  5 22 14 15  7  8 10 31 23 12 15  7 12\n [685]  9  6 38 12  5 17  4 30  4  3 42  2  1  5  5 18  3 29  3 20 14  3 10  7  6  4 13 10 15  7  3 13  4  3  8  6  8  4\n [723] 15  3  5  7 17  3  1 26 11  2  5 20  1  1 17  5 25 18  3  4 15  6 17 22  1 26 11 10  2 20  2 18  3  3  3  2 14  1\n [761]  6  9  4 40  4 12  9  5  1  9  1  4  8 10  6  8  2  3  9  3 12  6  7  6  7 21 20 21  8 14  7  6 11 27  6  2  4  1\n [799] 13 10 30  3 12 10 20  3  6  2  5  2  9 17  7 10  6  3  1  7  7  8  2 25  7 16  7 12 10 12 12  0  9  5  6  4  3  5\n [837]  7  6  2  2  2  8  9 13  7  4  9 35  8  2 10  2 12  3  7  6  6  8  1  7  3  4  3 15 10  6 13  3 15  1 10 10  6 15\n [875]  5  4  7 11  6 13  7  4 16  9 14  3  6  8 10 13 25  6 23 17  5  8  9  8  4  8  4 10 15 13  0 15  1 10  5 11 16 10\n [913]  8  2  1 12  3 15  1  9 10  9  6 14  5 20  9 30 16 12  3  5  2  1 10  5  1 23  2  2  4 34 14  4  5  6 14  3  5  9\n [951]  4  3  9  6 12 19  9 19 12  3  5 10  4  4  5  5 35  4  1  8  6  3 13  5  8  3  7 25  2  5  1  4  5  5 14 20 10  3\n [989]  5 11  2  1  2 12  4  8  3  8  5  9  4  3  8  4  2  9  1 10 15  8  7  3 10 22 16 15 23  2  8  6 10  9  3  2  6 25\n[1027] 12  3 12  7  5  5  1  2  5  8  3  2  8  6  3  3  8  2 10  3 25  4  5  6  4  1  1  2 15  6  6  4  7 10 15  4  1 46\n[1065] 10  8  5 11  1 15  5  1  4 33  2  6 39 15 10  1 11  5  9  7 15  7  3  2  5  1  4  2  9  4  8  3  0 20  5  5 25 12\n[1103] 31  3  2 12  2 14  5 25  6  3  4 11  2  1  2  1  1  4  1 14  5  3  2  4  7  2  1  1  4  1  8 11  5  1  1  2  3 16\n[1141]  2  4 11  7  6 11  2 20 12  5  1  3  3  3  4 30  5  2 30  8  4  4 15 15  3  3 10  8  6  7  5  1 47 25  4 13  1  8\n[1179]  3  8  2 11 39\n\n\n\n\n\n\n\n# To quickly see the first 6 rows of the dataset:\nhead(data_dev_survey)\n\n# A tibble: 6 × 24\n  response_id work_as_dev     age   employment remote_work coding_activities ed_level dev_type years_code years_code_pro\n        &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1         164 I am a develop… 25-3… Employed,… Remote      Hobby;Profession… Bachelo… Enginee…         14             10\n2         165 I am a develop… 18-2… Employed,… Hybrid (so… Hobby;School or … Bachelo… Data sc…          7              5\n3         190 I am a develop… 35-4… Employed,… Hybrid (so… Hobby;Profession… Profess… Data sc…          8              8\n4         218 I am a develop… 35-4… Employed,… Hybrid (so… Professional dev… Some co… Databas…         29             17\n5         220 I am a develop… 25-3… Employed,… Hybrid (so… Hobby;Contribute… Bachelo… Enginee…          7              6\n6         462 I am a develop… 45-5… Employed,… Hybrid (so… Hobby;Contribute… Master’… Data sc…         25             20\n# ℹ 14 more variables: org_size &lt;chr&gt;, country &lt;chr&gt;, language_have_worked_with &lt;chr&gt;,\n#   language_want_to_work_with &lt;chr&gt;, op_sys_professional_use &lt;chr&gt;, ai_view &lt;chr&gt;, ai_trust &lt;chr&gt;,\n#   ic_or_manager &lt;chr&gt;, work_exp &lt;dbl&gt;, industry &lt;chr&gt;, us_or_not &lt;chr&gt;, plans_to_use_ai &lt;chr&gt;,\n#   converted_comp_yearly &lt;dbl&gt;, survey_completion_date &lt;date&gt;\n\n\n\n\n\n\n\n# Use the $ and basic arithmetic operations in R to determine a new vector\n# called ratio_code_pro that is the ratio of years_code_pro to\n# years_code from data_dev_survey\nratio_code_pro &lt;- data_dev_survey$years_code_pro / data_dev_survey$years_code\n\n# Print out the first six entries of your new vector\nhead(ratio_code_pro)\n\n[1] 0.7142857 0.7142857 1.0000000 0.5862069 0.8571429 0.8000000\n\n\n\n\n\n\n\n# Check ?View to understand its arguments\n?View\n\n# Provide the names of the arguments to view the dataset and give it the name \"Survey Data\"\nView(title = \"Survey Data\", x = data_dev_survey) # Note you can switch the order around!\n\n# Do the same thing again but using positional arguments instead\nView(data_dev_survey, \"Survey Data\")\n\n\n\n\n\n\n\n\n\n# Load required packages for data visualization\nlibrary(ggplot2) # Not necessary if already loaded\nlibrary(moderndive) # Not necessary if already loaded\n\n# TIP: You can install packages if not already installed\n# install.packages(c(\"ggplot2\", \"moderndive\"))\n\n\n\n\n\n\n# Create a histogram of years of pro coding experience\nggplot(data_dev_survey, aes(x = years_code_pro)) +\n  geom_histogram(binwidth = 2, fill = \"orange\", color = \"black\") +\n  labs(title = \"Distribution of Professional Coding Experience\", \n       x = \"Years of Coding Experience\", y = \"Count\")\n\n\n\n\n\n\n\n\n\n# Create a boxplot of years of coding experience by developer type\nggplot(data_dev_survey, aes(x = dev_type, y = years_code_pro)) +\n  geom_boxplot(fill = \"lightgreen\") +\n  labs(title = \"Years of Professional Coding Experience by Developer Type\", \n       y = \"Years of Coding Experience\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n# Create a barplot of developer types\nggplot(data_dev_survey, aes(x = dev_type)) +\n  geom_bar(fill = \"lightblue\") +\n  labs(title = \"Distribution of Employment Types\", \n       x = \"Employment Type\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1))\n\n\n\n\n\n\n\n\nApply some alpha to check for overplotting!\n\n# Scatterplot showing the relationship between salary and years of professional \n# coding experience\nggplot(data_dev_survey, aes(x = years_code_pro, y = converted_comp_yearly)) +\n  geom_point(color = \"purple\", alpha = 0.3) +\n  labs(title = \"Salary vs. Years of Professional Coding Experience\", \n       x = \"Years of Professional Coding\", \n       y = \"Yearly Salary (USD)\")\n\n\n\n\nCheck out Subsection 2.3.2 of ModernDive V2 for overplotting discussion.\n\n\n\n\n\n# Faceted barplot showing age distribution across developer types\nggplot(data_dev_survey, aes(x = age)) +\n  geom_bar(fill = \"skyblue\") +\n  facet_wrap(~dev_type) +\n  labs(title = \"Age by Developer Type\", \n       x = \"Age\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n# Pie chart of remote work preferences\nggplot(data_dev_survey, aes(x = \"\", fill = remote_work)) +\n  geom_bar(width = 1) +\n  coord_polar(theta = \"y\") +\n  labs(title = \"Remote Work Preferences\")\n\n\n\n\nThe pie chart visualizes how respondents prefer to work (e.g., remote, hybrid, in-person). Given the small numbers of categories a pie chart can work OK here, but a bar chart is often better for comparisons.\n\n# Horizontal bar chart of remote work preferences\nggplot(data_dev_survey, aes(x = remote_work, fill = remote_work)) +\n  geom_bar() +\n  coord_flip() +\n  labs(title = \"Remote Work Preferences\")\n\n\n\n\n\n\n\n\nModify the code below to look at births in 2012 and 2013 instead of 2014.\n\nlibrary(fivethirtyeight)\nlibrary(lubridate)\n\n# Old Code\n\n# Filter the data for 2014\nUS_births_2014 &lt;- US_births_2000_2014 |&gt;\n  filter(year(date) == 2014)\n\n# Create a line graph showing the number of births in 2014\nggplot(US_births_2014, aes(x = date, y = births)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Daily U.S. Births in 2014\", \n       x = \"Date\", y = \"Number of Births\")\n\n\n\n# Revised code\n# Filter the data for 2012 and 2013\nUS_births_2012_2013 &lt;- US_births_2000_2014 |&gt;\n  filter(year(date) == 2012 | year(date) == 2013)\n\n# This can also be done with the `%in%` operator\nUS_births_2012_2013 &lt;- US_births_2000_2014 |&gt;\n  filter(year(date) %in% c(2012, 2013))\n\n# Create a line graph showing the number of births in 2014\nggplot(US_births_2012_2013, aes(x = date, y = births)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Daily U.S. Births in 2012 and 2013\", \n       x = \"Date\", y = \"Number of Births\")\n\n\n\n\n\n\n\n\n\n\n\n\n# Filter 18 to 24 year olds with less than 2 years of coding\nyoung_coders &lt;- data_dev_survey |&gt;\n  filter(age == \"18-24 years old\", years_code &lt; 2)\nyoung_coders\n\n# A tibble: 5 × 24\n  response_id work_as_dev     age   employment remote_work coding_activities ed_level dev_type years_code years_code_pro\n        &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1       52658 I am not prima… 18-2… Employed,… Hybrid (so… Hobby;Bootstrapp… Bachelo… Data or…          1              0\n2       54834 I am not prima… 18-2… Employed,… Remote      Contribute to op… Bachelo… Data or…          1              0\n3       56787 I am a develop… 18-2… Employed,… Remote      Hobby;Profession… Somethi… Enginee…          1              0\n4       71528 I am a develop… 18-2… Employed,… In-person   Hobby;Bootstrapp… Associa… Data or…          1              0\n5       84566 I am a develop… 18-2… Employed,… Hybrid (so… Hobby             Bachelo… Enginee…          1              0\n# ℹ 14 more variables: org_size &lt;chr&gt;, country &lt;chr&gt;, language_have_worked_with &lt;chr&gt;,\n#   language_want_to_work_with &lt;chr&gt;, op_sys_professional_use &lt;chr&gt;, ai_view &lt;chr&gt;, ai_trust &lt;chr&gt;,\n#   ic_or_manager &lt;chr&gt;, work_exp &lt;dbl&gt;, industry &lt;chr&gt;, us_or_not &lt;chr&gt;, plans_to_use_ai &lt;chr&gt;,\n#   converted_comp_yearly &lt;dbl&gt;, survey_completion_date &lt;date&gt;\n\n\n\n\n\n\n\n# Summarize average salary and standard deviation of salary\nsummary_stats &lt;- data_dev_survey |&gt; \n  summarize(\n    avg_salary = mean(converted_comp_yearly, na.rm = TRUE),\n    std_salary = sd(converted_comp_yearly, na.rm = TRUE)\n  )\nsummary_stats\n\n# A tibble: 1 × 2\n  avg_salary std_salary\n       &lt;dbl&gt;      &lt;dbl&gt;\n1     90684.     81896.\n\n\n\n\n\n\n\n# Use kable() on your summarized data to make it look nice\nlibrary(knitr)\n\n# Group by remote_work and summarize the average salary \n# and standard deviation of salary\nsalary_by_remoteness &lt;- data_dev_survey |&gt; \n  group_by(remote_work) |&gt; \n  summarize(\n    avg_salary = mean(converted_comp_yearly, na.rm = TRUE),\n    std_salary = sd(converted_comp_yearly, na.rm = TRUE)\n  )\nkable(salary_by_remoteness)\n\n\n\n\nremote_work\navg_salary\nstd_salary\n\n\n\n\nHybrid (some remote, some in-person)\n90133.84\n75159.61\n\n\nIn-person\n60711.75\n65364.24\n\n\nRemote\n102048.93\n92879.35\n\n\n\n\n\n\n\n\n\n\n# Create a new variable called coding_experience_category\n# that categorizes developers by years of professional coding experience\n# More than 20 = \"Veteran\"\n# More than 10 = \"Experienced\"\n# More than 5 = \"Intermediate\"\n# Otherwise = \"Junior\"\ndata_dev_survey &lt;- data_dev_survey |&gt; \n  mutate(\n    coding_experience_category = case_when(\n      years_code_pro &gt; 20 ~ \"Veteran\",\n      years_code_pro &gt; 10 ~ \"Experienced\",\n      years_code_pro &gt; 5 ~ \"Intermediate\",\n      TRUE ~ \"Junior\"\n    ), .after = years_code_pro\n  )\n\n# Can also do this more explicitly using the `between()` function\n# which is a shortcut for x &gt;= left & x &lt;= right\n# Since the bounds are inclusive, we need to adjust the values\ndata_dev_survey &lt;- data_dev_survey |&gt; \n  mutate(\n    coding_experience_category = case_when(\n      years_code_pro &gt; 20 ~ \"Veteran\",\n      between(years_code_pro, 11, 20) ~ \"Experienced\",\n      between(years_code_pro, 6, 10) ~ \"Intermediate\",\n      TRUE ~ \"Junior\"\n    ), .after = years_code_pro\n  )\n\n\n\n\n\n\n# Arrange respondents by years coding in descending order\ndata_dev_survey |&gt; \n  arrange(desc(years_code))\n\n# A tibble: 1,183 × 25\n   response_id work_as_dev    age   employment remote_work coding_activities ed_level dev_type years_code years_code_pro\n         &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n 1       33252 I am a develo… 65 y… Employed,… Remote      Hobby             Master’… Data sc…         50             50\n 2       32151 I am a develo… 55-6… Employed,… Remote      I don’t code out… Some co… Databas…         47             45\n 3       88354 I am a develo… 55-6… Employed,… Hybrid (so… Hobby;Freelance/… Master’… Enginee…         46             38\n 4       40023 I am a develo… 65 y… Employed,… Remote      I don’t code out… Some co… Databas…         45             42\n 5       38567 I am not prim… 45-5… Independe… Hybrid (so… Hobby;Profession… Associa… Databas…         44             35\n 6       28489 I am a develo… 55-6… Employed,… Remote      Hobby;Contribute… Bachelo… Enginee…         43             40\n 7       36249 I am a develo… 55-6… Employed,… In-person   Hobby             Bachelo… Data or…         42             30\n 8       10923 I am a develo… 45-5… Employed,… Hybrid (so… I don’t code out… Bachelo… Enginee…         41             33\n 9       50951 I am not prim… 55-6… Employed,… Remote      Hobby;Contribute… Master’… Data or…         41             35\n10       81625 I am not prim… 65 y… Employed,… Hybrid (so… Professional dev… Bachelo… Enginee…         41             38\n# ℹ 1,173 more rows\n# ℹ 15 more variables: coding_experience_category &lt;chr&gt;, org_size &lt;chr&gt;, country &lt;chr&gt;,\n#   language_have_worked_with &lt;chr&gt;, language_want_to_work_with &lt;chr&gt;, op_sys_professional_use &lt;chr&gt;, ai_view &lt;chr&gt;,\n#   ai_trust &lt;chr&gt;, ic_or_manager &lt;chr&gt;, work_exp &lt;dbl&gt;, industry &lt;chr&gt;, us_or_not &lt;chr&gt;, plans_to_use_ai &lt;chr&gt;,\n#   converted_comp_yearly &lt;dbl&gt;, survey_completion_date &lt;date&gt;\n\n\n\n\n\n\n\n# After doing the previous arranging, select id, developer type, and professional\n# coding experience columns\nselected_data &lt;- data_dev_survey |&gt; \n  arrange(desc(years_code)) |&gt; \n  select(response_id, dev_type, years_code_pro)\nselected_data\n\n# A tibble: 1,183 × 3\n   response_id dev_type                                      years_code_pro\n         &lt;dbl&gt; &lt;chr&gt;                                                  &lt;dbl&gt;\n 1       33252 Data scientist or machine learning specialist             50\n 2       32151 Database administrator                                    45\n 3       88354 Engineer, data                                            38\n 4       40023 Database administrator                                    42\n 5       38567 Database administrator                                    35\n 6       28489 Engineer, data                                            40\n 7       36249 Data or business analyst                                  30\n 8       10923 Engineer, data                                            33\n 9       50951 Data or business analyst                                  35\n10       81625 Engineer, data                                            38\n# ℹ 1,173 more rows\n\n\n\n\n\n\n\n# Create a pipeline that filters, groups, summarizes, mutates, arranges, \n# and selects columns for Database administrators and Data Engineers\n# Group by country and summarize average years of professional coding experience\n# and average salary, categorizing salary as High (&gt; 100000), \n# Medium (&gt;5e4), or Low\n# Sort by descending years of average professional coding experience\n# Choose country, Average Years Coding, Average Salary, and salary_category \n# columns\ndata_dev_survey |&gt;\n  filter(dev_type %in% c(\"Database administrator\", \"Engineer, data\")) |&gt;\n  group_by(country) |&gt;\n  summarize(\n    avg_years_code_pro = mean(years_code_pro, na.rm = TRUE),\n    avg_salary = mean(converted_comp_yearly, na.rm = TRUE)\n  ) |&gt;\n  ungroup() |&gt; \n  mutate(\n    salary_category = case_when(\n      avg_salary &gt; 1e5 ~ \"High\",\n      avg_salary &gt; 5e4 ~ \"Medium\",\n      TRUE ~ \"Low\"\n    )\n  ) |&gt;\n  arrange(desc(avg_years_code_pro)) |&gt;\n  select(country, \n         `Average Years Coding` = avg_years_code_pro, \n         `Average Salary` = avg_salary, \n         salary_category)\n\n# A tibble: 72 × 4\n   country            `Average Years Coding` `Average Salary` salary_category\n   &lt;chr&gt;                               &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;          \n 1 Norway                               20             90613  Medium         \n 2 Argentina                            19             26544. Low            \n 3 Russian Federation                   17              6188  Low            \n 4 China                                16                 3  Low            \n 5 South Africa                         16             58655  Medium         \n 6 Belgium                              15.3          114390. High           \n 7 Finland                              15.2           72018  Medium         \n 8 Italy                                15.1           51556  Medium         \n 9 Cuba                                 15             24000  Low            \n10 Bolivia                              14             60000  Medium         \n# ℹ 62 more rows\n\n\n\n\n\n\n# Create a data frame for geom_col() to count the \n# number of respondents per age group\nage_counts &lt;- data_dev_survey |&gt; \n  group_by(age) |&gt; \n  summarize(num_respondents = n()) |&gt; \n  arrange(desc(num_respondents))\n\n# Use x = reorder(age, num_respondents) to order the age groups by \n# number of respondents and make a horizontal bar chart\nggplot(age_counts, aes(x = reorder(age, num_respondents), y = num_respondents)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Number of Respondents by Age Group\", \n       x = \"Age Group\", y = \"Number of Respondents\")\n\n\n\n\n\n\n\n\n# Convert wide data into tidy data\nlibrary(tibble)\nlibrary(tidyr)\n\n# Collected from https://data.worldbank.org/?locations=CA-AR-AU\n# CO2 emissions (metric tons per capita)\nwide_co2_emissions &lt;- tibble(\n  country = c(\"Canada\", \"Argentina\", \"Australia\"),\n  `2017` = c(15.5, 4.1, 16.1),\n  `2018` = c(15.6, 4, 15.9),\n  `2019` = c(15, 3.7, 15.6),\n  `2020` = c(13.6, 3.4, 14.8)\n)\n\n# Use pivot_longer to convert data to tidy format making sure to turn year\n# into an integer variable\ntidy_co2 &lt;- wide_co2_emissions |&gt; \n  pivot_longer(cols = -country, \n               names_to = \"year\", \n               values_to = \"co2_emissions\") |&gt; \n  mutate(year = as.integer(year))\n\n# Plot data as a linegraph\nggplot(tidy_co2, aes(x = year, y = co2_emissions, color = country)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"CO2 Emissions Over Time\", \n       x = \"Year\", y = \"CO2 Emissions\") +\n  scale_x_continuous(breaks = c(2017, 2018, 2019, 2020))\n\n\n\n\n\n\n\n\n# Check out the help file for read_csv to learn about the default arguments\n# such as col_names = TRUE that are set automatically if not specified\n?read_csv"
  },
  {
    "objectID": "answers/day1_exercise_answers.html#session-1-introduction-to-r-and-rstudio",
    "href": "answers/day1_exercise_answers.html#session-1-introduction-to-r-and-rstudio",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "We did this during the course in walkthroughs, but if you haven’t done it yet:\n\nYou need to install R first from https://cloud.r-project.org/ and then install RStudio from https://posit.co/download/rstudio-desktop/.\nOnce installed, work in RStudio to interact with R efficiently.\n\n\n\n\n\n\nIn RStudio, you will see three panes: Console, Environment, and Files.\nThe Console is where you type and run your R code.\nThe Environment pane shows all objects (like datasets) currently in memory.\nThe Files pane helps you navigate files in your project.\n\n\n\n\n\nWe also did this during the walkthroughs, but if you haven’t done it yet:\n\n# To install a package, use the install.packages() function.\ninstall.packages(c(\"moderndive\", \"dplyr\", \"ggplot2\", \"readr\", \"tidyr\", \n                   \"lubridate\", \"fivethirtyeight\", \"knitr\"))\n\nThe c() function creates a vector in R, which is a sequence of elements. Here, it combines the package names into a single vector for installation.\n\n\n\n\n\n# To load a package, use the library() function.\nlibrary(moderndive)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(lubridate)\nlibrary(fivethirtyeight)\nlibrary(knitr)\n\n\n\n\n\n\n# Load the data_dev_survey dataset \n# (if you don't have it still loaded from the end of class)\ndata_dev_survey &lt;- read_csv(\"../data_dev_survey.csv\")\n\nRows: 1183 Columns: 24\n── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (18): work_as_dev, age, employment, remote_work, coding_activities, ed_level, dev_type, org_size, country, lang...\ndbl   (5): response_id, years_code, years_code_pro, work_exp, converted_comp_yearly\ndate  (1): survey_completion_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# View the data_dev_survey data\nView(data_dev_survey)\n\nThe Stack Overflow survey data stored in data_dev_survey includes 1,183 responses from professionals in the data and tech fields, with information on their employment, coding activities, education level, and technical skills. The dataset provides a comprehensive view of respondents’ experience levels, preferred programming languages, work environments, and AI perspectives. It captures various demographic and professional details, including their country of work, salary, and views on AI, making it a valuable resource for analyzing trends in the data science and tech professions.\n\nIdentification variable(s): response_id\nMeasurement variable(s): remaining columns\n\n\n\n\n\nEach column in a dataset has a data type, such as:\n\nchr for character (text)\n\ndbl for numeric (decimal values)\n\nlgl for logical (TRUE/FALSE)\n\nint for integer (non-decimal values)\n\nfct for factor (categorical)\n\nglimpse() from the dplyr package gives a preview of the data types and the first few entries in each column.\n\n# To see an overview of the dataset’s structure:\nglimpse(data_dev_survey)\n\nRows: 1,183\nColumns: 24\n$ response_id                &lt;dbl&gt; 164, 165, 190, 218, 220, 462, 542, 558, 577, 802, 815, 885, 1099, 1187, 1210, 1212,…\n$ work_as_dev                &lt;chr&gt; \"I am a developer by profession\", \"I am a developer by profession\", \"I am a develop…\n$ age                        &lt;chr&gt; \"25-34 years old\", \"18-24 years old\", \"35-44 years old\", \"35-44 years old\", \"25-34 …\n$ employment                 &lt;chr&gt; \"Employed, full-time;Independent contractor, freelancer, or self-employed\", \"Employ…\n$ remote_work                &lt;chr&gt; \"Remote\", \"Hybrid (some remote, some in-person)\", \"Hybrid (some remote, some in-per…\n$ coding_activities          &lt;chr&gt; \"Hobby;Professional development or self-paced learning from online courses;Freelanc…\n$ ed_level                   &lt;chr&gt; \"Bachelor’s degree (B.A., B.S., B.Eng., etc.)\", \"Bachelor’s degree (B.A., B.S., B.E…\n$ dev_type                   &lt;chr&gt; \"Engineer, data\", \"Data scientist or machine learning specialist\", \"Data scientist …\n$ years_code                 &lt;dbl&gt; 14, 7, 8, 29, 7, 25, 13, 8, 22, 10, 3, 13, 8, 19, 3, 3, 7, 10, 15, 7, 27, 40, 15, 4…\n$ years_code_pro             &lt;dbl&gt; 10, 5, 8, 17, 6, 20, 3, 7, 13, 10, 2, 12, 3, 19, 2, 1, 4, 5, 5, 2, 25, 30, 10, 4, 3…\n$ org_size                   &lt;chr&gt; \"1,000 to 4,999 employees\", \"10 to 19 employees\", \"500 to 999 employees\", \"1,000 to…\n$ country                    &lt;chr&gt; \"Romania\", \"Canada\", \"Cyprus\", \"Italy\", \"Netherlands\", \"Switzerland\", \"Germany\", \"U…\n$ language_have_worked_with  &lt;chr&gt; \"HTML/CSS;Java;JavaScript;R;SQL;VBA\", \"C;C++;HTML/CSS;JavaScript;Python;TypeScript\"…\n$ language_want_to_work_with &lt;chr&gt; \"PowerShell;Python;SQL\", \"C;C++;Python\", \"Bash/Shell (all shells);HTML/CSS;Python;R…\n$ op_sys_professional_use    &lt;chr&gt; \"Ubuntu;Windows\", \"Ubuntu\", \"Ubuntu;Windows\", \"Red Hat;Windows\", \"MacOS;Windows\", \"…\n$ ai_view                    &lt;chr&gt; \"Very favorable\", \"Very favorable\", \"Favorable\", \"Very favorable\", \"Favorable\", \"Fa…\n$ ai_trust                   &lt;chr&gt; \"Somewhat trust\", \"Somewhat trust\", \"Somewhat trust\", \"Somewhat trust\", \"Somewhat t…\n$ ic_or_manager              &lt;chr&gt; \"Individual contributor\", \"Individual contributor\", \"Individual contributor\", \"Indi…\n$ work_exp                   &lt;dbl&gt; 10, 7, 16, 17, 7, 25, 3, 7, 13, 10, 10, 13, 10, 19, 3, 14, 3, 5, 5, 2, 25, 30, 11, …\n$ industry                   &lt;chr&gt; \"Information Services, IT, Software Development, or other Technology\", \"Information…\n$ us_or_not                  &lt;chr&gt; \"Not US\", \"Not US\", \"Not US\", \"Not US\", \"Not US\", \"Not US\", \"Not US\", \"US\", \"Not US…\n$ plans_to_use_ai            &lt;chr&gt; \"Using\", \"Using\", \"Using\", \"Plan to use\", \"Using\", \"Plan to use\", \"Using\", \"Plan to…\n$ converted_comp_yearly      &lt;dbl&gt; 3237, 52046, 74963, 56757, 74963, 132260, 117798, 165974, 64254, 71779, 36351, 8134…\n$ survey_completion_date     &lt;date&gt; 2023-05-02, 2023-05-05, 2023-05-13, 2023-05-28, 2023-05-31, 2023-05-17, 2023-05-13…\n\n\n\n\n\n\n\n# Access the work_exp column from the data_dev_survey dataset\ndata_dev_survey$work_exp\n\n   [1] 10  7 16 17  7 25  3  7 13 10 10 13 10 19  3 14  3  5  5  2 25 30 11  2  3 26  2  2  7  3  6 13 20 13  3 13  6 10\n  [39]  3 15  3  8  8  8 10 16  5  6 17 10 10  5 15  4  4  5  8  2  2 12  9  5 10  8 11 19 10 15 35  4  6  6 20  6 24  5\n  [77] 21  5  8  5  4  2  2 15 25 15  3 20 13 20 12  1  7 15  1  6  6  9  6  6  6  2  6 10  1 15  7  2  6  5 10  2  6  1\n [115]  3 10  5  4  5  5  2 22  4 10  1  8  3 18  2  4 13  1 11  2  5 22  5 11  1  3 11  6 33  6  7  4  2  2  2 12  6  2\n [153] 25  7 11  3 10 13  5 16 24 35  5  9 10 10  3 20  6  2 13  2 16  5 25  5  2 20 10 20 20 12  9  8  2  7  2 19 15 19\n [191] 15  5  7 10 13 30  6  4  4  2 34  5  9  2  9  6  3  3 14 10  8  5 26 10  4 28  9  6 13  8  2 18 14  7 10 21 13  7\n [229]  6 10 27  7  7  5 13  6  5  3 10  2 28  3 25  3 15  4 10  9 10  6  6  2  4 10  3  2  5  5  8  5  8  1 11  8  2 12\n [267]  1  6  7 29 10  3  7  2 13  4  9  3  6  1  5  1  5 25 15  3 27  5 22  2 25  4  2  5 15  8  2 20  1  6 19  4  1  6\n [305] 21  0 10 10 12 15 13  8  3  6 10 10  8 36  2 12 35 12  3  3  7 18 10  1  8 11  8 13  2 10  7  8 10 12 16 10  6  7\n [343]  2 14 24 40 26 15 16  4 20  4 27  8  3  3 12 15 10 15  1  3  5 10 10  9 17 24 25  8  5  9  2  2  3 14 40  4 11  7\n [381]  3 13  5 25  6  5 15  2  6 25  0  6  9 12 28 23  3  5  1  1  9 10  2  5 31 23  2  0 16 18  6  6 12  9  4  2 15 25\n [419] 18  4  5  3  8  7 28  1 10  4 47 15 27  5 30  4  4  7 25  7  6  7  1  6  4 10 15 48 13 12 10  5 15 13  2 25 25 10\n [457]  4 16  5 10  2 11  7 14 23 10  6 15 25 19 25  1  8  2  9 27 15 28  3 10  5  5 10 10  1 19  2 12  7 35 23  4  1 10\n [495]  7 40  3  7 10 11  5  5 31 21 25  7 25 12 14 12  5  2 17 17  9 20  5  7  9 36  8  4 22 10 10 22  9  7  3 13  3 12\n [533]  7 15 23 10 13  3  1  5 15  6  1  7 23 10  8 42  5 20 14  7  2 11  8 25 35 16 24  7 10 10 12 19 21  4 16 12 16  3\n [571]  8  6  7 12 10 30 11 20 10 23  6  4 10  7  8  7 17 15 20  8 14  0  6  1 16  1 18 30 10  9 14  6 10 18 13  4 22  1\n [609]  2  8  4  5  8  3  4  4 30 30 26  5  5 11  4  7 15  5  5  7 20  4 10  8  3  5  3  2  5  1  3  7 31 20  1  5  6  3\n [647] 12  8 16  7  3  1  1  7  9 32 11  8  9  2  1  6  7 22  8 21  2 15 10 35  5  5 22 14 15  7  8 10 31 23 12 15  7 12\n [685]  9  6 38 12  5 17  4 30  4  3 42  2  1  5  5 18  3 29  3 20 14  3 10  7  6  4 13 10 15  7  3 13  4  3  8  6  8  4\n [723] 15  3  5  7 17  3  1 26 11  2  5 20  1  1 17  5 25 18  3  4 15  6 17 22  1 26 11 10  2 20  2 18  3  3  3  2 14  1\n [761]  6  9  4 40  4 12  9  5  1  9  1  4  8 10  6  8  2  3  9  3 12  6  7  6  7 21 20 21  8 14  7  6 11 27  6  2  4  1\n [799] 13 10 30  3 12 10 20  3  6  2  5  2  9 17  7 10  6  3  1  7  7  8  2 25  7 16  7 12 10 12 12  0  9  5  6  4  3  5\n [837]  7  6  2  2  2  8  9 13  7  4  9 35  8  2 10  2 12  3  7  6  6  8  1  7  3  4  3 15 10  6 13  3 15  1 10 10  6 15\n [875]  5  4  7 11  6 13  7  4 16  9 14  3  6  8 10 13 25  6 23 17  5  8  9  8  4  8  4 10 15 13  0 15  1 10  5 11 16 10\n [913]  8  2  1 12  3 15  1  9 10  9  6 14  5 20  9 30 16 12  3  5  2  1 10  5  1 23  2  2  4 34 14  4  5  6 14  3  5  9\n [951]  4  3  9  6 12 19  9 19 12  3  5 10  4  4  5  5 35  4  1  8  6  3 13  5  8  3  7 25  2  5  1  4  5  5 14 20 10  3\n [989]  5 11  2  1  2 12  4  8  3  8  5  9  4  3  8  4  2  9  1 10 15  8  7  3 10 22 16 15 23  2  8  6 10  9  3  2  6 25\n[1027] 12  3 12  7  5  5  1  2  5  8  3  2  8  6  3  3  8  2 10  3 25  4  5  6  4  1  1  2 15  6  6  4  7 10 15  4  1 46\n[1065] 10  8  5 11  1 15  5  1  4 33  2  6 39 15 10  1 11  5  9  7 15  7  3  2  5  1  4  2  9  4  8  3  0 20  5  5 25 12\n[1103] 31  3  2 12  2 14  5 25  6  3  4 11  2  1  2  1  1  4  1 14  5  3  2  4  7  2  1  1  4  1  8 11  5  1  1  2  3 16\n[1141]  2  4 11  7  6 11  2 20 12  5  1  3  3  3  4 30  5  2 30  8  4  4 15 15  3  3 10  8  6  7  5  1 47 25  4 13  1  8\n[1179]  3  8  2 11 39\n\n\n\n\n\n\n\n# To quickly see the first 6 rows of the dataset:\nhead(data_dev_survey)\n\n# A tibble: 6 × 24\n  response_id work_as_dev     age   employment remote_work coding_activities ed_level dev_type years_code years_code_pro\n        &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1         164 I am a develop… 25-3… Employed,… Remote      Hobby;Profession… Bachelo… Enginee…         14             10\n2         165 I am a develop… 18-2… Employed,… Hybrid (so… Hobby;School or … Bachelo… Data sc…          7              5\n3         190 I am a develop… 35-4… Employed,… Hybrid (so… Hobby;Profession… Profess… Data sc…          8              8\n4         218 I am a develop… 35-4… Employed,… Hybrid (so… Professional dev… Some co… Databas…         29             17\n5         220 I am a develop… 25-3… Employed,… Hybrid (so… Hobby;Contribute… Bachelo… Enginee…          7              6\n6         462 I am a develop… 45-5… Employed,… Hybrid (so… Hobby;Contribute… Master’… Data sc…         25             20\n# ℹ 14 more variables: org_size &lt;chr&gt;, country &lt;chr&gt;, language_have_worked_with &lt;chr&gt;,\n#   language_want_to_work_with &lt;chr&gt;, op_sys_professional_use &lt;chr&gt;, ai_view &lt;chr&gt;, ai_trust &lt;chr&gt;,\n#   ic_or_manager &lt;chr&gt;, work_exp &lt;dbl&gt;, industry &lt;chr&gt;, us_or_not &lt;chr&gt;, plans_to_use_ai &lt;chr&gt;,\n#   converted_comp_yearly &lt;dbl&gt;, survey_completion_date &lt;date&gt;\n\n\n\n\n\n\n\n# Use the $ and basic arithmetic operations in R to determine a new vector\n# called ratio_code_pro that is the ratio of years_code_pro to\n# years_code from data_dev_survey\nratio_code_pro &lt;- data_dev_survey$years_code_pro / data_dev_survey$years_code\n\n# Print out the first six entries of your new vector\nhead(ratio_code_pro)\n\n[1] 0.7142857 0.7142857 1.0000000 0.5862069 0.8571429 0.8000000\n\n\n\n\n\n\n\n# Check ?View to understand its arguments\n?View\n\n# Provide the names of the arguments to view the dataset and give it the name \"Survey Data\"\nView(title = \"Survey Data\", x = data_dev_survey) # Note you can switch the order around!\n\n# Do the same thing again but using positional arguments instead\nView(data_dev_survey, \"Survey Data\")"
  },
  {
    "objectID": "answers/day1_exercise_answers.html#session-2-data-visualization-with-ggplot2",
    "href": "answers/day1_exercise_answers.html#session-2-data-visualization-with-ggplot2",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# Load required packages for data visualization\nlibrary(ggplot2) # Not necessary if already loaded\nlibrary(moderndive) # Not necessary if already loaded\n\n# TIP: You can install packages if not already installed\n# install.packages(c(\"ggplot2\", \"moderndive\"))\n\n\n\n\n\n\n# Create a histogram of years of pro coding experience\nggplot(data_dev_survey, aes(x = years_code_pro)) +\n  geom_histogram(binwidth = 2, fill = \"orange\", color = \"black\") +\n  labs(title = \"Distribution of Professional Coding Experience\", \n       x = \"Years of Coding Experience\", y = \"Count\")\n\n\n\n\n\n\n\n\n\n# Create a boxplot of years of coding experience by developer type\nggplot(data_dev_survey, aes(x = dev_type, y = years_code_pro)) +\n  geom_boxplot(fill = \"lightgreen\") +\n  labs(title = \"Years of Professional Coding Experience by Developer Type\", \n       y = \"Years of Coding Experience\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n# Create a barplot of developer types\nggplot(data_dev_survey, aes(x = dev_type)) +\n  geom_bar(fill = \"lightblue\") +\n  labs(title = \"Distribution of Employment Types\", \n       x = \"Employment Type\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1))\n\n\n\n\n\n\n\n\nApply some alpha to check for overplotting!\n\n# Scatterplot showing the relationship between salary and years of professional \n# coding experience\nggplot(data_dev_survey, aes(x = years_code_pro, y = converted_comp_yearly)) +\n  geom_point(color = \"purple\", alpha = 0.3) +\n  labs(title = \"Salary vs. Years of Professional Coding Experience\", \n       x = \"Years of Professional Coding\", \n       y = \"Yearly Salary (USD)\")\n\n\n\n\nCheck out Subsection 2.3.2 of ModernDive V2 for overplotting discussion.\n\n\n\n\n\n# Faceted barplot showing age distribution across developer types\nggplot(data_dev_survey, aes(x = age)) +\n  geom_bar(fill = \"skyblue\") +\n  facet_wrap(~dev_type) +\n  labs(title = \"Age by Developer Type\", \n       x = \"Age\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n# Pie chart of remote work preferences\nggplot(data_dev_survey, aes(x = \"\", fill = remote_work)) +\n  geom_bar(width = 1) +\n  coord_polar(theta = \"y\") +\n  labs(title = \"Remote Work Preferences\")\n\n\n\n\nThe pie chart visualizes how respondents prefer to work (e.g., remote, hybrid, in-person). Given the small numbers of categories a pie chart can work OK here, but a bar chart is often better for comparisons.\n\n# Horizontal bar chart of remote work preferences\nggplot(data_dev_survey, aes(x = remote_work, fill = remote_work)) +\n  geom_bar() +\n  coord_flip() +\n  labs(title = \"Remote Work Preferences\")\n\n\n\n\n\n\n\n\nModify the code below to look at births in 2012 and 2013 instead of 2014.\n\nlibrary(fivethirtyeight)\nlibrary(lubridate)\n\n# Old Code\n\n# Filter the data for 2014\nUS_births_2014 &lt;- US_births_2000_2014 |&gt;\n  filter(year(date) == 2014)\n\n# Create a line graph showing the number of births in 2014\nggplot(US_births_2014, aes(x = date, y = births)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Daily U.S. Births in 2014\", \n       x = \"Date\", y = \"Number of Births\")\n\n\n\n# Revised code\n# Filter the data for 2012 and 2013\nUS_births_2012_2013 &lt;- US_births_2000_2014 |&gt;\n  filter(year(date) == 2012 | year(date) == 2013)\n\n# This can also be done with the `%in%` operator\nUS_births_2012_2013 &lt;- US_births_2000_2014 |&gt;\n  filter(year(date) %in% c(2012, 2013))\n\n# Create a line graph showing the number of births in 2014\nggplot(US_births_2012_2013, aes(x = date, y = births)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Daily U.S. Births in 2012 and 2013\", \n       x = \"Date\", y = \"Number of Births\")"
  },
  {
    "objectID": "answers/day1_exercise_answers.html#session-3-data-wrangling-and-tidy-data",
    "href": "answers/day1_exercise_answers.html#session-3-data-wrangling-and-tidy-data",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# Filter 18 to 24 year olds with less than 2 years of coding\nyoung_coders &lt;- data_dev_survey |&gt;\n  filter(age == \"18-24 years old\", years_code &lt; 2)\nyoung_coders\n\n# A tibble: 5 × 24\n  response_id work_as_dev     age   employment remote_work coding_activities ed_level dev_type years_code years_code_pro\n        &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1       52658 I am not prima… 18-2… Employed,… Hybrid (so… Hobby;Bootstrapp… Bachelo… Data or…          1              0\n2       54834 I am not prima… 18-2… Employed,… Remote      Contribute to op… Bachelo… Data or…          1              0\n3       56787 I am a develop… 18-2… Employed,… Remote      Hobby;Profession… Somethi… Enginee…          1              0\n4       71528 I am a develop… 18-2… Employed,… In-person   Hobby;Bootstrapp… Associa… Data or…          1              0\n5       84566 I am a develop… 18-2… Employed,… Hybrid (so… Hobby             Bachelo… Enginee…          1              0\n# ℹ 14 more variables: org_size &lt;chr&gt;, country &lt;chr&gt;, language_have_worked_with &lt;chr&gt;,\n#   language_want_to_work_with &lt;chr&gt;, op_sys_professional_use &lt;chr&gt;, ai_view &lt;chr&gt;, ai_trust &lt;chr&gt;,\n#   ic_or_manager &lt;chr&gt;, work_exp &lt;dbl&gt;, industry &lt;chr&gt;, us_or_not &lt;chr&gt;, plans_to_use_ai &lt;chr&gt;,\n#   converted_comp_yearly &lt;dbl&gt;, survey_completion_date &lt;date&gt;\n\n\n\n\n\n\n\n# Summarize average salary and standard deviation of salary\nsummary_stats &lt;- data_dev_survey |&gt; \n  summarize(\n    avg_salary = mean(converted_comp_yearly, na.rm = TRUE),\n    std_salary = sd(converted_comp_yearly, na.rm = TRUE)\n  )\nsummary_stats\n\n# A tibble: 1 × 2\n  avg_salary std_salary\n       &lt;dbl&gt;      &lt;dbl&gt;\n1     90684.     81896.\n\n\n\n\n\n\n\n# Use kable() on your summarized data to make it look nice\nlibrary(knitr)\n\n# Group by remote_work and summarize the average salary \n# and standard deviation of salary\nsalary_by_remoteness &lt;- data_dev_survey |&gt; \n  group_by(remote_work) |&gt; \n  summarize(\n    avg_salary = mean(converted_comp_yearly, na.rm = TRUE),\n    std_salary = sd(converted_comp_yearly, na.rm = TRUE)\n  )\nkable(salary_by_remoteness)\n\n\n\n\nremote_work\navg_salary\nstd_salary\n\n\n\n\nHybrid (some remote, some in-person)\n90133.84\n75159.61\n\n\nIn-person\n60711.75\n65364.24\n\n\nRemote\n102048.93\n92879.35\n\n\n\n\n\n\n\n\n\n\n# Create a new variable called coding_experience_category\n# that categorizes developers by years of professional coding experience\n# More than 20 = \"Veteran\"\n# More than 10 = \"Experienced\"\n# More than 5 = \"Intermediate\"\n# Otherwise = \"Junior\"\ndata_dev_survey &lt;- data_dev_survey |&gt; \n  mutate(\n    coding_experience_category = case_when(\n      years_code_pro &gt; 20 ~ \"Veteran\",\n      years_code_pro &gt; 10 ~ \"Experienced\",\n      years_code_pro &gt; 5 ~ \"Intermediate\",\n      TRUE ~ \"Junior\"\n    ), .after = years_code_pro\n  )\n\n# Can also do this more explicitly using the `between()` function\n# which is a shortcut for x &gt;= left & x &lt;= right\n# Since the bounds are inclusive, we need to adjust the values\ndata_dev_survey &lt;- data_dev_survey |&gt; \n  mutate(\n    coding_experience_category = case_when(\n      years_code_pro &gt; 20 ~ \"Veteran\",\n      between(years_code_pro, 11, 20) ~ \"Experienced\",\n      between(years_code_pro, 6, 10) ~ \"Intermediate\",\n      TRUE ~ \"Junior\"\n    ), .after = years_code_pro\n  )\n\n\n\n\n\n\n# Arrange respondents by years coding in descending order\ndata_dev_survey |&gt; \n  arrange(desc(years_code))\n\n# A tibble: 1,183 × 25\n   response_id work_as_dev    age   employment remote_work coding_activities ed_level dev_type years_code years_code_pro\n         &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n 1       33252 I am a develo… 65 y… Employed,… Remote      Hobby             Master’… Data sc…         50             50\n 2       32151 I am a develo… 55-6… Employed,… Remote      I don’t code out… Some co… Databas…         47             45\n 3       88354 I am a develo… 55-6… Employed,… Hybrid (so… Hobby;Freelance/… Master’… Enginee…         46             38\n 4       40023 I am a develo… 65 y… Employed,… Remote      I don’t code out… Some co… Databas…         45             42\n 5       38567 I am not prim… 45-5… Independe… Hybrid (so… Hobby;Profession… Associa… Databas…         44             35\n 6       28489 I am a develo… 55-6… Employed,… Remote      Hobby;Contribute… Bachelo… Enginee…         43             40\n 7       36249 I am a develo… 55-6… Employed,… In-person   Hobby             Bachelo… Data or…         42             30\n 8       10923 I am a develo… 45-5… Employed,… Hybrid (so… I don’t code out… Bachelo… Enginee…         41             33\n 9       50951 I am not prim… 55-6… Employed,… Remote      Hobby;Contribute… Master’… Data or…         41             35\n10       81625 I am not prim… 65 y… Employed,… Hybrid (so… Professional dev… Bachelo… Enginee…         41             38\n# ℹ 1,173 more rows\n# ℹ 15 more variables: coding_experience_category &lt;chr&gt;, org_size &lt;chr&gt;, country &lt;chr&gt;,\n#   language_have_worked_with &lt;chr&gt;, language_want_to_work_with &lt;chr&gt;, op_sys_professional_use &lt;chr&gt;, ai_view &lt;chr&gt;,\n#   ai_trust &lt;chr&gt;, ic_or_manager &lt;chr&gt;, work_exp &lt;dbl&gt;, industry &lt;chr&gt;, us_or_not &lt;chr&gt;, plans_to_use_ai &lt;chr&gt;,\n#   converted_comp_yearly &lt;dbl&gt;, survey_completion_date &lt;date&gt;\n\n\n\n\n\n\n\n# After doing the previous arranging, select id, developer type, and professional\n# coding experience columns\nselected_data &lt;- data_dev_survey |&gt; \n  arrange(desc(years_code)) |&gt; \n  select(response_id, dev_type, years_code_pro)\nselected_data\n\n# A tibble: 1,183 × 3\n   response_id dev_type                                      years_code_pro\n         &lt;dbl&gt; &lt;chr&gt;                                                  &lt;dbl&gt;\n 1       33252 Data scientist or machine learning specialist             50\n 2       32151 Database administrator                                    45\n 3       88354 Engineer, data                                            38\n 4       40023 Database administrator                                    42\n 5       38567 Database administrator                                    35\n 6       28489 Engineer, data                                            40\n 7       36249 Data or business analyst                                  30\n 8       10923 Engineer, data                                            33\n 9       50951 Data or business analyst                                  35\n10       81625 Engineer, data                                            38\n# ℹ 1,173 more rows\n\n\n\n\n\n\n\n# Create a pipeline that filters, groups, summarizes, mutates, arranges, \n# and selects columns for Database administrators and Data Engineers\n# Group by country and summarize average years of professional coding experience\n# and average salary, categorizing salary as High (&gt; 100000), \n# Medium (&gt;5e4), or Low\n# Sort by descending years of average professional coding experience\n# Choose country, Average Years Coding, Average Salary, and salary_category \n# columns\ndata_dev_survey |&gt;\n  filter(dev_type %in% c(\"Database administrator\", \"Engineer, data\")) |&gt;\n  group_by(country) |&gt;\n  summarize(\n    avg_years_code_pro = mean(years_code_pro, na.rm = TRUE),\n    avg_salary = mean(converted_comp_yearly, na.rm = TRUE)\n  ) |&gt;\n  ungroup() |&gt; \n  mutate(\n    salary_category = case_when(\n      avg_salary &gt; 1e5 ~ \"High\",\n      avg_salary &gt; 5e4 ~ \"Medium\",\n      TRUE ~ \"Low\"\n    )\n  ) |&gt;\n  arrange(desc(avg_years_code_pro)) |&gt;\n  select(country, \n         `Average Years Coding` = avg_years_code_pro, \n         `Average Salary` = avg_salary, \n         salary_category)\n\n# A tibble: 72 × 4\n   country            `Average Years Coding` `Average Salary` salary_category\n   &lt;chr&gt;                               &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;          \n 1 Norway                               20             90613  Medium         \n 2 Argentina                            19             26544. Low            \n 3 Russian Federation                   17              6188  Low            \n 4 China                                16                 3  Low            \n 5 South Africa                         16             58655  Medium         \n 6 Belgium                              15.3          114390. High           \n 7 Finland                              15.2           72018  Medium         \n 8 Italy                                15.1           51556  Medium         \n 9 Cuba                                 15             24000  Low            \n10 Bolivia                              14             60000  Medium         \n# ℹ 62 more rows\n\n\n\n\n\n\n# Create a data frame for geom_col() to count the \n# number of respondents per age group\nage_counts &lt;- data_dev_survey |&gt; \n  group_by(age) |&gt; \n  summarize(num_respondents = n()) |&gt; \n  arrange(desc(num_respondents))\n\n# Use x = reorder(age, num_respondents) to order the age groups by \n# number of respondents and make a horizontal bar chart\nggplot(age_counts, aes(x = reorder(age, num_respondents), y = num_respondents)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Number of Respondents by Age Group\", \n       x = \"Age Group\", y = \"Number of Respondents\")\n\n\n\n\n\n\n\n\n# Convert wide data into tidy data\nlibrary(tibble)\nlibrary(tidyr)\n\n# Collected from https://data.worldbank.org/?locations=CA-AR-AU\n# CO2 emissions (metric tons per capita)\nwide_co2_emissions &lt;- tibble(\n  country = c(\"Canada\", \"Argentina\", \"Australia\"),\n  `2017` = c(15.5, 4.1, 16.1),\n  `2018` = c(15.6, 4, 15.9),\n  `2019` = c(15, 3.7, 15.6),\n  `2020` = c(13.6, 3.4, 14.8)\n)\n\n# Use pivot_longer to convert data to tidy format making sure to turn year\n# into an integer variable\ntidy_co2 &lt;- wide_co2_emissions |&gt; \n  pivot_longer(cols = -country, \n               names_to = \"year\", \n               values_to = \"co2_emissions\") |&gt; \n  mutate(year = as.integer(year))\n\n# Plot data as a linegraph\nggplot(tidy_co2, aes(x = year, y = co2_emissions, color = country)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"CO2 Emissions Over Time\", \n       x = \"Year\", y = \"CO2 Emissions\") +\n  scale_x_continuous(breaks = c(2017, 2018, 2019, 2020))\n\n\n\n\n\n\n\n\n# Check out the help file for read_csv to learn about the default arguments\n# such as col_names = TRUE that are set automatically if not specified\n?read_csv"
  },
  {
    "objectID": "answers/day2_exercise_answers.html",
    "href": "answers/day2_exercise_answers.html",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "We did this during the course in walkthroughs, but if you haven’t done it yet:\n\n# Load the required libraries\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(readr)\n\n\nThese packages provide tools for data wrangling, visualization, and modeling.\n\n\n\n\n\n\n# Load in the data_dev_survey CSV file as a data frame again\n# selecting the columns of interest keeping only complete rows of information\ndata_dev_survey &lt;- read_csv(\"../data_dev_survey.csv\") |&gt; \n  select(response_id, converted_comp_yearly, work_exp, \n         years_code, ai_trust) |&gt; \n  # This code sets the levels of the ai_trust column to be more intuitive\n  # Note this will change the baseline level for the factor!\n  mutate(ai_trust = factor(ai_trust, levels = c(\"Highly distrust\", \n                                                \"Somewhat distrust\",\n                                                \"Neither trust nor distrust\",\n                                                \"Somewhat trust\",\n                                                \"Highly trust\"))) |&gt; \n  na.omit()\n\nRows: 1183 Columns: 24\n── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (18): work_as_dev, age, employment, remote_work, coding_activities, ed_level, dev_type, org_size, country, lang...\ndbl   (5): response_id, years_code, years_code_pro, work_exp, converted_comp_yearly\ndate  (1): survey_completion_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nWe’re still exploring the data_dev_survey data from Day 1 here.\n\n\n\n\n\n\n# Use glimpse to remind yourself of the structure of the data\nglimpse(data_dev_survey)\n\nRows: 1,182\nColumns: 5\n$ response_id           &lt;dbl&gt; 164, 165, 190, 218, 220, 462, 542, 558, 577, 802, 815, 885, 1099, 1187, 1210, 1212, 1223…\n$ converted_comp_yearly &lt;dbl&gt; 3237, 52046, 74963, 56757, 74963, 132260, 117798, 165974, 64254, 71779, 36351, 81349, 10…\n$ work_exp              &lt;dbl&gt; 10, 7, 16, 17, 7, 25, 3, 7, 13, 10, 10, 13, 10, 19, 3, 14, 3, 5, 5, 2, 25, 30, 11, 2, 3,…\n$ years_code            &lt;dbl&gt; 14, 7, 8, 29, 7, 25, 13, 8, 22, 10, 3, 13, 8, 19, 3, 3, 7, 10, 15, 7, 27, 40, 15, 4, 7, …\n$ ai_trust              &lt;fct&gt; Somewhat trust, Somewhat trust, Somewhat trust, Somewhat trust, Somewhat trust, Neither …\n\n\n\n\n\n\n\n# Summarize mean and median values for converted_comp_yearly and work_exp\ndata_dev_survey |&gt;\n  summarize(mean_comp = mean(converted_comp_yearly),\n            median_comp = median(converted_comp_yearly),\n            mean_exp = mean(work_exp),\n            median_exp = median(work_exp))\n\n# A tibble: 1 × 4\n  mean_comp median_comp mean_exp median_exp\n      &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1    90702.      72768.     9.65          7\n\n\n\n\n\n\n\n# Use tidy_summary to get a clean summary of the two columns\ndata_dev_survey |&gt;\n  select(converted_comp_yearly, work_exp) |&gt;\n  tidy_summary()\n\n# A tibble: 2 × 11\n  column                    n group type      min     Q1     mean median     Q3     max       sd\n  &lt;chr&gt;                 &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 converted_comp_yearly  1182 &lt;NA&gt;  numeric     3 41390. 90702.   72768. 120000 1200000 81928.  \n2 work_exp               1182 &lt;NA&gt;  numeric     0     4      9.65     7      13      48     8.19\n\n\n\n\n\n\n\n# Compute the correlation between converted_comp_yearly and work_exp\ndata_dev_survey |&gt;\n  get_correlation(formula = converted_comp_yearly ~ work_exp)\n\n# A tibble: 1 × 1\n    cor\n  &lt;dbl&gt;\n1 0.277\n\n\n\n\n\n\n\n# Plot the relationship between work experience and yearly compensation\nggplot(data_dev_survey, aes(x = work_exp, y = converted_comp_yearly)) +\n  geom_point(alpha = 0.2) +\n  labs(x = \"Work Experience (years)\", y = \"Yearly Compensation (USD)\",\n       title = \"Scatterplot of Work Experience and Yearly Compensation\")\n\n\n\n\n\n\n\n\n\n# The same scatterplot with a regression line added\nggplot(data_dev_survey, aes(x = work_exp, y = converted_comp_yearly)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Work Experience (years)\", y = \"Yearly Compensation (USD)\",\n       title = \"Work Experience vs. Yearly Compensation with Regression Line\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Fit a linear regression model with compensation as the outcome\nsalary_slr_model &lt;- lm(converted_comp_yearly ~ work_exp, data = data_dev_survey)\n\n# Get the regression coefficients\ncoef(salary_slr_model)\n\n(Intercept)    work_exp \n  63973.960    2769.115 \n\n\n\n\n\n\n\n\n\n\n# Fit a simple linear regression model with ai_trust as a predictor on salary\ntrust_model &lt;- lm(converted_comp_yearly ~ ai_trust, data = data_dev_survey)\n\n# Get regression coefficients\ncoef(trust_model)\n\n                       (Intercept)          ai_trustSomewhat distrust ai_trustNeither trust nor distrust \n                         113080.86                          -14688.72                          -20883.40 \n            ai_trustSomewhat trust               ai_trustHighly trust \n                         -29605.58                          -35437.81 \n\n\n\n\n\n\n\n# Scatterplot with ai_trust as color for grouping on work_exp predicting\n# converted_comp_yearly\nggplot(data_dev_survey, \n       aes(x = work_exp, y = converted_comp_yearly, color = ai_trust)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Work Experience (years)\", y = \"Yearly Compensation (USD)\",\n       color = \"Trust in AI\",\n       title = \"Scatterplot of Work Experience and Yearly Compensation by Trust in AI\")\n\n\n\n\n\n\n\n\n\n# Summarize the target and regressors\ndata_dev_survey |&gt;\n  select(converted_comp_yearly, work_exp, ai_trust) |&gt;\n  tidy_summary()\n\n# A tibble: 7 × 11\n  column                    n group                      type      min     Q1     mean median     Q3     max       sd\n  &lt;chr&gt;                 &lt;int&gt; &lt;chr&gt;                      &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 converted_comp_yearly  1182 &lt;NA&gt;                       numeric     3 41390. 90702.   72768. 120000 1200000 81928.  \n2 work_exp               1182 &lt;NA&gt;                       numeric     0     4      9.65     7      13      48     8.19\n3 ai_trust                 66 Highly distrust            factor     NA    NA     NA       NA      NA      NA    NA   \n4 ai_trust                270 Somewhat distrust          factor     NA    NA     NA       NA      NA      NA    NA   \n5 ai_trust                319 Neither trust nor distrust factor     NA    NA     NA       NA      NA      NA    NA   \n6 ai_trust                489 Somewhat trust             factor     NA    NA     NA       NA      NA      NA    NA   \n7 ai_trust                 38 Highly trust               factor     NA    NA     NA       NA      NA      NA    NA   \n\n\n\n\n\n\n\n# Fit a multiple regression model with interaction\nmulti_model_interaction &lt;- lm(converted_comp_yearly ~ work_exp * ai_trust, \n                              data = data_dev_survey)\n\n# Get regression coefficients\ncoef(multi_model_interaction)\n\n                                (Intercept)                                    work_exp \n                                 94685.6495                                   1724.5513 \n                  ai_trustSomewhat distrust          ai_trustNeither trust nor distrust \n                                -21369.0390                                 -44973.0562 \n                     ai_trustSomewhat trust                        ai_trustHighly trust \n                                -31608.7285                                 -32523.5905 \n         work_exp:ai_trustSomewhat distrust work_exp:ai_trustNeither trust nor distrust \n                                   792.3242                                   3069.4610 \n            work_exp:ai_trustSomewhat trust               work_exp:ai_trustHighly trust \n                                   370.1143                                   -343.6176 \n\n\n\n\n\n\n\n# Scatterplot with regression lines by trust in AI\nggplot(data_dev_survey, aes(x = work_exp, y = converted_comp_yearly, color = ai_trust)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Work Experience (years)\", y = \"Yearly Compensation (USD)\",\n       color = \"Trust in AI\",\n       title = \"Work Experience vs. Yearly Compensation by Trust in AI\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Fit a multiple regression model without interaction terms\nmulti_model_no_interaction &lt;- lm(converted_comp_yearly ~ work_exp + ai_trust, \n                                 data = data_dev_survey)\n\n# Get regression coefficients\ncoef(multi_model_no_interaction)\n\n                       (Intercept)                           work_exp          ai_trustSomewhat distrust \n                         83491.170                           2774.034                         -12736.618 \nai_trustNeither trust nor distrust             ai_trustSomewhat trust               ai_trustHighly trust \n                        -15877.388                         -27030.100                         -36946.496 \n\n\n\n\n\n\n\n# Scatterplot with regression lines by trust in AI (parallel slopes)\nggplot(data_dev_survey, aes(x = work_exp, y = converted_comp_yearly, color = ai_trust)) +\n  geom_point(alpha = 0.5) +\n  geom_parallel_slopes(se = FALSE) +\n  labs(x = \"Work Experience (years)\", y = \"Yearly Compensation (USD)\",\n       color = \"Trust in AI\",\n       title = \"Work Experience vs. Yearly Compensation by Trust in AI\")\n\n\n\n\n\n\n\n\n\n# Use tidy_summary to get a clean summary of the data for a multiple regression\n# with work experience and years coding as regressors and yearly compensation as\n# the outcome\ndata_dev_survey |&gt;\n  select(converted_comp_yearly, work_exp, years_code) |&gt;\n  tidy_summary()\n\n# A tibble: 3 × 11\n  column                    n group type      min     Q1     mean median     Q3     max       sd\n  &lt;chr&gt;                 &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 converted_comp_yearly  1182 &lt;NA&gt;  numeric     3 41390. 90702.   72768. 120000 1200000 81928.  \n2 work_exp               1182 &lt;NA&gt;  numeric     0     4      9.65     7      13      48     8.19\n3 years_code             1182 &lt;NA&gt;  numeric     1     7     12.5     10      16      50     8.38\n\n\n\n\n\n\n\n# Fit a multiple regression model\nmulti_model &lt;- lm(converted_comp_yearly ~ work_exp + years_code, \n                  data = data_dev_survey)\n\n# Get regression coefficients\ncoef(multi_model)\n\n(Intercept)    work_exp  years_code \n  55764.220    1343.020    1753.121 \n\n\n\n\n\n\n\n# Create scatterplots with regression lines for both variables with \n# converted_comp_yearly as the response variable in simple linear regression\nggplot(data_dev_survey, aes(x = work_exp, y = converted_comp_yearly)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Work Experience (years)\", y = \"Yearly Compensation (USD)\",\n       title = \"Yearly Compensation vs Work Experience\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(data_dev_survey, aes(x = years_code, y = converted_comp_yearly)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Years Coding\", y = \"Yearly Compensation (USD)\",\n       title = \"Yearly Compensation vs Years Coding\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "answers/day2_exercise_answers.html#session-4-simple-linear-regression-analysis",
    "href": "answers/day2_exercise_answers.html#session-4-simple-linear-regression-analysis",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "We did this during the course in walkthroughs, but if you haven’t done it yet:\n\n# Load the required libraries\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(readr)\n\n\nThese packages provide tools for data wrangling, visualization, and modeling.\n\n\n\n\n\n\n# Load in the data_dev_survey CSV file as a data frame again\n# selecting the columns of interest keeping only complete rows of information\ndata_dev_survey &lt;- read_csv(\"../data_dev_survey.csv\") |&gt; \n  select(response_id, converted_comp_yearly, work_exp, \n         years_code, ai_trust) |&gt; \n  # This code sets the levels of the ai_trust column to be more intuitive\n  # Note this will change the baseline level for the factor!\n  mutate(ai_trust = factor(ai_trust, levels = c(\"Highly distrust\", \n                                                \"Somewhat distrust\",\n                                                \"Neither trust nor distrust\",\n                                                \"Somewhat trust\",\n                                                \"Highly trust\"))) |&gt; \n  na.omit()\n\nRows: 1183 Columns: 24\n── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (18): work_as_dev, age, employment, remote_work, coding_activities, ed_level, dev_type, org_size, country, lang...\ndbl   (5): response_id, years_code, years_code_pro, work_exp, converted_comp_yearly\ndate  (1): survey_completion_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nWe’re still exploring the data_dev_survey data from Day 1 here.\n\n\n\n\n\n\n# Use glimpse to remind yourself of the structure of the data\nglimpse(data_dev_survey)\n\nRows: 1,182\nColumns: 5\n$ response_id           &lt;dbl&gt; 164, 165, 190, 218, 220, 462, 542, 558, 577, 802, 815, 885, 1099, 1187, 1210, 1212, 1223…\n$ converted_comp_yearly &lt;dbl&gt; 3237, 52046, 74963, 56757, 74963, 132260, 117798, 165974, 64254, 71779, 36351, 81349, 10…\n$ work_exp              &lt;dbl&gt; 10, 7, 16, 17, 7, 25, 3, 7, 13, 10, 10, 13, 10, 19, 3, 14, 3, 5, 5, 2, 25, 30, 11, 2, 3,…\n$ years_code            &lt;dbl&gt; 14, 7, 8, 29, 7, 25, 13, 8, 22, 10, 3, 13, 8, 19, 3, 3, 7, 10, 15, 7, 27, 40, 15, 4, 7, …\n$ ai_trust              &lt;fct&gt; Somewhat trust, Somewhat trust, Somewhat trust, Somewhat trust, Somewhat trust, Neither …\n\n\n\n\n\n\n\n# Summarize mean and median values for converted_comp_yearly and work_exp\ndata_dev_survey |&gt;\n  summarize(mean_comp = mean(converted_comp_yearly),\n            median_comp = median(converted_comp_yearly),\n            mean_exp = mean(work_exp),\n            median_exp = median(work_exp))\n\n# A tibble: 1 × 4\n  mean_comp median_comp mean_exp median_exp\n      &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1    90702.      72768.     9.65          7\n\n\n\n\n\n\n\n# Use tidy_summary to get a clean summary of the two columns\ndata_dev_survey |&gt;\n  select(converted_comp_yearly, work_exp) |&gt;\n  tidy_summary()\n\n# A tibble: 2 × 11\n  column                    n group type      min     Q1     mean median     Q3     max       sd\n  &lt;chr&gt;                 &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 converted_comp_yearly  1182 &lt;NA&gt;  numeric     3 41390. 90702.   72768. 120000 1200000 81928.  \n2 work_exp               1182 &lt;NA&gt;  numeric     0     4      9.65     7      13      48     8.19\n\n\n\n\n\n\n\n# Compute the correlation between converted_comp_yearly and work_exp\ndata_dev_survey |&gt;\n  get_correlation(formula = converted_comp_yearly ~ work_exp)\n\n# A tibble: 1 × 1\n    cor\n  &lt;dbl&gt;\n1 0.277\n\n\n\n\n\n\n\n# Plot the relationship between work experience and yearly compensation\nggplot(data_dev_survey, aes(x = work_exp, y = converted_comp_yearly)) +\n  geom_point(alpha = 0.2) +\n  labs(x = \"Work Experience (years)\", y = \"Yearly Compensation (USD)\",\n       title = \"Scatterplot of Work Experience and Yearly Compensation\")\n\n\n\n\n\n\n\n\n\n# The same scatterplot with a regression line added\nggplot(data_dev_survey, aes(x = work_exp, y = converted_comp_yearly)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Work Experience (years)\", y = \"Yearly Compensation (USD)\",\n       title = \"Work Experience vs. Yearly Compensation with Regression Line\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Fit a linear regression model with compensation as the outcome\nsalary_slr_model &lt;- lm(converted_comp_yearly ~ work_exp, data = data_dev_survey)\n\n# Get the regression coefficients\ncoef(salary_slr_model)\n\n(Intercept)    work_exp \n  63973.960    2769.115"
  },
  {
    "objectID": "answers/day2_exercise_answers.html#session-5-multiple-linear-regression-analysis-part-1",
    "href": "answers/day2_exercise_answers.html#session-5-multiple-linear-regression-analysis-part-1",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# Fit a simple linear regression model with ai_trust as a predictor on salary\ntrust_model &lt;- lm(converted_comp_yearly ~ ai_trust, data = data_dev_survey)\n\n# Get regression coefficients\ncoef(trust_model)\n\n                       (Intercept)          ai_trustSomewhat distrust ai_trustNeither trust nor distrust \n                         113080.86                          -14688.72                          -20883.40 \n            ai_trustSomewhat trust               ai_trustHighly trust \n                         -29605.58                          -35437.81 \n\n\n\n\n\n\n\n# Scatterplot with ai_trust as color for grouping on work_exp predicting\n# converted_comp_yearly\nggplot(data_dev_survey, \n       aes(x = work_exp, y = converted_comp_yearly, color = ai_trust)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Work Experience (years)\", y = \"Yearly Compensation (USD)\",\n       color = \"Trust in AI\",\n       title = \"Scatterplot of Work Experience and Yearly Compensation by Trust in AI\")\n\n\n\n\n\n\n\n\n\n# Summarize the target and regressors\ndata_dev_survey |&gt;\n  select(converted_comp_yearly, work_exp, ai_trust) |&gt;\n  tidy_summary()\n\n# A tibble: 7 × 11\n  column                    n group                      type      min     Q1     mean median     Q3     max       sd\n  &lt;chr&gt;                 &lt;int&gt; &lt;chr&gt;                      &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 converted_comp_yearly  1182 &lt;NA&gt;                       numeric     3 41390. 90702.   72768. 120000 1200000 81928.  \n2 work_exp               1182 &lt;NA&gt;                       numeric     0     4      9.65     7      13      48     8.19\n3 ai_trust                 66 Highly distrust            factor     NA    NA     NA       NA      NA      NA    NA   \n4 ai_trust                270 Somewhat distrust          factor     NA    NA     NA       NA      NA      NA    NA   \n5 ai_trust                319 Neither trust nor distrust factor     NA    NA     NA       NA      NA      NA    NA   \n6 ai_trust                489 Somewhat trust             factor     NA    NA     NA       NA      NA      NA    NA   \n7 ai_trust                 38 Highly trust               factor     NA    NA     NA       NA      NA      NA    NA   \n\n\n\n\n\n\n\n# Fit a multiple regression model with interaction\nmulti_model_interaction &lt;- lm(converted_comp_yearly ~ work_exp * ai_trust, \n                              data = data_dev_survey)\n\n# Get regression coefficients\ncoef(multi_model_interaction)\n\n                                (Intercept)                                    work_exp \n                                 94685.6495                                   1724.5513 \n                  ai_trustSomewhat distrust          ai_trustNeither trust nor distrust \n                                -21369.0390                                 -44973.0562 \n                     ai_trustSomewhat trust                        ai_trustHighly trust \n                                -31608.7285                                 -32523.5905 \n         work_exp:ai_trustSomewhat distrust work_exp:ai_trustNeither trust nor distrust \n                                   792.3242                                   3069.4610 \n            work_exp:ai_trustSomewhat trust               work_exp:ai_trustHighly trust \n                                   370.1143                                   -343.6176 \n\n\n\n\n\n\n\n# Scatterplot with regression lines by trust in AI\nggplot(data_dev_survey, aes(x = work_exp, y = converted_comp_yearly, color = ai_trust)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Work Experience (years)\", y = \"Yearly Compensation (USD)\",\n       color = \"Trust in AI\",\n       title = \"Work Experience vs. Yearly Compensation by Trust in AI\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "answers/day2_exercise_answers.html#session-6-multiple-linear-regression-analysis-part-2",
    "href": "answers/day2_exercise_answers.html#session-6-multiple-linear-regression-analysis-part-2",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# Fit a multiple regression model without interaction terms\nmulti_model_no_interaction &lt;- lm(converted_comp_yearly ~ work_exp + ai_trust, \n                                 data = data_dev_survey)\n\n# Get regression coefficients\ncoef(multi_model_no_interaction)\n\n                       (Intercept)                           work_exp          ai_trustSomewhat distrust \n                         83491.170                           2774.034                         -12736.618 \nai_trustNeither trust nor distrust             ai_trustSomewhat trust               ai_trustHighly trust \n                        -15877.388                         -27030.100                         -36946.496 \n\n\n\n\n\n\n\n# Scatterplot with regression lines by trust in AI (parallel slopes)\nggplot(data_dev_survey, aes(x = work_exp, y = converted_comp_yearly, color = ai_trust)) +\n  geom_point(alpha = 0.5) +\n  geom_parallel_slopes(se = FALSE) +\n  labs(x = \"Work Experience (years)\", y = \"Yearly Compensation (USD)\",\n       color = \"Trust in AI\",\n       title = \"Work Experience vs. Yearly Compensation by Trust in AI\")\n\n\n\n\n\n\n\n\n\n# Use tidy_summary to get a clean summary of the data for a multiple regression\n# with work experience and years coding as regressors and yearly compensation as\n# the outcome\ndata_dev_survey |&gt;\n  select(converted_comp_yearly, work_exp, years_code) |&gt;\n  tidy_summary()\n\n# A tibble: 3 × 11\n  column                    n group type      min     Q1     mean median     Q3     max       sd\n  &lt;chr&gt;                 &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 converted_comp_yearly  1182 &lt;NA&gt;  numeric     3 41390. 90702.   72768. 120000 1200000 81928.  \n2 work_exp               1182 &lt;NA&gt;  numeric     0     4      9.65     7      13      48     8.19\n3 years_code             1182 &lt;NA&gt;  numeric     1     7     12.5     10      16      50     8.38\n\n\n\n\n\n\n\n# Fit a multiple regression model\nmulti_model &lt;- lm(converted_comp_yearly ~ work_exp + years_code, \n                  data = data_dev_survey)\n\n# Get regression coefficients\ncoef(multi_model)\n\n(Intercept)    work_exp  years_code \n  55764.220    1343.020    1753.121 \n\n\n\n\n\n\n\n# Create scatterplots with regression lines for both variables with \n# converted_comp_yearly as the response variable in simple linear regression\nggplot(data_dev_survey, aes(x = work_exp, y = converted_comp_yearly)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Work Experience (years)\", y = \"Yearly Compensation (USD)\",\n       title = \"Yearly Compensation vs Work Experience\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(data_dev_survey, aes(x = years_code, y = converted_comp_yearly)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Years Coding\", y = \"Yearly Compensation (USD)\",\n       title = \"Yearly Compensation vs Years Coding\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "answers/day4_walkthrough_answers.html",
    "href": "answers/day4_walkthrough_answers.html",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# Load the required packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(infer)\n\n# We'll again use `palmerspenguins` for data in this session\nlibrary(palmerpenguins)\n\n# We'll use GGally when creating a scatterplot matrix later in Session 11\n# install.packages(\"GGally\")\nlibrary(GGally)\n\n\nThese packages provide tools for data wrangling, visualization, modeling, and inference.\n\nThe infer package is particularly useful for hypothesis testing and confidence intervals.\n\n\n\n\n\n \n\n\n\n\nWe have recently discovered that the food sources for the penguins near Palmer Station has changed over the last year. There is some evidence that the penguins have more food available to them. Previously the penguins had an average body mass of 4100 grams. Suppose we want to test the claim that the average body mass of penguins is different than this hypothesized value of 4100 grams. We have a sample of 333 penguins from the penguins dataset after removing values.\n\n# Prepare the data\npenguins_data &lt;- penguins |&gt; \n  na.omit()\n\n# Set hypothesized value for use throughout\nmu_hypothesized &lt;- 4100\n\n# Set significance level\nalpha &lt;- 0.05\n\n# Calculate the observed test statistic (from our sample data)\nobserved_mean_mass &lt;- penguins_data |&gt; \n  observe(response = body_mass_g, stat = \"mean\")\n\n# Can also be done by skipping over the `generate()` step\nobserved_mean_mass &lt;- penguins_data |&gt; \n  specify(response = body_mass_g) |&gt; \n  calculate(stat = \"mean\")\n\n\nset.seed(2024)\n\n# Walk through the infer steps\nnull_distribution &lt;- penguins_data |&gt; \n  specify(response = body_mass_g) |&gt; \n  hypothesize(null = \"point\", mu = mu_hypothesized) |&gt; \n  generate(reps = 1000, type = \"bootstrap\") |&gt; \n  calculate(stat = \"mean\")\n\n\n# Visualize the null distribution and p-value\nnull_distribution |&gt; \n  visualize() +\n  shade_p_value(obs_stat = observed_mean_mass, direction = \"right\")\n\n\n\n\n\n# Retrieve the p-value\nboot_p_value &lt;- null_distribution |&gt; \n  get_p_value(obs_stat = observed_mean_mass, direction = \"right\")\nboot_p_value\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.012\n\n\n\n\nSince the p-value = 0.012 is less than \\(\\alpha = 0.05\\), we reject the null hypothesis. We, therefore, have enough evidence to suggest that the average body mass of penguins is greater than the previously assumed value of 4100 grams.\n\n\n\n\n\nWe can also use the traditional approach to test the same hypothesis using a \\(t\\)-test with infer.\n\n# Conduct the t-test\nobs_stat_t &lt;- penguins_data |&gt; \n  specify(response = body_mass_g) |&gt; \n  hypothesize(null = \"point\", mu = mu_hypothesized) |&gt; \n  calculate(stat = \"t\")\n\n# Construct the theoretical null distribution\nnull_dist_t &lt;- penguins_data |&gt; \n  specify(response = body_mass_g) |&gt; \n  assume(distribution = \"t\")\n\n# Visualize the null distribution and p-value\nvisualize(null_dist_t) +\n  shade_p_value(obs_stat_t, direction = \"right\")\n\n\n\n# Find the theoretical p-value\nt_p_value &lt;- null_dist_t |&gt; \n  get_p_value(obs_stat = obs_stat_t, direction = \"right\")\nt_p_value\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1 0.00790\n\n\nThe interpretation is the same as for the simulation-based test with bootstrapping.\nBonus: This can also be done directly in R, with much less code overall, but not all tests work like this and without a common framework it can get confusing in my opinion.\n\n# Conduct the t-test directly in R\nt.test(penguins_data$body_mass_g, \n       mu = mu_hypothesized,\n       alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  penguins_data$body_mass_g\nt = 2.4262, df = 332, p-value = 0.007895\nalternative hypothesis: true mean is greater than 4100\n95 percent confidence interval:\n 4134.274      Inf\nsample estimates:\nmean of x \n 4207.057 \n\n\n\n\n\n\nWe now want to test the claim that the average body mass of Adelie penguins is statistically discernibly different than the average body mass of Chinstrap penguins.\n\\(H_0: \\mu_{\\text{Adelie}} = \\mu_{\\text{Chinstrap}}\\)\n\\(H_A: \\mu_{\\text{Adelie}} \\ne \\mu_{\\text{Chinstrap}}\\)\nOR\n\\(H_0: \\mu_{\\text{Adelie}} - \\mu_{\\text{Chinstrap}} = 0\\)\n\\(H_A: \\mu_{\\text{Adelie}} - \\mu_{\\text{Chinstrap}} \\ne 0\\)\n\n# Set significance level\nalpha &lt;- 0.1\n\n# Prepare the data\nadelie_chinstrap_data &lt;- penguins_data |&gt; \n  filter(species %in% c(\"Adelie\", \"Chinstrap\")) |&gt; \n  # Not necessary, but will remove the warning seen if not done\n  droplevels()\n\n# Calculate the observed test statistic (from our sample data)\nobserved_diff_mean_mass &lt;- adelie_chinstrap_data |&gt; \n  specify(formula = body_mass_g ~ species) |&gt; \n  calculate(stat = \"diff in means\", order = c(\"Adelie\", \"Chinstrap\"))\n\n\n# Do some data visualization\nggplot(data = adelie_chinstrap_data, aes(x = species, y = body_mass_g)) +\n  geom_boxplot() +\n  labs(title = \"Body Mass of Adelie and Chinstrap Penguins\",\n       x = \"Species\",\n       y = \"Body Mass (g)\")\n\n\n\n\n\nset.seed(2024)\n\n# Walk through the infer steps\nnull_dist &lt;- adelie_chinstrap_data |&gt; \n  specify(formula = body_mass_g ~ species) |&gt; \n  hypothesize(null = \"independence\") |&gt; \n  generate(reps = 1000, type = \"permute\") |&gt; \n  calculate(stat = \"diff in means\", order = c(\"Adelie\", \"Chinstrap\"))\n\n\n# Visualize the null distribution and p-value\nnull_dist |&gt; \n  visualize() +\n  shade_p_value(obs_stat = observed_diff_mean_mass, direction = \"two sided\")\n\n\n\n\n\n# Retrieve the p-value\npermute_p_value &lt;- null_dist |&gt; \n  get_p_value(obs_stat = observed_diff_mean_mass, direction = \"two sided\")\npermute_p_value\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1    0.69\n\n\nCheck out ?get_p_value to see the options for direction.\n\n\nSince the p-value = 0.69 is greater than \\(\\alpha = 0.1\\), we fail to reject the null hypothesis. We, therefore, do not have enough evidence to suggest that the average body mass of penguins is different for the Adelie group compared to the Chinstrap group.\n\n\n\n\n\nWe can also use the traditional approach to test the same hypothesis using a \\(t\\)-test with infer.\n\n# Conduct the t-test\nobs_statistic_t &lt;- adelie_chinstrap_data |&gt; \n  specify(formula = body_mass_g ~ species) |&gt; \n  hypothesize(null = \"independence\") |&gt;\n  calculate(stat = \"t\", order = c(\"Adelie\", \"Chinstrap\"))\n\n# Construct the theoretical null distribution\nnull_distro_t &lt;- adelie_chinstrap_data |&gt; \n  specify(formula = body_mass_g ~ species) |&gt; \n  assume(distribution = \"t\")\n\n# Visualize the null distribution and p-value\nvisualize(null_distro_t) +\n  shade_p_value(obs_statistic_t, direction = \"two sided\")\n\n\n\n# Find the theoretical p-value\ntheory_p_value &lt;- null_distro_t |&gt; \n  get_p_value(obs_stat = obs_statistic_t, direction = \"two sided\")\ntheory_p_value\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.655\n\n\nBonus: This can also be done directly in R, but unfortunately the formula notation isn’t available for all tests in the stats package.\n\n# Conduct the t-test directly\nadelie_chinstrap_data |&gt;\n  t.test(formula = body_mass_g ~ species, \n         # Can use the _ to pass into arguments when they aren't the first one\n         data = _, \n         alternative = \"two.sided\")\n\n\n    Welch Two Sample t-test\n\ndata:  body_mass_g by species\nt = -0.44793, df = 154.03, p-value = 0.6548\nalternative hypothesis: true difference in means between group Adelie and group Chinstrap is not equal to 0\n95 percent confidence interval:\n -145.66494   91.81724\nsample estimates:\n   mean in group Adelie mean in group Chinstrap \n               3706.164                3733.088 \n\n\nWe have a t_test() function in infer that follows a common framework across all tests that are implemented.\n\n# Can use one of the `infer` wrapper functions too\nadelie_chinstrap_data |&gt; \n  t_test(formula = body_mass_g ~ species,\n         order = c(\"Adelie\", \"Chinstrap\"),\n         alternative = \"two.sided\")\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1    -0.448  154.   0.655 two.sided      -26.9    -146.     91.8\n\n\n\n\n\nWe can also check if the 0 value is in the confidence interval for the difference in means. If it is, we fail to reject the null hypothesis at the \\(\\alpha\\) level of significance. In order for this to work, \\(\\alpha\\) should be 100% - the confidence level.\n\n# Calculate the confidence interval\nboot_dist &lt;- adelie_chinstrap_data |&gt; \n  specify(formula = body_mass_g ~ species) |&gt; \n#  hypothesize(null = \"independence\") |&gt; \n  generate(reps = 1000, type = \"bootstrap\") |&gt; \n#           type = \"permute\") |&gt; \n  calculate(stat = \"diff in means\", order = c(\"Adelie\", \"Chinstrap\"))\n\nboot_dist |&gt; \n  get_confidence_interval(level = 0.9)\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    -127.     71.7\n\n\nThe value of 0 is contained in the interval, so it is a plausible value for the difference of \\(\\mu_{\\text{Adelie}} - \\mu_{\\text{Chinstrap}}\\). Thus, from the confidence interval, we fail to reject the null hypothesis. Notice that I used a confidence level of 0.9 to match up with the 0.1 significance level we used earlier.\n\n\n\n\nHere are five multiple-choice questions covering the content from Session 10 on One- and Two-Sample Hypothesis Tests:\n\n(10.1) What is the null hypothesis for the one-sample mean test conducted on the penguin body mass data?\nA. The mean body mass of penguins is greater than 4100 grams.\nB. The mean body mass of penguins is less than 4100 grams.\nC. The mean body mass of penguins is different from 4100 grams. D. The mean body mass of penguins is equal to 4100 grams.\n\n(10.2) When performing a two-sample mean test comparing the flipper length between the penguins from the islands of Biscoe and Dream, what does the null hypothesis represent?\nA. The flipper length of penguins on Biscoe Island is greater than that of penguins on Dream Island.\nB. The flipper length of penguins on Biscoe Island is less than that of penguins on Dream Island.\nC. The mean flipper length of penguins on Biscoe Island is equal to the mean flipper length of penguins on Dream Island.\nD. The difference in flipper length between penguins on Biscoe and Dream Islands is significant.\n\n(10.3) In the one-sample hypothesis test using bootstrapping, what does a \\(p\\)-value less than \\(\\alpha\\) indicate?\nA. The sample mean is equal to the hypothesized mean.\nB. We reject the null hypothesis and have support for the alternative hypothesis.\nC. We fail to reject the null hypothesis and conclude that the null hypothesis is true.\nD. The sample is not large enough to make any conclusions.\n\n(10.4) What does the shaded area in the null distribution represent when visualizing the hypothesis test?\nA. The \\(p\\)-value, which is the probability of obtaining a test statistic as extreme as the observed one under the null hypothesis.\nB. The confidence interval for the difference in means.\nC. The mean value of the population.\nD. The observed test statistic.\n\n(10.5) In a two-sample means hypothesis test, if the \\(p\\)-value is greater than the significance level, what can we conclude?\nA. We reject the null hypothesis and conclude that the means are statistically different.\nB. We fail to reject the null hypothesis and conclude that there is no evidence of a difference in means.\nC. The null hypothesis is incorrect.\nD. The test is invalid, and no conclusions can be drawn.\n\n\n\n\n(10.1) What is the null hypothesis for the one-sample mean test conducted on the penguin body mass data?\nCorrect Answer:\nD. The mean body mass of penguins is equal to 4100 grams.\nExplanation:\nThe null hypothesis in this case states that there is no difference between the true mean body mass of penguins and the hypothesized value of 4100 grams.\n\n(10.2) When performing a two-sample mean test comparing the flipper length between the penguins from the island of Biscoe to those from Dream, what does the null hypothesis represent?\nCorrect Answer:\nC. The mean flipper length of penguins on Biscoe Island is equal to the mean flipper length of penguins on Dream Island.\nExplanation:\nThe null hypothesis in a two-sample test asserts that there is no difference between the mean flipper lengths of penguins on Biscoe and Dream Islands, meaning any observed difference is due to random variation in the sample.\n\n(10.3) In a one-sample hypothesis test using bootstrapping, what does a \\(p\\)-value less than \\(\\alpha\\) indicate?\nCorrect Answer:\nB. We reject the null hypothesis and have support for the alternative hypothesis.\nExplanation:\nA \\(p\\)-value below \\(\\alpha\\) suggests that there is sufficient evidence to reject the null hypothesis, meaning the sample data provides evidence that the mean body mass is not 4100 grams.\n\n(10.4) What does the shaded area in the null distribution represent when visualizing the hypothesis test?\nCorrect Answer:\nA. The \\(p\\)-value, which is the probability of obtaining a test statistic as extreme as the observed one under the null hypothesis.\nExplanation:\nThe shaded region represents the p-value, which quantifies how likely it is to observe a test statistic as extreme or more extreme than the one obtained, assuming the null hypothesis is true.\n\n(10.5) In a two-sample hypothesis test, if the \\(p\\)-value is greater than the significance level, what can we conclude?\nCorrect Answer:\nB. We fail to reject the null hypothesis and conclude that there is no evidence of a difference in means.\nExplanation:\nA \\(p\\)-value greater than the significance level means we do not have enough evidence to reject the null hypothesis, so we conclude that there is no statistically significant difference between the two group means.\n\n\n\n\n\n\n\n\n# Fit a multiple regression model without interaction terms\nmulti_model_no_interaction &lt;- lm(body_mass_g ~ flipper_length_mm + species, \n                              data = penguins_data)\n\n# Get regression coefficients\ncoef(multi_model_no_interaction)\n\n      (Intercept) flipper_length_mm  speciesChinstrap     speciesGentoo \n      -4013.17889          40.60617        -205.37548         284.52360 \n\n\n\n\n\n\n\n# Focus on only the columns of interest\nmodel_data &lt;- penguins_data |&gt; \n  select(body_mass_g, flipper_length_mm, species)\n\n# Plot a matrix to view the relationships between variables\nggpairs(model_data, columns = 1:3, mapping = aes(color = species))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can perform hypothesis tests to determine if the regression (partial slope) coefficients are significantly different from zero using the moderndive package.\n\n# Conduct hypothesis tests for regression coefficients\nget_regression_table(multi_model_no_interaction)\n\n# A tibble: 4 × 7\n  term               estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept           -4013.     586.       -6.85   0      -5166.   -2860. \n2 flipper_length_mm      40.6      3.08     13.2    0         34.5     46.7\n3 species: Chinstrap   -205.      57.6      -3.57   0       -319.     -92.1\n4 species: Gentoo       285.      95.4       2.98   0.003     96.8    472. \n\n\nThe p_value column in the output table provides the \\(p\\)-values for testing the null hypothesis that each coefficient is equal to zero. Each of these p_values is very small, indicating that the coefficients are significantly different from zero. This corresponds to each of them being significant predictors in the model. Note also that none of the confidence intervals in the lower_ci to upper_ci columns contain zero when looking at each row.\nReview Section 10.5 of ModernDive Second Edition for more details on the theory behind hypothesis testing and confidence intervals for regression coefficients.\n\n\n\n\n# Get fitted values and residuals:\nfit_and_res_mult &lt;- get_regression_points(multi_model_no_interaction)\nfit_and_res_mult\n\n# A tibble: 333 × 6\n      ID body_mass_g flipper_length_mm species body_mass_g_hat residual\n   &lt;int&gt;       &lt;int&gt;             &lt;int&gt; &lt;fct&gt;             &lt;dbl&gt;    &lt;dbl&gt;\n 1     1        3750               181 Adelie            3337.    413. \n 2     2        3800               186 Adelie            3540.    260. \n 3     3        3250               195 Adelie            3905.   -655. \n 4     4        3450               193 Adelie            3824.   -374. \n 5     5        3650               190 Adelie            3702.    -52.0\n 6     6        3625               181 Adelie            3337.    288. \n 7     7        4675               195 Adelie            3905.    770. \n 8     8        3200               182 Adelie            3377.   -177. \n 9     9        3800               191 Adelie            3743.     57.4\n10    10        4400               198 Adelie            4027.    373. \n# ℹ 323 more rows\n\n\n\n# Visualize the fitted values and the residuals\nggplot(fit_and_res_mult, aes(x = body_mass_g_hat, y = residual)) +\n  geom_point() +\n  labs(x = \"fitted values (body mass in grams)\", y = \"residual\") +\n  geom_hline(yintercept = 0, col = \"blue\")\n\n\n\n# Visualize the residuals using a QQ plot\nggplot(fit_and_res_mult, aes(sample = residual)) +\n  geom_qq() +\n  geom_qq_line(col=\"blue\", linewidth = 0.5)\n\n\n\n\nMore details on checking model fit and model assumptions can be found in Subsection 10.2.6 of ModernDive Second Edition.\n\nThe first plot shows the residuals against the fitted values, which should be randomly scattered around the horizontal line at zero. We don’t see much deviation from this line, which is a good sign, and evidence for the both the of assumptions of Linearity and Equal (constant) variance.\nWe also do not have any reason to assume that Independence is violated based on how the penguins were likely sampled for study.\nThe second plot shows the residuals against a theoretical normal distribution. The residuals should fall along the line, which they do for the most part. There is a slight deviation at the tails, but this is not a major concern. Thus, the Normality assumption is not violated.\n\nBy checking the LINE acrostic, we can feel confident in the model fit and assumptions.\n\n\n\n\nWe can also use simulation-based inference to test the significance of the regression coefficients. Here, we will use bootstrapping to generate confidence intervals for the partial slope coefficients. Bootstrapping now bootstraps the entire row of data, instead of just a single variable\n\n# Get the observed model fit\nobserved_fit &lt;- penguins_data |&gt; \n  specify(formula = body_mass_g ~ flipper_length_mm + species) |&gt; \n  fit()\nobserved_fit\n\n# A tibble: 4 × 2\n  term              estimate\n  &lt;chr&gt;                &lt;dbl&gt;\n1 intercept          -4013. \n2 flipper_length_mm     40.6\n3 speciesChinstrap    -205. \n4 speciesGentoo        285. \n\n# Bootstrap distribution for the partial slope coefficients\nset.seed(2024)\nmlr_resamples &lt;- penguins_data |&gt; \n  specify(formula = body_mass_g ~ flipper_length_mm + species) |&gt; \n  generate(reps = 1000, type = \"bootstrap\")\n\n# Then fit a linear regression model to each replicate of data\nbootstrap_models &lt;- mlr_resamples |&gt; \n  fit()\nbootstrap_models\n\n# A tibble: 4,000 × 3\n# Groups:   replicate [1,000]\n   replicate term              estimate\n       &lt;int&gt; &lt;chr&gt;                &lt;dbl&gt;\n 1         1 intercept          -5067. \n 2         1 flipper_length_mm     46.3\n 3         1 speciesChinstrap    -320. \n 4         1 speciesGentoo         92.3\n 5         2 intercept          -3609. \n 6         2 flipper_length_mm     38.4\n 7         2 speciesChinstrap    -147. \n 8         2 speciesGentoo        371. \n 9         3 intercept          -2408. \n10         3 flipper_length_mm     32.3\n# ℹ 3,990 more rows\n\n# Visualize the bootstrap distribution for the partial slope coefficients\nvisualize(bootstrap_models)\n\n\n\n# Get the confidence intervals for the partial slope coefficients\nconfidence_intervals_mlr &lt;- bootstrap_models |&gt; \n  get_confidence_interval(\n    level = 0.95,\n    type = \"percentile\",\n    point_estimate = observed_fit)\nconfidence_intervals_mlr\n\n# A tibble: 4 × 3\n  term              lower_ci upper_ci\n  &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n1 flipper_length_mm     35.2     46.9\n2 intercept          -5210.   -2991. \n3 speciesChinstrap    -315.    -105. \n4 speciesGentoo         77.1    466. \n\n\nWe see similar results here compared to the coefficients produced by get_regression_table() for the theoretical results:\n\nget_regression_table(multi_model_no_interaction) |&gt; \n  select(term, lower_ci, upper_ci)\n\n# A tibble: 4 × 3\n  term               lower_ci upper_ci\n  &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept           -5166.   -2860. \n2 flipper_length_mm      34.5     46.7\n3 species: Chinstrap   -319.     -92.1\n4 species: Gentoo        96.8    472. \n\n\n\n\n\n\n\n# Generate the null distributions for each of the regression coefficients\nnull_distribution_mlr &lt;- penguins_data |&gt;\n  specify(formula = body_mass_g ~ flipper_length_mm + species) |&gt; \n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  fit()\n\n# Visualize and shade the p-value for each regressor\nnull_distribution_mlr |&gt; \n  visualize() +\n  shade_p_value(obs_stat = observed_fit, direction = \"two sided\")\n\n\n\n# Get the p-values\nnull_distribution_mlr |&gt;\n  get_p_value(obs_stat = observed_fit, direction = \"two-sided\")\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an approximation based on the number of `reps` chosen in\nthe `generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\nPlease be cautious in reporting a p-value of 0. This result is an approximation based on the number of `reps` chosen in\nthe `generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\n\n\n# A tibble: 4 × 2\n  term              p_value\n  &lt;chr&gt;               &lt;dbl&gt;\n1 flipper_length_mm   0    \n2 intercept           0    \n3 speciesChinstrap    0.098\n4 speciesGentoo       0.18 \n\n\nWe see different results for species Gentoo with the simulation-based methods. Further analysis should be done to check the normality assumption on the Gentoo data.\n\n\n\n\n(11.1) What is the null hypothesis when performing a hypothesis test on a regression coefficient in a multiple regression model?\nA. The coefficient is significantly different from zero.\nB. The coefficient is equal to the observed mean.\nC. The coefficient is equal to zero, indicating no effect of the predictor on the response variable.\nD. The coefficient is greater than zero, indicating a positive effect of the predictor on the response variable.\n\n(11.2) When interpreting the p-value for a regression coefficient, what does a small \\(p\\)-value (e.g., &lt; 0.05) indicate?\nA. The predictor is not a significant variable in the model.\nB. There is strong evidence against the null hypothesis, suggesting the coefficient is significantly different from zero.\nC. The model fit is poor, and assumptions of linearity are violated.\nD. The residuals are not normally distributed.\n\n(11.3) In multiple regression, what does it mean when a confidence interval for a regression coefficient does not contain zero?\nA. The coefficient is not statistically significant.\nB. The coefficient is equal to zero.\nC. The residuals are randomly distributed around zero. D. The coefficient is statistically significant/discernible, and the predictor has an effect on the response variable.\n\n(11.4) Why is it important to check the residuals of a multiple regression model using diagnostic plots like the residual plot and the QQ plot?\nA. To ensure that the response variable is correctly predicted by the model.\nB. To verify that the assumptions of linearity, normality, and constant variance are not violated.\nC. To determine the p-value for each coefficient in the model.\nD. To check if the confidence intervals contain zero.\n\n(11.5) What is the purpose of bootstrapping in the context of multiple regression?\nA. To simulate many different regression models and estimate the sampling distribution of the regression coefficients.\nB. To generate random samples from the population to estimate new coefficients.\nC. To test the normality of the residuals.\nD. To calculate the exact p-values for the regression coefficients.\n\n\n\n\n(11.1) What is the null hypothesis when performing a hypothesis test on a regression coefficient in a multiple regression model?\nCorrect Answer:\nC. The coefficient is equal to zero, indicating no effect of the predictor on the response variable.\nExplanation:\nThe null hypothesis in a regression test is that the coefficient equals zero, meaning the predictor does not significantly influence the response variable.\n\n(11.2) When interpreting the p-value for a regression coefficient, what does a small p-value (e.g., &lt; 0.05) indicate?\nCorrect Answer:\nB. There is strong evidence against the null hypothesis, suggesting the coefficient is significantly different from zero.\nExplanation:\nA small p-value suggests that the coefficient is statistically significant and that the predictor has an effect on the response variable.\n\n(11.3) In multiple regression, what does it mean when a confidence interval for a regression coefficient does not contain zero?\nCorrect Answer:\nD. The coefficient is statistically significant, and the predictor has an effect on the response variable.\nExplanation:\nIf the confidence interval does not contain zero, the coefficient is considered significant, indicating that the predictor likely has an effect on the response variable.\n\n(11.4) Why is it important to check the residuals of a multiple regression model using diagnostic plots like the residual plot and the QQ plot?\nCorrect Answer:\nB. To verify that the assumptions of linearity, normality, and constant variance are not violated.\nExplanation:\nDiagnostic plots help confirm that the model assumptions (linearity, normality, constant variance) are satisfied, ensuring the validity of the regression results.\n\n(11.5) What is the purpose of bootstrapping in the context of multiple regression?\nCorrect Answer:\nA. To simulate many different regression models and estimate the sampling distribution of the regression coefficients.\nExplanation:\nBootstrapping resamples the data and fits new regression models to estimate the sampling distribution of the coefficients, allowing for confidence intervals and significance tests.\n\n\n\n\n\n\n\n\n\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n✔ readr     2.1.5     \n── Conflicts ────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThere are more packages in the tidyverse that we don’t cover much in the book, but I’ve found incredibly useful for data wrangling:\n\nforcats for working with factors\nstringr for working with strings\npurrr for working with functions\n\n\n# This is the same as running each of these individually\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(forcats)\n\nThere are even more packages loaded than this though. Check out the full list on the tidyverse page.\nThe infer package is part of the tidymodels collection of packages that are designed to work well with the tidyverse for modeling and inference too.\nThere’s so much more we could cover in this course. Check out Chapter 11 of ModernDive Second Edition for an overview as well. I hope you’ve gotten a good amount of tools and theory to help you get started with data analysis and statistics in R using the tidyverse, moderndive, and infer to wrangle, visualize, model, and infer from your data. Go tell your story with data!"
  },
  {
    "objectID": "answers/day4_walkthrough_answers.html#session-10-one-and-two-sample-hypothesis-testing",
    "href": "answers/day4_walkthrough_answers.html#session-10-one-and-two-sample-hypothesis-testing",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# Load the required packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(infer)\n\n# We'll again use `palmerspenguins` for data in this session\nlibrary(palmerpenguins)\n\n# We'll use GGally when creating a scatterplot matrix later in Session 11\n# install.packages(\"GGally\")\nlibrary(GGally)\n\n\nThese packages provide tools for data wrangling, visualization, modeling, and inference.\n\nThe infer package is particularly useful for hypothesis testing and confidence intervals.\n\n\n\n\n\n \n\n\n\n\nWe have recently discovered that the food sources for the penguins near Palmer Station has changed over the last year. There is some evidence that the penguins have more food available to them. Previously the penguins had an average body mass of 4100 grams. Suppose we want to test the claim that the average body mass of penguins is different than this hypothesized value of 4100 grams. We have a sample of 333 penguins from the penguins dataset after removing values.\n\n# Prepare the data\npenguins_data &lt;- penguins |&gt; \n  na.omit()\n\n# Set hypothesized value for use throughout\nmu_hypothesized &lt;- 4100\n\n# Set significance level\nalpha &lt;- 0.05\n\n# Calculate the observed test statistic (from our sample data)\nobserved_mean_mass &lt;- penguins_data |&gt; \n  observe(response = body_mass_g, stat = \"mean\")\n\n# Can also be done by skipping over the `generate()` step\nobserved_mean_mass &lt;- penguins_data |&gt; \n  specify(response = body_mass_g) |&gt; \n  calculate(stat = \"mean\")\n\n\nset.seed(2024)\n\n# Walk through the infer steps\nnull_distribution &lt;- penguins_data |&gt; \n  specify(response = body_mass_g) |&gt; \n  hypothesize(null = \"point\", mu = mu_hypothesized) |&gt; \n  generate(reps = 1000, type = \"bootstrap\") |&gt; \n  calculate(stat = \"mean\")\n\n\n# Visualize the null distribution and p-value\nnull_distribution |&gt; \n  visualize() +\n  shade_p_value(obs_stat = observed_mean_mass, direction = \"right\")\n\n\n\n\n\n# Retrieve the p-value\nboot_p_value &lt;- null_distribution |&gt; \n  get_p_value(obs_stat = observed_mean_mass, direction = \"right\")\nboot_p_value\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.012\n\n\n\n\nSince the p-value = 0.012 is less than \\(\\alpha = 0.05\\), we reject the null hypothesis. We, therefore, have enough evidence to suggest that the average body mass of penguins is greater than the previously assumed value of 4100 grams.\n\n\n\n\n\nWe can also use the traditional approach to test the same hypothesis using a \\(t\\)-test with infer.\n\n# Conduct the t-test\nobs_stat_t &lt;- penguins_data |&gt; \n  specify(response = body_mass_g) |&gt; \n  hypothesize(null = \"point\", mu = mu_hypothesized) |&gt; \n  calculate(stat = \"t\")\n\n# Construct the theoretical null distribution\nnull_dist_t &lt;- penguins_data |&gt; \n  specify(response = body_mass_g) |&gt; \n  assume(distribution = \"t\")\n\n# Visualize the null distribution and p-value\nvisualize(null_dist_t) +\n  shade_p_value(obs_stat_t, direction = \"right\")\n\n\n\n# Find the theoretical p-value\nt_p_value &lt;- null_dist_t |&gt; \n  get_p_value(obs_stat = obs_stat_t, direction = \"right\")\nt_p_value\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1 0.00790\n\n\nThe interpretation is the same as for the simulation-based test with bootstrapping.\nBonus: This can also be done directly in R, with much less code overall, but not all tests work like this and without a common framework it can get confusing in my opinion.\n\n# Conduct the t-test directly in R\nt.test(penguins_data$body_mass_g, \n       mu = mu_hypothesized,\n       alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  penguins_data$body_mass_g\nt = 2.4262, df = 332, p-value = 0.007895\nalternative hypothesis: true mean is greater than 4100\n95 percent confidence interval:\n 4134.274      Inf\nsample estimates:\nmean of x \n 4207.057 \n\n\n\n\n\n\nWe now want to test the claim that the average body mass of Adelie penguins is statistically discernibly different than the average body mass of Chinstrap penguins.\n\\(H_0: \\mu_{\\text{Adelie}} = \\mu_{\\text{Chinstrap}}\\)\n\\(H_A: \\mu_{\\text{Adelie}} \\ne \\mu_{\\text{Chinstrap}}\\)\nOR\n\\(H_0: \\mu_{\\text{Adelie}} - \\mu_{\\text{Chinstrap}} = 0\\)\n\\(H_A: \\mu_{\\text{Adelie}} - \\mu_{\\text{Chinstrap}} \\ne 0\\)\n\n# Set significance level\nalpha &lt;- 0.1\n\n# Prepare the data\nadelie_chinstrap_data &lt;- penguins_data |&gt; \n  filter(species %in% c(\"Adelie\", \"Chinstrap\")) |&gt; \n  # Not necessary, but will remove the warning seen if not done\n  droplevels()\n\n# Calculate the observed test statistic (from our sample data)\nobserved_diff_mean_mass &lt;- adelie_chinstrap_data |&gt; \n  specify(formula = body_mass_g ~ species) |&gt; \n  calculate(stat = \"diff in means\", order = c(\"Adelie\", \"Chinstrap\"))\n\n\n# Do some data visualization\nggplot(data = adelie_chinstrap_data, aes(x = species, y = body_mass_g)) +\n  geom_boxplot() +\n  labs(title = \"Body Mass of Adelie and Chinstrap Penguins\",\n       x = \"Species\",\n       y = \"Body Mass (g)\")\n\n\n\n\n\nset.seed(2024)\n\n# Walk through the infer steps\nnull_dist &lt;- adelie_chinstrap_data |&gt; \n  specify(formula = body_mass_g ~ species) |&gt; \n  hypothesize(null = \"independence\") |&gt; \n  generate(reps = 1000, type = \"permute\") |&gt; \n  calculate(stat = \"diff in means\", order = c(\"Adelie\", \"Chinstrap\"))\n\n\n# Visualize the null distribution and p-value\nnull_dist |&gt; \n  visualize() +\n  shade_p_value(obs_stat = observed_diff_mean_mass, direction = \"two sided\")\n\n\n\n\n\n# Retrieve the p-value\npermute_p_value &lt;- null_dist |&gt; \n  get_p_value(obs_stat = observed_diff_mean_mass, direction = \"two sided\")\npermute_p_value\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1    0.69\n\n\nCheck out ?get_p_value to see the options for direction.\n\n\nSince the p-value = 0.69 is greater than \\(\\alpha = 0.1\\), we fail to reject the null hypothesis. We, therefore, do not have enough evidence to suggest that the average body mass of penguins is different for the Adelie group compared to the Chinstrap group.\n\n\n\n\n\nWe can also use the traditional approach to test the same hypothesis using a \\(t\\)-test with infer.\n\n# Conduct the t-test\nobs_statistic_t &lt;- adelie_chinstrap_data |&gt; \n  specify(formula = body_mass_g ~ species) |&gt; \n  hypothesize(null = \"independence\") |&gt;\n  calculate(stat = \"t\", order = c(\"Adelie\", \"Chinstrap\"))\n\n# Construct the theoretical null distribution\nnull_distro_t &lt;- adelie_chinstrap_data |&gt; \n  specify(formula = body_mass_g ~ species) |&gt; \n  assume(distribution = \"t\")\n\n# Visualize the null distribution and p-value\nvisualize(null_distro_t) +\n  shade_p_value(obs_statistic_t, direction = \"two sided\")\n\n\n\n# Find the theoretical p-value\ntheory_p_value &lt;- null_distro_t |&gt; \n  get_p_value(obs_stat = obs_statistic_t, direction = \"two sided\")\ntheory_p_value\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.655\n\n\nBonus: This can also be done directly in R, but unfortunately the formula notation isn’t available for all tests in the stats package.\n\n# Conduct the t-test directly\nadelie_chinstrap_data |&gt;\n  t.test(formula = body_mass_g ~ species, \n         # Can use the _ to pass into arguments when they aren't the first one\n         data = _, \n         alternative = \"two.sided\")\n\n\n    Welch Two Sample t-test\n\ndata:  body_mass_g by species\nt = -0.44793, df = 154.03, p-value = 0.6548\nalternative hypothesis: true difference in means between group Adelie and group Chinstrap is not equal to 0\n95 percent confidence interval:\n -145.66494   91.81724\nsample estimates:\n   mean in group Adelie mean in group Chinstrap \n               3706.164                3733.088 \n\n\nWe have a t_test() function in infer that follows a common framework across all tests that are implemented.\n\n# Can use one of the `infer` wrapper functions too\nadelie_chinstrap_data |&gt; \n  t_test(formula = body_mass_g ~ species,\n         order = c(\"Adelie\", \"Chinstrap\"),\n         alternative = \"two.sided\")\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1    -0.448  154.   0.655 two.sided      -26.9    -146.     91.8\n\n\n\n\n\nWe can also check if the 0 value is in the confidence interval for the difference in means. If it is, we fail to reject the null hypothesis at the \\(\\alpha\\) level of significance. In order for this to work, \\(\\alpha\\) should be 100% - the confidence level.\n\n# Calculate the confidence interval\nboot_dist &lt;- adelie_chinstrap_data |&gt; \n  specify(formula = body_mass_g ~ species) |&gt; \n#  hypothesize(null = \"independence\") |&gt; \n  generate(reps = 1000, type = \"bootstrap\") |&gt; \n#           type = \"permute\") |&gt; \n  calculate(stat = \"diff in means\", order = c(\"Adelie\", \"Chinstrap\"))\n\nboot_dist |&gt; \n  get_confidence_interval(level = 0.9)\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    -127.     71.7\n\n\nThe value of 0 is contained in the interval, so it is a plausible value for the difference of \\(\\mu_{\\text{Adelie}} - \\mu_{\\text{Chinstrap}}\\). Thus, from the confidence interval, we fail to reject the null hypothesis. Notice that I used a confidence level of 0.9 to match up with the 0.1 significance level we used earlier.\n\n\n\n\nHere are five multiple-choice questions covering the content from Session 10 on One- and Two-Sample Hypothesis Tests:\n\n(10.1) What is the null hypothesis for the one-sample mean test conducted on the penguin body mass data?\nA. The mean body mass of penguins is greater than 4100 grams.\nB. The mean body mass of penguins is less than 4100 grams.\nC. The mean body mass of penguins is different from 4100 grams. D. The mean body mass of penguins is equal to 4100 grams.\n\n(10.2) When performing a two-sample mean test comparing the flipper length between the penguins from the islands of Biscoe and Dream, what does the null hypothesis represent?\nA. The flipper length of penguins on Biscoe Island is greater than that of penguins on Dream Island.\nB. The flipper length of penguins on Biscoe Island is less than that of penguins on Dream Island.\nC. The mean flipper length of penguins on Biscoe Island is equal to the mean flipper length of penguins on Dream Island.\nD. The difference in flipper length between penguins on Biscoe and Dream Islands is significant.\n\n(10.3) In the one-sample hypothesis test using bootstrapping, what does a \\(p\\)-value less than \\(\\alpha\\) indicate?\nA. The sample mean is equal to the hypothesized mean.\nB. We reject the null hypothesis and have support for the alternative hypothesis.\nC. We fail to reject the null hypothesis and conclude that the null hypothesis is true.\nD. The sample is not large enough to make any conclusions.\n\n(10.4) What does the shaded area in the null distribution represent when visualizing the hypothesis test?\nA. The \\(p\\)-value, which is the probability of obtaining a test statistic as extreme as the observed one under the null hypothesis.\nB. The confidence interval for the difference in means.\nC. The mean value of the population.\nD. The observed test statistic.\n\n(10.5) In a two-sample means hypothesis test, if the \\(p\\)-value is greater than the significance level, what can we conclude?\nA. We reject the null hypothesis and conclude that the means are statistically different.\nB. We fail to reject the null hypothesis and conclude that there is no evidence of a difference in means.\nC. The null hypothesis is incorrect.\nD. The test is invalid, and no conclusions can be drawn.\n\n\n\n\n(10.1) What is the null hypothesis for the one-sample mean test conducted on the penguin body mass data?\nCorrect Answer:\nD. The mean body mass of penguins is equal to 4100 grams.\nExplanation:\nThe null hypothesis in this case states that there is no difference between the true mean body mass of penguins and the hypothesized value of 4100 grams.\n\n(10.2) When performing a two-sample mean test comparing the flipper length between the penguins from the island of Biscoe to those from Dream, what does the null hypothesis represent?\nCorrect Answer:\nC. The mean flipper length of penguins on Biscoe Island is equal to the mean flipper length of penguins on Dream Island.\nExplanation:\nThe null hypothesis in a two-sample test asserts that there is no difference between the mean flipper lengths of penguins on Biscoe and Dream Islands, meaning any observed difference is due to random variation in the sample.\n\n(10.3) In a one-sample hypothesis test using bootstrapping, what does a \\(p\\)-value less than \\(\\alpha\\) indicate?\nCorrect Answer:\nB. We reject the null hypothesis and have support for the alternative hypothesis.\nExplanation:\nA \\(p\\)-value below \\(\\alpha\\) suggests that there is sufficient evidence to reject the null hypothesis, meaning the sample data provides evidence that the mean body mass is not 4100 grams.\n\n(10.4) What does the shaded area in the null distribution represent when visualizing the hypothesis test?\nCorrect Answer:\nA. The \\(p\\)-value, which is the probability of obtaining a test statistic as extreme as the observed one under the null hypothesis.\nExplanation:\nThe shaded region represents the p-value, which quantifies how likely it is to observe a test statistic as extreme or more extreme than the one obtained, assuming the null hypothesis is true.\n\n(10.5) In a two-sample hypothesis test, if the \\(p\\)-value is greater than the significance level, what can we conclude?\nCorrect Answer:\nB. We fail to reject the null hypothesis and conclude that there is no evidence of a difference in means.\nExplanation:\nA \\(p\\)-value greater than the significance level means we do not have enough evidence to reject the null hypothesis, so we conclude that there is no statistically significant difference between the two group means."
  },
  {
    "objectID": "answers/day4_walkthrough_answers.html#session-11-inference-for-regression",
    "href": "answers/day4_walkthrough_answers.html#session-11-inference-for-regression",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# Fit a multiple regression model without interaction terms\nmulti_model_no_interaction &lt;- lm(body_mass_g ~ flipper_length_mm + species, \n                              data = penguins_data)\n\n# Get regression coefficients\ncoef(multi_model_no_interaction)\n\n      (Intercept) flipper_length_mm  speciesChinstrap     speciesGentoo \n      -4013.17889          40.60617        -205.37548         284.52360 \n\n\n\n\n\n\n\n# Focus on only the columns of interest\nmodel_data &lt;- penguins_data |&gt; \n  select(body_mass_g, flipper_length_mm, species)\n\n# Plot a matrix to view the relationships between variables\nggpairs(model_data, columns = 1:3, mapping = aes(color = species))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can perform hypothesis tests to determine if the regression (partial slope) coefficients are significantly different from zero using the moderndive package.\n\n# Conduct hypothesis tests for regression coefficients\nget_regression_table(multi_model_no_interaction)\n\n# A tibble: 4 × 7\n  term               estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept           -4013.     586.       -6.85   0      -5166.   -2860. \n2 flipper_length_mm      40.6      3.08     13.2    0         34.5     46.7\n3 species: Chinstrap   -205.      57.6      -3.57   0       -319.     -92.1\n4 species: Gentoo       285.      95.4       2.98   0.003     96.8    472. \n\n\nThe p_value column in the output table provides the \\(p\\)-values for testing the null hypothesis that each coefficient is equal to zero. Each of these p_values is very small, indicating that the coefficients are significantly different from zero. This corresponds to each of them being significant predictors in the model. Note also that none of the confidence intervals in the lower_ci to upper_ci columns contain zero when looking at each row.\nReview Section 10.5 of ModernDive Second Edition for more details on the theory behind hypothesis testing and confidence intervals for regression coefficients.\n\n\n\n\n# Get fitted values and residuals:\nfit_and_res_mult &lt;- get_regression_points(multi_model_no_interaction)\nfit_and_res_mult\n\n# A tibble: 333 × 6\n      ID body_mass_g flipper_length_mm species body_mass_g_hat residual\n   &lt;int&gt;       &lt;int&gt;             &lt;int&gt; &lt;fct&gt;             &lt;dbl&gt;    &lt;dbl&gt;\n 1     1        3750               181 Adelie            3337.    413. \n 2     2        3800               186 Adelie            3540.    260. \n 3     3        3250               195 Adelie            3905.   -655. \n 4     4        3450               193 Adelie            3824.   -374. \n 5     5        3650               190 Adelie            3702.    -52.0\n 6     6        3625               181 Adelie            3337.    288. \n 7     7        4675               195 Adelie            3905.    770. \n 8     8        3200               182 Adelie            3377.   -177. \n 9     9        3800               191 Adelie            3743.     57.4\n10    10        4400               198 Adelie            4027.    373. \n# ℹ 323 more rows\n\n\n\n# Visualize the fitted values and the residuals\nggplot(fit_and_res_mult, aes(x = body_mass_g_hat, y = residual)) +\n  geom_point() +\n  labs(x = \"fitted values (body mass in grams)\", y = \"residual\") +\n  geom_hline(yintercept = 0, col = \"blue\")\n\n\n\n# Visualize the residuals using a QQ plot\nggplot(fit_and_res_mult, aes(sample = residual)) +\n  geom_qq() +\n  geom_qq_line(col=\"blue\", linewidth = 0.5)\n\n\n\n\nMore details on checking model fit and model assumptions can be found in Subsection 10.2.6 of ModernDive Second Edition.\n\nThe first plot shows the residuals against the fitted values, which should be randomly scattered around the horizontal line at zero. We don’t see much deviation from this line, which is a good sign, and evidence for the both the of assumptions of Linearity and Equal (constant) variance.\nWe also do not have any reason to assume that Independence is violated based on how the penguins were likely sampled for study.\nThe second plot shows the residuals against a theoretical normal distribution. The residuals should fall along the line, which they do for the most part. There is a slight deviation at the tails, but this is not a major concern. Thus, the Normality assumption is not violated.\n\nBy checking the LINE acrostic, we can feel confident in the model fit and assumptions.\n\n\n\n\nWe can also use simulation-based inference to test the significance of the regression coefficients. Here, we will use bootstrapping to generate confidence intervals for the partial slope coefficients. Bootstrapping now bootstraps the entire row of data, instead of just a single variable\n\n# Get the observed model fit\nobserved_fit &lt;- penguins_data |&gt; \n  specify(formula = body_mass_g ~ flipper_length_mm + species) |&gt; \n  fit()\nobserved_fit\n\n# A tibble: 4 × 2\n  term              estimate\n  &lt;chr&gt;                &lt;dbl&gt;\n1 intercept          -4013. \n2 flipper_length_mm     40.6\n3 speciesChinstrap    -205. \n4 speciesGentoo        285. \n\n# Bootstrap distribution for the partial slope coefficients\nset.seed(2024)\nmlr_resamples &lt;- penguins_data |&gt; \n  specify(formula = body_mass_g ~ flipper_length_mm + species) |&gt; \n  generate(reps = 1000, type = \"bootstrap\")\n\n# Then fit a linear regression model to each replicate of data\nbootstrap_models &lt;- mlr_resamples |&gt; \n  fit()\nbootstrap_models\n\n# A tibble: 4,000 × 3\n# Groups:   replicate [1,000]\n   replicate term              estimate\n       &lt;int&gt; &lt;chr&gt;                &lt;dbl&gt;\n 1         1 intercept          -5067. \n 2         1 flipper_length_mm     46.3\n 3         1 speciesChinstrap    -320. \n 4         1 speciesGentoo         92.3\n 5         2 intercept          -3609. \n 6         2 flipper_length_mm     38.4\n 7         2 speciesChinstrap    -147. \n 8         2 speciesGentoo        371. \n 9         3 intercept          -2408. \n10         3 flipper_length_mm     32.3\n# ℹ 3,990 more rows\n\n# Visualize the bootstrap distribution for the partial slope coefficients\nvisualize(bootstrap_models)\n\n\n\n# Get the confidence intervals for the partial slope coefficients\nconfidence_intervals_mlr &lt;- bootstrap_models |&gt; \n  get_confidence_interval(\n    level = 0.95,\n    type = \"percentile\",\n    point_estimate = observed_fit)\nconfidence_intervals_mlr\n\n# A tibble: 4 × 3\n  term              lower_ci upper_ci\n  &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n1 flipper_length_mm     35.2     46.9\n2 intercept          -5210.   -2991. \n3 speciesChinstrap    -315.    -105. \n4 speciesGentoo         77.1    466. \n\n\nWe see similar results here compared to the coefficients produced by get_regression_table() for the theoretical results:\n\nget_regression_table(multi_model_no_interaction) |&gt; \n  select(term, lower_ci, upper_ci)\n\n# A tibble: 4 × 3\n  term               lower_ci upper_ci\n  &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept           -5166.   -2860. \n2 flipper_length_mm      34.5     46.7\n3 species: Chinstrap   -319.     -92.1\n4 species: Gentoo        96.8    472. \n\n\n\n\n\n\n\n# Generate the null distributions for each of the regression coefficients\nnull_distribution_mlr &lt;- penguins_data |&gt;\n  specify(formula = body_mass_g ~ flipper_length_mm + species) |&gt; \n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  fit()\n\n# Visualize and shade the p-value for each regressor\nnull_distribution_mlr |&gt; \n  visualize() +\n  shade_p_value(obs_stat = observed_fit, direction = \"two sided\")\n\n\n\n# Get the p-values\nnull_distribution_mlr |&gt;\n  get_p_value(obs_stat = observed_fit, direction = \"two-sided\")\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an approximation based on the number of `reps` chosen in\nthe `generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\nPlease be cautious in reporting a p-value of 0. This result is an approximation based on the number of `reps` chosen in\nthe `generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\n\n\n# A tibble: 4 × 2\n  term              p_value\n  &lt;chr&gt;               &lt;dbl&gt;\n1 flipper_length_mm   0    \n2 intercept           0    \n3 speciesChinstrap    0.098\n4 speciesGentoo       0.18 \n\n\nWe see different results for species Gentoo with the simulation-based methods. Further analysis should be done to check the normality assumption on the Gentoo data.\n\n\n\n\n(11.1) What is the null hypothesis when performing a hypothesis test on a regression coefficient in a multiple regression model?\nA. The coefficient is significantly different from zero.\nB. The coefficient is equal to the observed mean.\nC. The coefficient is equal to zero, indicating no effect of the predictor on the response variable.\nD. The coefficient is greater than zero, indicating a positive effect of the predictor on the response variable.\n\n(11.2) When interpreting the p-value for a regression coefficient, what does a small \\(p\\)-value (e.g., &lt; 0.05) indicate?\nA. The predictor is not a significant variable in the model.\nB. There is strong evidence against the null hypothesis, suggesting the coefficient is significantly different from zero.\nC. The model fit is poor, and assumptions of linearity are violated.\nD. The residuals are not normally distributed.\n\n(11.3) In multiple regression, what does it mean when a confidence interval for a regression coefficient does not contain zero?\nA. The coefficient is not statistically significant.\nB. The coefficient is equal to zero.\nC. The residuals are randomly distributed around zero. D. The coefficient is statistically significant/discernible, and the predictor has an effect on the response variable.\n\n(11.4) Why is it important to check the residuals of a multiple regression model using diagnostic plots like the residual plot and the QQ plot?\nA. To ensure that the response variable is correctly predicted by the model.\nB. To verify that the assumptions of linearity, normality, and constant variance are not violated.\nC. To determine the p-value for each coefficient in the model.\nD. To check if the confidence intervals contain zero.\n\n(11.5) What is the purpose of bootstrapping in the context of multiple regression?\nA. To simulate many different regression models and estimate the sampling distribution of the regression coefficients.\nB. To generate random samples from the population to estimate new coefficients.\nC. To test the normality of the residuals.\nD. To calculate the exact p-values for the regression coefficients.\n\n\n\n\n(11.1) What is the null hypothesis when performing a hypothesis test on a regression coefficient in a multiple regression model?\nCorrect Answer:\nC. The coefficient is equal to zero, indicating no effect of the predictor on the response variable.\nExplanation:\nThe null hypothesis in a regression test is that the coefficient equals zero, meaning the predictor does not significantly influence the response variable.\n\n(11.2) When interpreting the p-value for a regression coefficient, what does a small p-value (e.g., &lt; 0.05) indicate?\nCorrect Answer:\nB. There is strong evidence against the null hypothesis, suggesting the coefficient is significantly different from zero.\nExplanation:\nA small p-value suggests that the coefficient is statistically significant and that the predictor has an effect on the response variable.\n\n(11.3) In multiple regression, what does it mean when a confidence interval for a regression coefficient does not contain zero?\nCorrect Answer:\nD. The coefficient is statistically significant, and the predictor has an effect on the response variable.\nExplanation:\nIf the confidence interval does not contain zero, the coefficient is considered significant, indicating that the predictor likely has an effect on the response variable.\n\n(11.4) Why is it important to check the residuals of a multiple regression model using diagnostic plots like the residual plot and the QQ plot?\nCorrect Answer:\nB. To verify that the assumptions of linearity, normality, and constant variance are not violated.\nExplanation:\nDiagnostic plots help confirm that the model assumptions (linearity, normality, constant variance) are satisfied, ensuring the validity of the regression results.\n\n(11.5) What is the purpose of bootstrapping in the context of multiple regression?\nCorrect Answer:\nA. To simulate many different regression models and estimate the sampling distribution of the regression coefficients.\nExplanation:\nBootstrapping resamples the data and fits new regression models to estimate the sampling distribution of the coefficients, allowing for confidence intervals and significance tests."
  },
  {
    "objectID": "answers/day4_walkthrough_answers.html#session-12-storytelling-with-data",
    "href": "answers/day4_walkthrough_answers.html#session-12-storytelling-with-data",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n✔ readr     2.1.5     \n── Conflicts ────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThere are more packages in the tidyverse that we don’t cover much in the book, but I’ve found incredibly useful for data wrangling:\n\nforcats for working with factors\nstringr for working with strings\npurrr for working with functions\n\n\n# This is the same as running each of these individually\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(forcats)\n\nThere are even more packages loaded than this though. Check out the full list on the tidyverse page.\nThe infer package is part of the tidymodels collection of packages that are designed to work well with the tidyverse for modeling and inference too.\nThere’s so much more we could cover in this course. Check out Chapter 11 of ModernDive Second Edition for an overview as well. I hope you’ve gotten a good amount of tools and theory to help you get started with data analysis and statistics in R using the tidyverse, moderndive, and infer to wrangle, visualize, model, and infer from your data. Go tell your story with data!"
  },
  {
    "objectID": "answers/day2_walkthrough_answers.html",
    "href": "answers/day2_walkthrough_answers.html",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# Load the required libraries\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(moderndive)\n\n\nThese packages provide tools for data wrangling, visualization, and modeling.\n\n\n\n\n\n\nlibrary(palmerpenguins)\n\n# Only use complete rows of data\npenguins_data &lt;- penguins |&gt; \n  na.omit()\n\n\nWe’ll be exploring the penguins data frame from the palmerpenguins package as well. It contains size measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica. More information is here.\n\nWe remove rows with missing values.\n\n\n\n\n\n\n# Use glimpse to explore the structure of the dataset\nglimpse(penguins_data)\n\nRows: 333\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torg…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6, 34.6, 36.6, 38.7, 42.5, 34.4, 46.0, 37…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2, 21.1, 17.8, 19.0, 20.7, 18.4, 21.5, 18…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 185, 195, 197, 184, 194, 174, 180, 189, 18…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800, 4400, 3700, 3450, 4500, 3325, 4200, 34…\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, female, male, male, female, female, male, …\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 20…\n\n\nThe glimpse() function gives an overview of the data, showing the columns and data types.\n\n\n\n\n\n# Summarize mean and median values for flipper length and body mass\npenguins_data |&gt;\n  summarize(mean_flipper_length = mean(flipper_length_mm),\n            mean_body_mass = mean(body_mass_g),\n            median_flipper_length = median(flipper_length_mm),\n            median_body_mass = median(body_mass_g))\n\n# A tibble: 1 × 4\n  mean_flipper_length mean_body_mass median_flipper_length median_body_mass\n                &lt;dbl&gt;          &lt;dbl&gt;                 &lt;int&gt;            &lt;int&gt;\n1                201.          4207.                   197             4050\n\n\nHere, we calculate the mean and median for both flipper_length_mm and body_mass_g.\n\n\n\n\n\n# Use tidy_summary to get a clean summary of the data\npenguins_data |&gt;\n  select(flipper_length_mm, body_mass_g) |&gt;\n  tidy_summary()\n\n# A tibble: 2 × 11\n  column                n group type      min    Q1  mean median    Q3   max    sd\n  &lt;chr&gt;             &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 flipper_length_mm   333 &lt;NA&gt;  numeric   172   190  201.    197   213   231  14.0\n2 body_mass_g         333 &lt;NA&gt;  numeric  2700  3550 4207.   4050  4775  6300 805. \n\n\nThe tidy_summary() function from moderndive provides a tidy summary of flipper_length_mm and body_mass_g, showing key statistics like mean, median, and standard deviation in a clean format.\n\n\n\n\n\n# Compute the correlation between flipper length and body mass\npenguins_data |&gt;\n  get_correlation(formula = flipper_length_mm ~ body_mass_g)\n\n# A tibble: 1 × 1\n    cor\n  &lt;dbl&gt;\n1 0.873\n\n\nThis command calculates the correlation between flipper_length_mm and body_mass_g.\n\n\n\n\n\n# Plot the relationship between flipper length and body mass\nggplot(penguins_data, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\",\n       title = \"Scatterplot of Flipper Length and Body Mass\")\n\n\n\n\nThis scatterplot shows the relationship between flipper length and body mass.\n\n\n\n\n\n# The same scatterplot with a regression line added\nggplot(penguins_data, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\",\n       title = \"Flipper Length vs. Body Mass with Regression Line\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThis scatterplot includes a linear regression line to show the trend in the data.\n\n\n\n\n\n# Fit a linear regression model\nflipper_model &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins_data)\n\n# Get the regression coefficients\ncoef(flipper_model)\n\n      (Intercept) flipper_length_mm \n      -5872.09268          50.15327 \n\n\nHere, we fit a linear model to predict body_mass_g using flipper_length_mm and extract the coefficients of the model.\n\n\n\n\n(4.1) What is the purpose of the na.omit() function in the code below?\npenguins_data &lt;- penguins |&gt;\n  select(species, island, flipper_length_mm, body_mass_g) |&gt;\n  na.omit()\nA. It replaces missing values with the median.\nB. It removes any rows that contain missing values.\nC. It converts missing values to zeros.\nD. It fills missing values with the previous non-missing value.\n\n(4.2) What does the tidy_summary() function do when applied to numeric columns in the penguins_data data frame?\nA. It generates summary statistics for each column in the data frame.\nB. It prints the first 6 rows of the data frame.\nC. It provides a compact overview of the data, including column names and data types.\nD. It creates a scatterplot of the variables in the data frame.\n\n(4.3) Which of the following correctly calculates the mean body mass in the penguins_data data frame?\nA. summarize |&gt; penguins_data(mean_flipper = mean(body_mass_g))\nB. summarize(mean(body_mass_g))\nC. summarize(penguins_data, mean = body_mass_g) D. penguins_data |&gt; summarize(mean_body_mass = mean(body_mass_g))\n\n(4.4) What does the get_correlation() function do when applied in the code below?\npenguins_data |&gt;\n  get_correlation(formula = flipper_length_mm ~ body_mass_g)\nA. It fits a linear regression model.\nB. It gives a measure of the linear relationship between flipper length and body mass. C. It plots a scatterplot of flipper length and body mass.\nD. It generates summary statistics for flipper length and body mass.\n\n(4.5) In the following code, what is the purpose of geom_smooth(method = \"lm\", se = FALSE)?\nggplot(penguins_data, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\",\n       title = \"Flipper Length vs. Body Mass with Regression Line\")\nA. It adds a smoothed curve based on a polynomial fit to the scatterplot.\nB. It adds a linear regression line to the scatterplot without displaying the confidence interval.\nC. It adjusts the transparency of the points in the scatterplot.\nD. It calculates and displays residuals on the plot.\n\n\n\n\n(4.1) What is the purpose of the na.omit() function in the code below?\nCorrect Answer:\nB. It removes any rows that contain missing values.\nExplanation:\nThe na.omit() function is used to filter out rows that have missing values, ensuring the analysis is performed on complete cases only.\n\n(4.2) What does the tidy_summary() function do when applied to numeric columns in the penguins_data data frame?\nCorrect Answer:\nA. It generates summary statistics for each column in the data frame.\nExplanation:\ntidy_summary() in the moderndive package produces summary statistics for each selected numeric column in the data frame provided.\n\n(4.3) Which of the following correctly calculates the mean body mass in the penguins_data data frame?\nCorrect Answer:\nD. penguins_data |&gt; summarize(mean_body_mass = mean(body_mass_g)\nExplanation:\nThe correct syntax includes both column names and calculated statistics for each variable using the summarize() function.\n\n(4.4) What does the get_correlation() function do when applied in the code below?\npenguins_data |&gt;\n  get_correlation(formula = flipper_length_mm ~ body_mass_g)\nCorrect Answer:\nB. It gives a measure of the linear relationship between flipper length and body mass.\nExplanation:\nThe get_correlation() function computes the correlation coefficient between the two specified variables, indicating the strength and direction of their linear relationship.\n\n(4.5) In the following code, what is the purpose of geom_smooth(method = \"lm\", se = FALSE)?\nggplot(penguins_data, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\",\n       title = \"Flipper Length vs. Body Mass with Regression Line\")\nCorrect Answer:\nB. It adds a linear regression line to the scatterplot without displaying the confidence interval.\nExplanation:\ngeom_smooth(method = \"lm\") fits a linear model to the data, and se = FALSE suppresses the shaded confidence interval around the regression line.\n\n\n\n\n\n\n\n\n# Fit a simple linear regression model with species as a predictor\nspecies_model &lt;- lm(body_mass_g ~ species, data = penguins_data)\n\n# Get regression coefficients\ncoef(species_model)\n\n     (Intercept) speciesChinstrap    speciesGentoo \n      3706.16438         26.92385       1386.27259 \n\n\nExplanation: We fit a simple linear regression model to predict body_mass_g using species as a categorical predictor. The coefficients represent the differences in body mass between the species with a baseline set as the first species alphabetically (Adélie).\n\n\n\n\n\n# Scatterplot with species as color for grouping\nggplot(penguins_data, \n       aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", color = \"Species\")\n\n\n\n\nExplanation:\nWe create a scatterplot to visualize the relationship between flipper_length_mm and body_mass_g, using species as a color grouping. This allows us to see how the relationship varies across penguin species.\n\n\n\n\n\n# Summarize the target and regressors\npenguins_data |&gt;\n  select(body_mass_g, flipper_length_mm, species) |&gt;\n  tidy_summary()\n\n# A tibble: 5 × 11\n  column                n group     type      min    Q1  mean median    Q3   max    sd\n  &lt;chr&gt;             &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 body_mass_g         333 &lt;NA&gt;      numeric  2700  3550 4207.   4050  4775  6300 805. \n2 flipper_length_mm   333 &lt;NA&gt;      numeric   172   190  201.    197   213   231  14.0\n3 species             146 Adelie    factor     NA    NA   NA      NA    NA    NA  NA  \n4 species              68 Chinstrap factor     NA    NA   NA      NA    NA    NA  NA  \n5 species             119 Gentoo    factor     NA    NA   NA      NA    NA    NA  NA  \n\n\nExplanation:\nWe use tidy_summary() to summarize the target variable body_mass_g and the regressors flipper_length_mm and species. This provides a concise overview of the data, including key statistics for each variable. Note that species only lists the three levels of the group since the remaining columns correspond to numeric variables only.\n\n\n\n\n\n# Fit a multiple regression model with interaction\nmulti_model_interaction &lt;- lm(body_mass_g ~ flipper_length_mm * species, \n                              data = penguins_data)\n\n# Get regression coefficients\ncoef(multi_model_interaction)\n\n                       (Intercept)                  flipper_length_mm                   speciesChinstrap \n                       -2508.08774                           32.68891                         -529.10803 \n                     speciesGentoo flipper_length_mm:speciesChinstrap    flipper_length_mm:speciesGentoo \n                       -4166.11653                            1.88448                           21.47651 \n\n\nExplanation:\nWe fit a multiple regression model that includes both flipper_length_mm and species as predictors of body_mass_g. The * symbol indicates that we are fitting both main effects and interaction terms. The regression coefficients reflect the impact of both the flipper length and the species on body mass as well as the interaction between flipper length and species.\nIn the fitted model, species is represented as dummy variables. For example, the coefficient for speciesChinstrap shows how being a Chinstrap penguin affects body mass relative to the baseline species (Adélie). Similarly, the interaction terms (e.g., flipper_length_mm:speciesGentoo) tell us how the relationship between flipper length and body mass changes for each species compared to the baseline.\nIn this model (multi_model_interaction), we’ve included an interaction term between flipper_length_mm and species. This interaction term allows the effect of flipper length on body mass to differ depending on the species of penguin. Here’s how to interpret the coefficients:\n\n\nThe model is predicting body_mass_g (body mass of penguins in grams) based on flipper_length_mm (flipper length in millimeters), species, and their interaction (flipper_length_mm * species). The interaction term means that the relationship between flipper length and body mass can change depending on the species.\n\n\n\n\nThis is the predicted body mass for an Adelie penguin (the reference category for species) when its flipper length is 0. Again, this may not be realistic, but it provides a baseline for interpretation.\n\n\n\n\n\nFor Adelie penguins, each additional millimeter in flipper length increases their predicted body mass by about 32.69 grams. This coefficient represents the slope of flipper_length_mm for the reference species (Adelie).\n\n\n\n\n\nThis is the difference in the baseline body mass between Chinstrap penguins and Adelie penguins when flipper length is 0. Chinstrap penguins are predicted to weigh 529.11 grams less than Adelie penguins at the same flipper length of 0.\n\n\n\n\n\nSimilarly, Gentoo penguins are predicted to weigh 4166.12 grams less than Adelie penguins at a flipper length of 0. This is a large value, but remember that the intercept applies to the scenario where flipper length is 0, which may not be relevant in practice.\n\n\n\n\n\nThis is the interaction effect. It shows how the relationship between flipper length and body mass changes for Chinstrap penguins compared to Adelie penguins.\nFor Chinstrap penguins, each additional millimeter in flipper length is associated with an additional increase of 1.88 grams in body mass on top of the 32.69 grams increase for Adelie penguins. So the total effect of flipper length on body mass for Chinstrap penguins is 32.69 + 1.88 = 34.57 grams per millimeter of flipper length.\n\n\n\n\n\nThis interaction term shows how the relationship between flipper length and body mass changes for Gentoo penguins compared to Adelie penguins.\nFor Gentoo penguins, each additional millimeter in flipper length is associated with an additional increase of 21.48 grams in body mass on top of the 32.69 grams increase for Adelie penguins. So the total effect of flipper length on body mass for Gentoo penguins is 32.69 + 21.48 = 54.17 grams per millimeter of flipper length.\n\n\n\n\n\nFor Adelie penguins, each additional millimeter in flipper length increases body mass by about 32.69 grams.\nFor Chinstrap penguins, the increase in body mass per millimeter of flipper length is slightly higher, at about 34.57 grams (32.69 + 1.88).\nFor Gentoo penguins, the increase in body mass per millimeter of flipper length is even higher, at about 54.17 grams (32.69 + 21.48).\nThe baseline (intercept) body mass for Chinstrap and Gentoo penguins is lower than for Adelie penguins, but this only applies when flipper length is 0 (which is not a realistic flipper length but is the mathematical interpretation).\n\nBy including the interaction term, this model reveals that the relationship between flipper length and body mass differs across species.\n\n\n\n\n\n\n# Scatterplot with regression lines by species\nggplot(penguins_data, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", color = \"Species\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nExplanation:\nWe extend the scatterplot by adding regression lines for each species, showing how the relationship between flipper_length_mm and body_mass_g differs across species. Each line represents a separate slope for each species, which reflects the interaction between flipper length and species in the model.\n\n\n\n\n(5.1) What is the default baseline island in the simple linear regression model for predicting body_mass_g using island as a categorical variable?\nA. Torgersen\nB. Biscoe\nC. Dream\nD. The island with the highest body mass\n\n(5.2) In the following regression model with interaction terms, what does the coefficient for flipper_length_mm:speciesGentoo represent?\nmulti_model_interaction &lt;- lm(body_mass_g ~ flipper_length_mm * species, data = penguins_data)\nA. The slope of the regression line for Gentoo penguins\nB. The average body mass of Gentoo penguins\nC. The change in the relationship between flipper length and body mass for Gentoo penguins compared to the baseline\nD. The difference in intercept for Gentoo penguins compared to the baseline\n\n(5.3) What is the purpose of adding interaction terms to a multiple regression model?\nA. To make the model more complex without any real benefit\nB. To estimate the relationship between each explanatory variable and the response variable independently\nC. To account for how the relationship between one explanatory variable and the response depends on another explanatory variable\nD. To automatically improve the model’s R-squared value\n\n(5.4) What does the following plot indicate about the relationship between flipper length and body mass for different islands?\n\nggplot(penguins_data, aes(x = flipper_length_mm, y = body_mass_g, color = island)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", color = \"Island\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nA. The relationship between flipper length and body mass is the same for all islands\nB. The relationship between flipper length and body mass differs across species, with each island having a different slope\nC. The islands do not influence the relationship between flipper length and body mass\nD. Only the intercept differs for each island, but the slope is the same\n\n(5.5) In the multiple regression model with interaction terms, how are dummy variables used for categorical predictors?\nA. Dummy variables are used to represent the different categories of a categorical variable B. Dummy variables are used to represent each numerical variable\nC. Dummy variables are used to replace the intercept in the regression model\nD. Dummy variables are not necessary for categorical variables\n\n\n\n\n(5.1) What is the default baseline island in the simple linear regression model for predicting body_mass_g using island as a categorical variable?\nCorrect Answer:\nB. Biscoe\nExplanation:\nIn a regression model with categorical variables, the baseline group is typically the first level alphabetically. Since Biscoe comes first, it is the baseline for comparison.\n\n(5.2) In the following regression model with interaction terms, what does the coefficient for flipper_length_mm:speciesGentoo represent?\nCorrect Answer:\nD. The change in the relationship between flipper length and body mass for Gentoo penguins compared to the baseline\nExplanation:\nThe interaction term flipper_length_mm:speciesGentoo represents how the effect of flipper_length_mm on body_mass_g changes for Gentoo penguins compared to the baseline species, Adélie.\n\n(5.3) What is the purpose of adding interaction terms to a multiple regression model?\nCorrect Answer:\nC. To account for how the relationship between one explanatory variable and the response depends on another explanatory variable\nExplanation:\nInteraction terms allow us to model how the effect of one predictor (e.g., flipper length) on the outcome (body mass) changes depending on the level of another predictor (e.g., species).\n\n(5.4) What does the following plot indicate about the relationship between flipper length and body mass for different islands?\n\nggplot(penguins_data, aes(x = flipper_length_mm, y = body_mass_g, color = island)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", color = \"Island\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCorrect Answer:\nB. The relationship between flipper length and body mass differs across species, with each species having a different slope\nExplanation:\nThe plot shows separate regression lines for each species, indicating that the relationship between flipper_length_mm and body_mass_g varies by species, with each species having a unique slope.\n\n(5.5) In the multiple regression model with interaction terms, how are dummy variables used for categorical predictors?\nCorrect Answer:\nA. Dummy variables are used to represent the different categories of a categorical variable\nExplanation:\nIn regression models, dummy variables are created for categorical variables like species to represent the different levels (e.g., Adélie, Chinstrap, Gentoo) in the model. These dummy variables allow the model to estimate how each category affects the outcome relative to the baseline category.\n\n\n\n\n\n\n\n\n# Fit a multiple regression model without interaction terms\nmulti_model_no_interaction &lt;- lm(body_mass_g ~ flipper_length_mm + species, \n                              data = penguins_data)\n\n# Get regression coefficients\ncoef(multi_model_no_interaction)\n\n      (Intercept) flipper_length_mm  speciesChinstrap     speciesGentoo \n      -4013.17889          40.60617        -205.37548         284.52360 \n\n\nExplanation:\n\nWe fit a multiple regression model predicting body_mass_g (body mass) using both flipper_length_mm (flipper length) and species as explanatory variables.\nIn this model, the effect of each explanatory variable on body mass is assumed to be independent, meaning the slopes are the same across different levels of the other variable (i.e., no interaction).\nThe coefficients indicate the effect of each predictor on body mass, with each variable having its own impact on the response.\n\nThe output from our linear regression model (multi_model_no_interaction) can be interpreted as follows:\n\n\nThe model is predicting body_mass_g (the body mass of penguins in grams) based on flipper_length_mm (flipper length in millimeters) and species. Since the species variable is categorical, the model automatically creates dummy variables for the species, treating one species (Adelie) as the reference category.\nLet’s break down each coefficient:\n\n\n\n\nThis is the predicted body mass (in grams) for a penguin that belongs to the reference species Adelie and has a flipper_length_mm of 0 (which may not be realistic in this context, but it’s the mathematical meaning).\nIn simpler terms, it provides a baseline body mass for the reference species when other variables (like flipper length) are zero.\n\n\n\n\n\nFor each additional millimeter in flipper length, the model predicts an increase of approximately 40.61 grams in body mass, holding the species constant.\nThis means that flipper length has a positive effect on body mass.\n\n\n\n\n\nChinstrap penguins are expected to have, on average, body masses that are 205.38 grams lower than Adelie penguins (the reference species), holding flipper_length_mm constant.\nThis represents the difference between the Chinstrap species and the Adelie species in terms of body mass, after accounting for flipper length.\n\n\n\n\n\nGentoo penguins are expected to have, on average, body masses that are 284.52 grams higher than Adelie penguins, holding flipper_length_mm constant.\nThis shows that, after adjusting for flipper length, Gentoo penguins tend to be heavier than the Adelie species.\n\n\n\n\n\nThe model suggests that penguin body mass increases as flipper length increases.\nChinstrap penguins tend to have lower body masses than Adelie penguins (reference category), while Gentoo penguins tend to have higher body masses compared to Adelie penguins, even after accounting for differences in flipper length.\n\n\n\n\n\n\n\n# Scatterplot with regression lines by species (different intercepts but same slopes)\nggplot(penguins_data, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  geom_parallel_slopes(se = FALSE) +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", color = \"Species\")\n\n\n\n\nExplanation:\n\nWe visualize the fitted multiple regression model without interactions using parallel slope lines.\nThe function geom_parallel_slopes() is used to create regression lines with the same slope for each species.\nThis helps to visualize the idea of parallel slopes, where each regression line has the same slope but different intercepts depending on the other explanatory variable (flipper length).\n\n\n\n\n\n\n# Use tidy_summary to get a clean summary of the data for a multiple regression\n# with flipper length and bill length and regressors and body mass as the\n# outcome\npenguins_data |&gt;\n  select(body_mass_g, flipper_length_mm, bill_length_mm) |&gt;\n  tidy_summary()\n\n# A tibble: 3 × 11\n  column                n group type       min     Q1   mean median     Q3    max     sd\n  &lt;chr&gt;             &lt;int&gt; &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 body_mass_g         333 &lt;NA&gt;  numeric 2700   3550   4207.  4050   4775   6300   805.  \n2 flipper_length_mm   333 &lt;NA&gt;  numeric  172    190    201.   197    213    231    14.0 \n3 bill_length_mm      333 &lt;NA&gt;  numeric   32.1   39.5   44.0   44.5   48.6   59.6   5.47\n\n\nThis provides a tidy summary, giving key statistics such as the mean, median, and standard deviation for the selected variables.\n\n\n\n\n\n# Fit a multiple regression model\nmulti_model &lt;- lm(body_mass_g ~ flipper_length_mm + bill_length_mm, \n                  data = penguins_data)\n\n# Get regression coefficients\ncoef(multi_model)\n\n      (Intercept) flipper_length_mm    bill_length_mm \n     -5836.298732         48.889692          4.958601 \n\n\nIn this multiple regression model, we predict body_mass_g using both flipper_length_mm and bill_length_mm. The coefficients show the effect of each variable on the response.\n\n\n\n\n\n# Create scatterplot with regression lines for both variables\nggplot(penguins_data, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\",\n       title = \"Body Mass vs Flipper Length\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(penguins_data, aes(x = bill_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Bill Length (mm)\", y = \"Body Mass (g)\",\n       title = \"Body Mass vs Bill Length\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThese scatterplots display the relationships individually between flipper_length_mm and body_mass_g, and between bill_length_mm and body_mass_g, with linear regression lines added.\n\n\n\n\n(6.1) What is the main assumption in a multiple regression model without interaction terms?\nA. The relationship between the explanatory variables and the response is always quadratic.\nB. The slope of the regression line is different for each level of the categorical variable.\nC. The slope of the regression line is the same for all levels of the categorical variable.\nD. The intercept of the regression line is the same for all levels of the categorical variable.\n\n(6.2) In the model lm(body_mass_g ~ flipper_length_mm + species, data = penguins_data), what does the coefficient for speciesGentoo represent?\nA. The effect of being a Gentoo penguin on body mass compared to the baseline species (Adélie).\nB. The effect of flipper length on body mass for Gentoo penguins only.\nC. The change in flipper length due to species.\nD. The effect of body mass on flipper length for Gentoo penguins.\n\n(6.3) What does the function geom_parallel_slopes() do in a regression plot?\nA. It fits regression lines with different slopes for each group.\nB. It plots regression lines with the same slope but different intercepts for each group.\nC. It creates a scatterplot without any regression lines.\nD. It visualizes interaction effects between the variables.\n\n(6.4) In the model lm(body_mass_g ~ flipper_length_mm + bill_length_mm, data = penguins_data), what do the coefficients for flipper_length_mm and bill_length_mm represent?\nA. The total body mass of each penguin species.\nB. The predicted body mass for penguins with average bill and flipper lengths. C. The interaction effect between flipper length and bill length.\nD. The effect of bill length on body mass, controlling for flipper length, and vice versa.\n\n(6.5) What does a regression model without interaction terms assume about the relationship between the explanatory variables and the response variable?\nA. The explanatory variables are not related to the response.\nB. The relationship between the explanatory variables and the response is independent of one another.\nC. The explanatory variables interact to affect the response.\nD. The response variable depends only on the categorical variables.\n\n\n\n\n(6.1) What is the main assumption in a multiple regression model without interaction terms?\nCorrect Answer:\nC. The slope of the regression line is the same for all levels of the categorical variable.\nExplanation:\nIn a multiple regression model without interactions, the slopes are assumed to be the same across all levels of the categorical variable. Only the intercepts are allowed to differ by group.\n\n(6.2) In the model lm(body_mass_g ~ flipper_length_mm + species, data = penguins_data), what does the coefficient for speciesGentoo represent?\nCorrect Answer:\nA. The effect of being a Gentoo penguin on body mass compared to the baseline species (Adélie).\nExplanation:\nIn regression models with categorical variables, the coefficient for a level like speciesGentoo represents the difference in the response variable (body mass) for Gentoo penguins compared to the baseline species (Adélie).\n\n(6.3) What does the function geom_parallel_slopes() do in a regression plot?\nCorrect Answer:\nB. It plots regression lines with the same slope but different intercepts for each group.\nExplanation:\nThe geom_parallel_slopes() function is used to visualize regression models without interaction terms, where each group has the same slope but different intercepts.\n\n(6.4) In the model lm(body_mass_g ~ flipper_length_mm + bill_length_mm, data = penguins_data), what do the coefficients for flipper_length_mm and bill_length_mm represent?\nCorrect Answer:\nD. The effect of bill length on body mass, controlling for flipper length, and vice versa.\nExplanation:\nIn a multiple regression model, each coefficient represents the effect of that variable on the response, while controlling for the other variables in the model.\n\n(6.5) What does a regression model without interaction terms assume about the relationship between the explanatory variables and the response variable?\nCorrect Answer:\nB. The relationship between the explanatory variables and the response is independent of one another.\nExplanation:\nIn a regression model without interaction terms, it is assumed that the explanatory variables affect the response independently, meaning that the effect of one variable is not influenced by the level of the other."
  },
  {
    "objectID": "answers/day2_walkthrough_answers.html#session-4-simple-linear-regression-analysis",
    "href": "answers/day2_walkthrough_answers.html#session-4-simple-linear-regression-analysis",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# Load the required libraries\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(moderndive)\n\n\nThese packages provide tools for data wrangling, visualization, and modeling.\n\n\n\n\n\n\nlibrary(palmerpenguins)\n\n# Only use complete rows of data\npenguins_data &lt;- penguins |&gt; \n  na.omit()\n\n\nWe’ll be exploring the penguins data frame from the palmerpenguins package as well. It contains size measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica. More information is here.\n\nWe remove rows with missing values.\n\n\n\n\n\n\n# Use glimpse to explore the structure of the dataset\nglimpse(penguins_data)\n\nRows: 333\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torg…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6, 34.6, 36.6, 38.7, 42.5, 34.4, 46.0, 37…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2, 21.1, 17.8, 19.0, 20.7, 18.4, 21.5, 18…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 185, 195, 197, 184, 194, 174, 180, 189, 18…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800, 4400, 3700, 3450, 4500, 3325, 4200, 34…\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, female, male, male, female, female, male, …\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 20…\n\n\nThe glimpse() function gives an overview of the data, showing the columns and data types.\n\n\n\n\n\n# Summarize mean and median values for flipper length and body mass\npenguins_data |&gt;\n  summarize(mean_flipper_length = mean(flipper_length_mm),\n            mean_body_mass = mean(body_mass_g),\n            median_flipper_length = median(flipper_length_mm),\n            median_body_mass = median(body_mass_g))\n\n# A tibble: 1 × 4\n  mean_flipper_length mean_body_mass median_flipper_length median_body_mass\n                &lt;dbl&gt;          &lt;dbl&gt;                 &lt;int&gt;            &lt;int&gt;\n1                201.          4207.                   197             4050\n\n\nHere, we calculate the mean and median for both flipper_length_mm and body_mass_g.\n\n\n\n\n\n# Use tidy_summary to get a clean summary of the data\npenguins_data |&gt;\n  select(flipper_length_mm, body_mass_g) |&gt;\n  tidy_summary()\n\n# A tibble: 2 × 11\n  column                n group type      min    Q1  mean median    Q3   max    sd\n  &lt;chr&gt;             &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 flipper_length_mm   333 &lt;NA&gt;  numeric   172   190  201.    197   213   231  14.0\n2 body_mass_g         333 &lt;NA&gt;  numeric  2700  3550 4207.   4050  4775  6300 805. \n\n\nThe tidy_summary() function from moderndive provides a tidy summary of flipper_length_mm and body_mass_g, showing key statistics like mean, median, and standard deviation in a clean format.\n\n\n\n\n\n# Compute the correlation between flipper length and body mass\npenguins_data |&gt;\n  get_correlation(formula = flipper_length_mm ~ body_mass_g)\n\n# A tibble: 1 × 1\n    cor\n  &lt;dbl&gt;\n1 0.873\n\n\nThis command calculates the correlation between flipper_length_mm and body_mass_g.\n\n\n\n\n\n# Plot the relationship between flipper length and body mass\nggplot(penguins_data, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\",\n       title = \"Scatterplot of Flipper Length and Body Mass\")\n\n\n\n\nThis scatterplot shows the relationship between flipper length and body mass.\n\n\n\n\n\n# The same scatterplot with a regression line added\nggplot(penguins_data, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\",\n       title = \"Flipper Length vs. Body Mass with Regression Line\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThis scatterplot includes a linear regression line to show the trend in the data.\n\n\n\n\n\n# Fit a linear regression model\nflipper_model &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins_data)\n\n# Get the regression coefficients\ncoef(flipper_model)\n\n      (Intercept) flipper_length_mm \n      -5872.09268          50.15327 \n\n\nHere, we fit a linear model to predict body_mass_g using flipper_length_mm and extract the coefficients of the model.\n\n\n\n\n(4.1) What is the purpose of the na.omit() function in the code below?\npenguins_data &lt;- penguins |&gt;\n  select(species, island, flipper_length_mm, body_mass_g) |&gt;\n  na.omit()\nA. It replaces missing values with the median.\nB. It removes any rows that contain missing values.\nC. It converts missing values to zeros.\nD. It fills missing values with the previous non-missing value.\n\n(4.2) What does the tidy_summary() function do when applied to numeric columns in the penguins_data data frame?\nA. It generates summary statistics for each column in the data frame.\nB. It prints the first 6 rows of the data frame.\nC. It provides a compact overview of the data, including column names and data types.\nD. It creates a scatterplot of the variables in the data frame.\n\n(4.3) Which of the following correctly calculates the mean body mass in the penguins_data data frame?\nA. summarize |&gt; penguins_data(mean_flipper = mean(body_mass_g))\nB. summarize(mean(body_mass_g))\nC. summarize(penguins_data, mean = body_mass_g) D. penguins_data |&gt; summarize(mean_body_mass = mean(body_mass_g))\n\n(4.4) What does the get_correlation() function do when applied in the code below?\npenguins_data |&gt;\n  get_correlation(formula = flipper_length_mm ~ body_mass_g)\nA. It fits a linear regression model.\nB. It gives a measure of the linear relationship between flipper length and body mass. C. It plots a scatterplot of flipper length and body mass.\nD. It generates summary statistics for flipper length and body mass.\n\n(4.5) In the following code, what is the purpose of geom_smooth(method = \"lm\", se = FALSE)?\nggplot(penguins_data, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\",\n       title = \"Flipper Length vs. Body Mass with Regression Line\")\nA. It adds a smoothed curve based on a polynomial fit to the scatterplot.\nB. It adds a linear regression line to the scatterplot without displaying the confidence interval.\nC. It adjusts the transparency of the points in the scatterplot.\nD. It calculates and displays residuals on the plot.\n\n\n\n\n(4.1) What is the purpose of the na.omit() function in the code below?\nCorrect Answer:\nB. It removes any rows that contain missing values.\nExplanation:\nThe na.omit() function is used to filter out rows that have missing values, ensuring the analysis is performed on complete cases only.\n\n(4.2) What does the tidy_summary() function do when applied to numeric columns in the penguins_data data frame?\nCorrect Answer:\nA. It generates summary statistics for each column in the data frame.\nExplanation:\ntidy_summary() in the moderndive package produces summary statistics for each selected numeric column in the data frame provided.\n\n(4.3) Which of the following correctly calculates the mean body mass in the penguins_data data frame?\nCorrect Answer:\nD. penguins_data |&gt; summarize(mean_body_mass = mean(body_mass_g)\nExplanation:\nThe correct syntax includes both column names and calculated statistics for each variable using the summarize() function.\n\n(4.4) What does the get_correlation() function do when applied in the code below?\npenguins_data |&gt;\n  get_correlation(formula = flipper_length_mm ~ body_mass_g)\nCorrect Answer:\nB. It gives a measure of the linear relationship between flipper length and body mass.\nExplanation:\nThe get_correlation() function computes the correlation coefficient between the two specified variables, indicating the strength and direction of their linear relationship.\n\n(4.5) In the following code, what is the purpose of geom_smooth(method = \"lm\", se = FALSE)?\nggplot(penguins_data, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\",\n       title = \"Flipper Length vs. Body Mass with Regression Line\")\nCorrect Answer:\nB. It adds a linear regression line to the scatterplot without displaying the confidence interval.\nExplanation:\ngeom_smooth(method = \"lm\") fits a linear model to the data, and se = FALSE suppresses the shaded confidence interval around the regression line."
  },
  {
    "objectID": "answers/day2_walkthrough_answers.html#session-5-multiple-linear-regression-analysis-part-1",
    "href": "answers/day2_walkthrough_answers.html#session-5-multiple-linear-regression-analysis-part-1",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# Fit a simple linear regression model with species as a predictor\nspecies_model &lt;- lm(body_mass_g ~ species, data = penguins_data)\n\n# Get regression coefficients\ncoef(species_model)\n\n     (Intercept) speciesChinstrap    speciesGentoo \n      3706.16438         26.92385       1386.27259 \n\n\nExplanation: We fit a simple linear regression model to predict body_mass_g using species as a categorical predictor. The coefficients represent the differences in body mass between the species with a baseline set as the first species alphabetically (Adélie).\n\n\n\n\n\n# Scatterplot with species as color for grouping\nggplot(penguins_data, \n       aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", color = \"Species\")\n\n\n\n\nExplanation:\nWe create a scatterplot to visualize the relationship between flipper_length_mm and body_mass_g, using species as a color grouping. This allows us to see how the relationship varies across penguin species.\n\n\n\n\n\n# Summarize the target and regressors\npenguins_data |&gt;\n  select(body_mass_g, flipper_length_mm, species) |&gt;\n  tidy_summary()\n\n# A tibble: 5 × 11\n  column                n group     type      min    Q1  mean median    Q3   max    sd\n  &lt;chr&gt;             &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 body_mass_g         333 &lt;NA&gt;      numeric  2700  3550 4207.   4050  4775  6300 805. \n2 flipper_length_mm   333 &lt;NA&gt;      numeric   172   190  201.    197   213   231  14.0\n3 species             146 Adelie    factor     NA    NA   NA      NA    NA    NA  NA  \n4 species              68 Chinstrap factor     NA    NA   NA      NA    NA    NA  NA  \n5 species             119 Gentoo    factor     NA    NA   NA      NA    NA    NA  NA  \n\n\nExplanation:\nWe use tidy_summary() to summarize the target variable body_mass_g and the regressors flipper_length_mm and species. This provides a concise overview of the data, including key statistics for each variable. Note that species only lists the three levels of the group since the remaining columns correspond to numeric variables only.\n\n\n\n\n\n# Fit a multiple regression model with interaction\nmulti_model_interaction &lt;- lm(body_mass_g ~ flipper_length_mm * species, \n                              data = penguins_data)\n\n# Get regression coefficients\ncoef(multi_model_interaction)\n\n                       (Intercept)                  flipper_length_mm                   speciesChinstrap \n                       -2508.08774                           32.68891                         -529.10803 \n                     speciesGentoo flipper_length_mm:speciesChinstrap    flipper_length_mm:speciesGentoo \n                       -4166.11653                            1.88448                           21.47651 \n\n\nExplanation:\nWe fit a multiple regression model that includes both flipper_length_mm and species as predictors of body_mass_g. The * symbol indicates that we are fitting both main effects and interaction terms. The regression coefficients reflect the impact of both the flipper length and the species on body mass as well as the interaction between flipper length and species.\nIn the fitted model, species is represented as dummy variables. For example, the coefficient for speciesChinstrap shows how being a Chinstrap penguin affects body mass relative to the baseline species (Adélie). Similarly, the interaction terms (e.g., flipper_length_mm:speciesGentoo) tell us how the relationship between flipper length and body mass changes for each species compared to the baseline.\nIn this model (multi_model_interaction), we’ve included an interaction term between flipper_length_mm and species. This interaction term allows the effect of flipper length on body mass to differ depending on the species of penguin. Here’s how to interpret the coefficients:\n\n\nThe model is predicting body_mass_g (body mass of penguins in grams) based on flipper_length_mm (flipper length in millimeters), species, and their interaction (flipper_length_mm * species). The interaction term means that the relationship between flipper length and body mass can change depending on the species.\n\n\n\n\nThis is the predicted body mass for an Adelie penguin (the reference category for species) when its flipper length is 0. Again, this may not be realistic, but it provides a baseline for interpretation.\n\n\n\n\n\nFor Adelie penguins, each additional millimeter in flipper length increases their predicted body mass by about 32.69 grams. This coefficient represents the slope of flipper_length_mm for the reference species (Adelie).\n\n\n\n\n\nThis is the difference in the baseline body mass between Chinstrap penguins and Adelie penguins when flipper length is 0. Chinstrap penguins are predicted to weigh 529.11 grams less than Adelie penguins at the same flipper length of 0.\n\n\n\n\n\nSimilarly, Gentoo penguins are predicted to weigh 4166.12 grams less than Adelie penguins at a flipper length of 0. This is a large value, but remember that the intercept applies to the scenario where flipper length is 0, which may not be relevant in practice.\n\n\n\n\n\nThis is the interaction effect. It shows how the relationship between flipper length and body mass changes for Chinstrap penguins compared to Adelie penguins.\nFor Chinstrap penguins, each additional millimeter in flipper length is associated with an additional increase of 1.88 grams in body mass on top of the 32.69 grams increase for Adelie penguins. So the total effect of flipper length on body mass for Chinstrap penguins is 32.69 + 1.88 = 34.57 grams per millimeter of flipper length.\n\n\n\n\n\nThis interaction term shows how the relationship between flipper length and body mass changes for Gentoo penguins compared to Adelie penguins.\nFor Gentoo penguins, each additional millimeter in flipper length is associated with an additional increase of 21.48 grams in body mass on top of the 32.69 grams increase for Adelie penguins. So the total effect of flipper length on body mass for Gentoo penguins is 32.69 + 21.48 = 54.17 grams per millimeter of flipper length.\n\n\n\n\n\nFor Adelie penguins, each additional millimeter in flipper length increases body mass by about 32.69 grams.\nFor Chinstrap penguins, the increase in body mass per millimeter of flipper length is slightly higher, at about 34.57 grams (32.69 + 1.88).\nFor Gentoo penguins, the increase in body mass per millimeter of flipper length is even higher, at about 54.17 grams (32.69 + 21.48).\nThe baseline (intercept) body mass for Chinstrap and Gentoo penguins is lower than for Adelie penguins, but this only applies when flipper length is 0 (which is not a realistic flipper length but is the mathematical interpretation).\n\nBy including the interaction term, this model reveals that the relationship between flipper length and body mass differs across species.\n\n\n\n\n\n\n# Scatterplot with regression lines by species\nggplot(penguins_data, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", color = \"Species\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nExplanation:\nWe extend the scatterplot by adding regression lines for each species, showing how the relationship between flipper_length_mm and body_mass_g differs across species. Each line represents a separate slope for each species, which reflects the interaction between flipper length and species in the model.\n\n\n\n\n(5.1) What is the default baseline island in the simple linear regression model for predicting body_mass_g using island as a categorical variable?\nA. Torgersen\nB. Biscoe\nC. Dream\nD. The island with the highest body mass\n\n(5.2) In the following regression model with interaction terms, what does the coefficient for flipper_length_mm:speciesGentoo represent?\nmulti_model_interaction &lt;- lm(body_mass_g ~ flipper_length_mm * species, data = penguins_data)\nA. The slope of the regression line for Gentoo penguins\nB. The average body mass of Gentoo penguins\nC. The change in the relationship between flipper length and body mass for Gentoo penguins compared to the baseline\nD. The difference in intercept for Gentoo penguins compared to the baseline\n\n(5.3) What is the purpose of adding interaction terms to a multiple regression model?\nA. To make the model more complex without any real benefit\nB. To estimate the relationship between each explanatory variable and the response variable independently\nC. To account for how the relationship between one explanatory variable and the response depends on another explanatory variable\nD. To automatically improve the model’s R-squared value\n\n(5.4) What does the following plot indicate about the relationship between flipper length and body mass for different islands?\n\nggplot(penguins_data, aes(x = flipper_length_mm, y = body_mass_g, color = island)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", color = \"Island\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nA. The relationship between flipper length and body mass is the same for all islands\nB. The relationship between flipper length and body mass differs across species, with each island having a different slope\nC. The islands do not influence the relationship between flipper length and body mass\nD. Only the intercept differs for each island, but the slope is the same\n\n(5.5) In the multiple regression model with interaction terms, how are dummy variables used for categorical predictors?\nA. Dummy variables are used to represent the different categories of a categorical variable B. Dummy variables are used to represent each numerical variable\nC. Dummy variables are used to replace the intercept in the regression model\nD. Dummy variables are not necessary for categorical variables\n\n\n\n\n(5.1) What is the default baseline island in the simple linear regression model for predicting body_mass_g using island as a categorical variable?\nCorrect Answer:\nB. Biscoe\nExplanation:\nIn a regression model with categorical variables, the baseline group is typically the first level alphabetically. Since Biscoe comes first, it is the baseline for comparison.\n\n(5.2) In the following regression model with interaction terms, what does the coefficient for flipper_length_mm:speciesGentoo represent?\nCorrect Answer:\nD. The change in the relationship between flipper length and body mass for Gentoo penguins compared to the baseline\nExplanation:\nThe interaction term flipper_length_mm:speciesGentoo represents how the effect of flipper_length_mm on body_mass_g changes for Gentoo penguins compared to the baseline species, Adélie.\n\n(5.3) What is the purpose of adding interaction terms to a multiple regression model?\nCorrect Answer:\nC. To account for how the relationship between one explanatory variable and the response depends on another explanatory variable\nExplanation:\nInteraction terms allow us to model how the effect of one predictor (e.g., flipper length) on the outcome (body mass) changes depending on the level of another predictor (e.g., species).\n\n(5.4) What does the following plot indicate about the relationship between flipper length and body mass for different islands?\n\nggplot(penguins_data, aes(x = flipper_length_mm, y = body_mass_g, color = island)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", color = \"Island\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCorrect Answer:\nB. The relationship between flipper length and body mass differs across species, with each species having a different slope\nExplanation:\nThe plot shows separate regression lines for each species, indicating that the relationship between flipper_length_mm and body_mass_g varies by species, with each species having a unique slope.\n\n(5.5) In the multiple regression model with interaction terms, how are dummy variables used for categorical predictors?\nCorrect Answer:\nA. Dummy variables are used to represent the different categories of a categorical variable\nExplanation:\nIn regression models, dummy variables are created for categorical variables like species to represent the different levels (e.g., Adélie, Chinstrap, Gentoo) in the model. These dummy variables allow the model to estimate how each category affects the outcome relative to the baseline category."
  },
  {
    "objectID": "answers/day2_walkthrough_answers.html#session-6-multiple-linear-regression-analysis-part-2",
    "href": "answers/day2_walkthrough_answers.html#session-6-multiple-linear-regression-analysis-part-2",
    "title": "Statistics in R with the tidyverse",
    "section": "",
    "text": "# Fit a multiple regression model without interaction terms\nmulti_model_no_interaction &lt;- lm(body_mass_g ~ flipper_length_mm + species, \n                              data = penguins_data)\n\n# Get regression coefficients\ncoef(multi_model_no_interaction)\n\n      (Intercept) flipper_length_mm  speciesChinstrap     speciesGentoo \n      -4013.17889          40.60617        -205.37548         284.52360 \n\n\nExplanation:\n\nWe fit a multiple regression model predicting body_mass_g (body mass) using both flipper_length_mm (flipper length) and species as explanatory variables.\nIn this model, the effect of each explanatory variable on body mass is assumed to be independent, meaning the slopes are the same across different levels of the other variable (i.e., no interaction).\nThe coefficients indicate the effect of each predictor on body mass, with each variable having its own impact on the response.\n\nThe output from our linear regression model (multi_model_no_interaction) can be interpreted as follows:\n\n\nThe model is predicting body_mass_g (the body mass of penguins in grams) based on flipper_length_mm (flipper length in millimeters) and species. Since the species variable is categorical, the model automatically creates dummy variables for the species, treating one species (Adelie) as the reference category.\nLet’s break down each coefficient:\n\n\n\n\nThis is the predicted body mass (in grams) for a penguin that belongs to the reference species Adelie and has a flipper_length_mm of 0 (which may not be realistic in this context, but it’s the mathematical meaning).\nIn simpler terms, it provides a baseline body mass for the reference species when other variables (like flipper length) are zero.\n\n\n\n\n\nFor each additional millimeter in flipper length, the model predicts an increase of approximately 40.61 grams in body mass, holding the species constant.\nThis means that flipper length has a positive effect on body mass.\n\n\n\n\n\nChinstrap penguins are expected to have, on average, body masses that are 205.38 grams lower than Adelie penguins (the reference species), holding flipper_length_mm constant.\nThis represents the difference between the Chinstrap species and the Adelie species in terms of body mass, after accounting for flipper length.\n\n\n\n\n\nGentoo penguins are expected to have, on average, body masses that are 284.52 grams higher than Adelie penguins, holding flipper_length_mm constant.\nThis shows that, after adjusting for flipper length, Gentoo penguins tend to be heavier than the Adelie species.\n\n\n\n\n\nThe model suggests that penguin body mass increases as flipper length increases.\nChinstrap penguins tend to have lower body masses than Adelie penguins (reference category), while Gentoo penguins tend to have higher body masses compared to Adelie penguins, even after accounting for differences in flipper length.\n\n\n\n\n\n\n\n# Scatterplot with regression lines by species (different intercepts but same slopes)\nggplot(penguins_data, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  geom_parallel_slopes(se = FALSE) +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", color = \"Species\")\n\n\n\n\nExplanation:\n\nWe visualize the fitted multiple regression model without interactions using parallel slope lines.\nThe function geom_parallel_slopes() is used to create regression lines with the same slope for each species.\nThis helps to visualize the idea of parallel slopes, where each regression line has the same slope but different intercepts depending on the other explanatory variable (flipper length).\n\n\n\n\n\n\n# Use tidy_summary to get a clean summary of the data for a multiple regression\n# with flipper length and bill length and regressors and body mass as the\n# outcome\npenguins_data |&gt;\n  select(body_mass_g, flipper_length_mm, bill_length_mm) |&gt;\n  tidy_summary()\n\n# A tibble: 3 × 11\n  column                n group type       min     Q1   mean median     Q3    max     sd\n  &lt;chr&gt;             &lt;int&gt; &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 body_mass_g         333 &lt;NA&gt;  numeric 2700   3550   4207.  4050   4775   6300   805.  \n2 flipper_length_mm   333 &lt;NA&gt;  numeric  172    190    201.   197    213    231    14.0 \n3 bill_length_mm      333 &lt;NA&gt;  numeric   32.1   39.5   44.0   44.5   48.6   59.6   5.47\n\n\nThis provides a tidy summary, giving key statistics such as the mean, median, and standard deviation for the selected variables.\n\n\n\n\n\n# Fit a multiple regression model\nmulti_model &lt;- lm(body_mass_g ~ flipper_length_mm + bill_length_mm, \n                  data = penguins_data)\n\n# Get regression coefficients\ncoef(multi_model)\n\n      (Intercept) flipper_length_mm    bill_length_mm \n     -5836.298732         48.889692          4.958601 \n\n\nIn this multiple regression model, we predict body_mass_g using both flipper_length_mm and bill_length_mm. The coefficients show the effect of each variable on the response.\n\n\n\n\n\n# Create scatterplot with regression lines for both variables\nggplot(penguins_data, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Flipper Length (mm)\", y = \"Body Mass (g)\",\n       title = \"Body Mass vs Flipper Length\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(penguins_data, aes(x = bill_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Bill Length (mm)\", y = \"Body Mass (g)\",\n       title = \"Body Mass vs Bill Length\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThese scatterplots display the relationships individually between flipper_length_mm and body_mass_g, and between bill_length_mm and body_mass_g, with linear regression lines added.\n\n\n\n\n(6.1) What is the main assumption in a multiple regression model without interaction terms?\nA. The relationship between the explanatory variables and the response is always quadratic.\nB. The slope of the regression line is different for each level of the categorical variable.\nC. The slope of the regression line is the same for all levels of the categorical variable.\nD. The intercept of the regression line is the same for all levels of the categorical variable.\n\n(6.2) In the model lm(body_mass_g ~ flipper_length_mm + species, data = penguins_data), what does the coefficient for speciesGentoo represent?\nA. The effect of being a Gentoo penguin on body mass compared to the baseline species (Adélie).\nB. The effect of flipper length on body mass for Gentoo penguins only.\nC. The change in flipper length due to species.\nD. The effect of body mass on flipper length for Gentoo penguins.\n\n(6.3) What does the function geom_parallel_slopes() do in a regression plot?\nA. It fits regression lines with different slopes for each group.\nB. It plots regression lines with the same slope but different intercepts for each group.\nC. It creates a scatterplot without any regression lines.\nD. It visualizes interaction effects between the variables.\n\n(6.4) In the model lm(body_mass_g ~ flipper_length_mm + bill_length_mm, data = penguins_data), what do the coefficients for flipper_length_mm and bill_length_mm represent?\nA. The total body mass of each penguin species.\nB. The predicted body mass for penguins with average bill and flipper lengths. C. The interaction effect between flipper length and bill length.\nD. The effect of bill length on body mass, controlling for flipper length, and vice versa.\n\n(6.5) What does a regression model without interaction terms assume about the relationship between the explanatory variables and the response variable?\nA. The explanatory variables are not related to the response.\nB. The relationship between the explanatory variables and the response is independent of one another.\nC. The explanatory variables interact to affect the response.\nD. The response variable depends only on the categorical variables.\n\n\n\n\n(6.1) What is the main assumption in a multiple regression model without interaction terms?\nCorrect Answer:\nC. The slope of the regression line is the same for all levels of the categorical variable.\nExplanation:\nIn a multiple regression model without interactions, the slopes are assumed to be the same across all levels of the categorical variable. Only the intercepts are allowed to differ by group.\n\n(6.2) In the model lm(body_mass_g ~ flipper_length_mm + species, data = penguins_data), what does the coefficient for speciesGentoo represent?\nCorrect Answer:\nA. The effect of being a Gentoo penguin on body mass compared to the baseline species (Adélie).\nExplanation:\nIn regression models with categorical variables, the coefficient for a level like speciesGentoo represents the difference in the response variable (body mass) for Gentoo penguins compared to the baseline species (Adélie).\n\n(6.3) What does the function geom_parallel_slopes() do in a regression plot?\nCorrect Answer:\nB. It plots regression lines with the same slope but different intercepts for each group.\nExplanation:\nThe geom_parallel_slopes() function is used to visualize regression models without interaction terms, where each group has the same slope but different intercepts.\n\n(6.4) In the model lm(body_mass_g ~ flipper_length_mm + bill_length_mm, data = penguins_data), what do the coefficients for flipper_length_mm and bill_length_mm represent?\nCorrect Answer:\nD. The effect of bill length on body mass, controlling for flipper length, and vice versa.\nExplanation:\nIn a multiple regression model, each coefficient represents the effect of that variable on the response, while controlling for the other variables in the model.\n\n(6.5) What does a regression model without interaction terms assume about the relationship between the explanatory variables and the response variable?\nCorrect Answer:\nB. The relationship between the explanatory variables and the response is independent of one another.\nExplanation:\nIn a regression model without interaction terms, it is assumed that the explanatory variables affect the response independently, meaning that the effect of one variable is not influenced by the level of the other."
  }
]